from __future__ import annotations

import jieba
import jieba.posseg as pseg
import numpy as np
from typing import List, Dict, Any, Tuple
from langchain_core.documents import Document
from .enhanced_tokenizer import get_enhanced_tokenizer


class RelevanceRanker:
    """
    æ–‡æ¡£ç›¸å…³æ€§æ’åºå™¨
    
    æä¾›å¤šç§æ’åºç­–ç•¥ï¼š
    1. å…³é”®è¯é¢‘ç‡æ’åºï¼šæ ¹æ®å…³é”®è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡æ’åº
    2. å…³é”®è¯ä½ç½®æ’åºï¼šæ ¹æ®å…³é”®è¯åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®æ’åºï¼ˆè¶Šé å‰è¶Šç›¸å…³ï¼‰
    3. å…³é”®è¯è¦†ç›–åº¦æ’åºï¼šæ ¹æ®æ–‡æ¡£è¦†ç›–æŸ¥è¯¢å…³é”®è¯çš„æ¯”ä¾‹æ’åº
    4. ç»„åˆæ’åºï¼šç»¼åˆè€ƒè™‘ä»¥ä¸Šå› ç´ è¿›è¡Œæ’åº
    """
    
    def __init__(self, 
                 weight_keyword_freq: float = 0.4,
                 weight_keyword_pos: float = 0.3, 
                 weight_keyword_coverage: float = 0.3):
        """
        åˆå§‹åŒ–æ’åºå™¨
        
        å‚æ•°:
            weight_keyword_freq: å…³é”®è¯é¢‘ç‡æƒé‡
            weight_keyword_pos: å…³é”®è¯ä½ç½®æƒé‡
            weight_keyword_coverage: å…³é”®è¯è¦†ç›–åº¦æƒé‡
        """
        self.weight_keyword_freq = weight_keyword_freq
        self.weight_keyword_pos = weight_keyword_pos
        self.weight_keyword_coverage = weight_keyword_coverage
    
    def rank_by_keyword_frequency(self, query: str, docs: List[Document]) -> List[Document]:
        """æ ¹æ®å…³é”®è¯é¢‘ç‡æ’åºï¼ˆä½¿ç”¨å¢å¼ºåˆ†è¯å™¨ï¼‰"""
        try:
            # ä½¿ç”¨å¢å¼ºåˆ†è¯å™¨æå–æŸ¥è¯¢å…³é”®è¯
            tokenizer = get_enhanced_tokenizer()
            keyword_tuples = tokenizer.extract_keywords(query, top_k=20, min_word_len=2)
            
            # æ„å»ºåŠ æƒå…³é”®è¯å­—å…¸
            query_keywords = {}
            for word, pos, weight in keyword_tuples:
                query_keywords[word.lower()] = weight
            
            # æ·»åŠ å®ä½“å…³é”®è¯
            person_names = tokenizer.get_person_names(query)
            organizations = tokenizer.get_organizations(query)
            locations = tokenizer.get_locations(query)
            
            # å®ä½“å…³é”®è¯ç»™äºˆæ›´é«˜æƒé‡
            for entity in person_names + organizations + locations:
                query_keywords[entity.lower()] = query_keywords.get(entity.lower(), 0) + 3.0
            
            print(f"ğŸ” æ’åºå…³é”®è¯: {list(query_keywords.keys())}")
            
            # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„åŠ æƒå…³é”®è¯é¢‘ç‡å¾—åˆ†
            doc_scores = []
            for doc in docs:
                content = doc.page_content.lower()
                score = 0
                
                for keyword, weight in query_keywords.items():
                    # è®¡ç®—å…³é”®è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•°
                    count = content.count(keyword)
                    # åŠ æƒè®¡ç®—å¾—åˆ†
                    score += count * weight
                
                doc_scores.append((doc, score))
            
            # æŒ‰å¾—åˆ†æ’åº
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            # æ‰“å°æ’åºç»“æœç»Ÿè®¡
            if doc_scores:
                print(f"ğŸ“Š æ’åºç»“æœ: æœ€é«˜åˆ†={doc_scores[0][1]:.2f}, æœ€ä½åˆ†={doc_scores[-1][1]:.2f}")
            
            return [doc for doc, score in doc_scores]
            
        except Exception as e:
            print(f"å…³é”®è¯é¢‘ç‡æ’åºå¤±è´¥: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            query_words = set(jieba.lcut(query))
            doc_scores = []
            for doc in docs:
                content = doc.page_content.lower()
                score = sum(content.count(word) for word in query_words)
                doc_scores.append((doc, score))
            
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            return [doc for doc, score in doc_scores]
    
    def rank_by_keyword_position(self, docs: List[Document], keywords: List[str]) -> List[Document]:
        """
        æ ¹æ®å…³é”®è¯åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®æ’åºï¼ˆå…³é”®è¯è¶Šé å‰ï¼Œæ–‡æ¡£è¶Šç›¸å…³ï¼‰
        
        å‚æ•°:
            docs: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            
        è¿”å›:
            æŒ‰å…³é”®è¯ä½ç½®æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not keywords or not docs:
            return docs
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£ä¸­å…³é”®è¯çš„æœ€å°ä½ç½®ï¼ˆè¶Šå°è¶Šé å‰ï¼‰
        doc_scores = []
        for doc in docs:
            content = doc.page_content.lower()
            
            # æ‰¾å‡ºæ‰€æœ‰å…³é”®è¯åœ¨æ–‡æ¡£ä¸­ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®
            positions = []
            for keyword in keywords:
                pos = content.find(keyword.lower())
                if pos != -1:  # å¦‚æœæ‰¾åˆ°äº†å…³é”®è¯
                    positions.append(pos)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…³é”®è¯ï¼Œåˆ™ä½ç½®è®¾ä¸ºæ–‡æ¡£é•¿åº¦ï¼ˆæœ€å¤§å€¼ï¼‰
            min_pos = min(positions) if positions else len(content)
            
            # è®¡ç®—ä½ç½®åˆ†æ•°ï¼šè¶Šé å‰åˆ†æ•°è¶Šé«˜ï¼ˆä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼‰
            position_score = np.exp(-min_pos / max(1, len(content)))
            doc_scores.append((doc, position_score))
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        return [doc for doc, _ in doc_scores]
    
    def rank_by_keyword_coverage(self, docs: List[Document], keywords: List[str]) -> List[Document]:
        """
        æ ¹æ®æ–‡æ¡£è¦†ç›–æŸ¥è¯¢å…³é”®è¯çš„æ¯”ä¾‹æ’åº
        
        å‚æ•°:
            docs: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            
        è¿”å›:
            æŒ‰å…³é”®è¯è¦†ç›–åº¦æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not keywords or not docs:
            return docs
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£è¦†ç›–çš„å…³é”®è¯æ¯”ä¾‹
        doc_scores = []
        for doc in docs:
            content = doc.page_content.lower()
            
            # è®¡ç®—æ–‡æ¡£ä¸­åŒ…å«çš„å…³é”®è¯æ•°é‡
            covered_keywords = sum(1 for keyword in keywords if keyword.lower() in content)
            
            # è®¡ç®—è¦†ç›–ç‡
            coverage_score = covered_keywords / max(1, len(keywords))
            doc_scores.append((doc, coverage_score))
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        return [doc for doc, _ in doc_scores]
    
    def rank_documents(self, docs: List[Document], query: str) -> List[Document]:
        """
        ç»¼åˆå¤šç§æ’åºç­–ç•¥å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼ˆä½¿ç”¨å¢å¼ºåˆ†è¯å™¨ï¼‰
        
        å‚æ•°:
            docs: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        è¿”å›:
            æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not docs:
            return docs
        
        try:
            # ä½¿ç”¨å¢å¼ºåˆ†è¯å™¨æå–å…³é”®è¯
            tokenizer = get_enhanced_tokenizer()
            keyword_tuples = tokenizer.extract_keywords(query, top_k=15, min_word_len=2)
            
            # æ„å»ºåŠ æƒå…³é”®è¯å­—å…¸
            weighted_keywords = {}
            for word, pos, weight in keyword_tuples:
                weighted_keywords[word.lower()] = weight
            
            # æ·»åŠ å®ä½“å…³é”®è¯ï¼ˆç»™äºˆæ›´é«˜æƒé‡ï¼‰
            person_names = tokenizer.get_person_names(query)
            organizations = tokenizer.get_organizations(query)
            locations = tokenizer.get_locations(query)
            
            for entity in person_names + organizations + locations:
                weighted_keywords[entity.lower()] = weighted_keywords.get(entity.lower(), 0) + 4.0
            
            print(f"ğŸ¯ ç»¼åˆæ’åºå…³é”®è¯: {list(weighted_keywords.keys())}")
            
            # è®¡ç®—å„ç§æ’åºåˆ†æ•°
            doc_scores = []
            for doc in docs:
                content = doc.page_content.lower()
                
                # 1. åŠ æƒå…³é”®è¯é¢‘ç‡åˆ†æ•°
                freq_score = 0
                total_weight = sum(weighted_keywords.values())
                for keyword, weight in weighted_keywords.items():
                    count = content.count(keyword)
                    freq_score += (count * weight) / max(1, len(content))
                freq_score = freq_score / max(1, total_weight) if total_weight > 0 else 0
                
                # 2. å…³é”®è¯ä½ç½®åˆ†æ•°ï¼ˆè€ƒè™‘æƒé‡ï¼‰
                weighted_positions = []
                for keyword, weight in weighted_keywords.items():
                    pos = content.find(keyword)
                    if pos != -1:
                        # ä½ç½®åˆ†æ•°ä¹˜ä»¥æƒé‡
                        position_value = np.exp(-pos / max(1, len(content))) * weight
                        weighted_positions.append(position_value)
                
                position_score = sum(weighted_positions) / max(1, total_weight) if weighted_positions and total_weight > 0 else 0
                
                # 3. å…³é”®è¯è¦†ç›–åº¦åˆ†æ•°
                covered_keywords = sum(weight for keyword, weight in weighted_keywords.items() if keyword in content)
                coverage_score = covered_keywords / max(1, total_weight) if total_weight > 0 else 0
                
                # 4. ç»„åˆåˆ†æ•°
                combined_score = (
                    self.weight_keyword_freq * freq_score +
                    self.weight_keyword_pos * position_score +
                    self.weight_keyword_coverage * coverage_score
                )
                
                # å­˜å‚¨æ–‡æ¡£å’Œåˆ†æ•°
                doc_scores.append((doc, combined_score))
            
            # æŒ‰ç»„åˆåˆ†æ•°é™åºæ’åº
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            # æ‰“å°æ’åºç»Ÿè®¡
            if doc_scores:
                print(f"ğŸ“ˆ ç»¼åˆæ’åºç»“æœ: æœ€é«˜åˆ†={doc_scores[0][1]:.4f}, æœ€ä½åˆ†={doc_scores[-1][1]:.4f}")
            
            # è¿”å›æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
            return [doc for doc, _ in doc_scores]
            
        except Exception as e:
            print(f"ç»¼åˆæ’åºå¤±è´¥: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            keywords = [word for word in jieba.lcut_for_search(query) if len(word) > 1]
            if not keywords:
                keywords = [query]
            
            doc_scores = []
            for doc in docs:
                content = doc.page_content.lower()
                
                freq_count = sum(content.count(keyword.lower()) for keyword in keywords)
                freq_score = freq_count / max(1, len(content))
                
                positions = []
                for keyword in keywords:
                    pos = content.find(keyword.lower())
                    if pos != -1:
                        positions.append(pos)
                min_pos = min(positions) if positions else len(content)
                position_score = np.exp(-min_pos / max(1, len(content)))
                
                covered_keywords = sum(1 for keyword in keywords if keyword.lower() in content)
                coverage_score = covered_keywords / max(1, len(keywords))
                
                combined_score = (
                    self.weight_keyword_freq * freq_score +
                    self.weight_keyword_pos * position_score +
                    self.weight_keyword_coverage * coverage_score
                )
                
                doc_scores.append((doc, combined_score))
            
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            return [doc for doc, _ in doc_scores]
    
    def rank_documents_with_scores(self, docs: List[Document], query: str) -> List[Tuple[Document, Dict[str, float]]]:
        """
        ç»¼åˆå¤šç§æ’åºç­–ç•¥å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼Œå¹¶è¿”å›è¯¦ç»†çš„åˆ†æ•°ä¿¡æ¯
        
        å‚æ•°:
            docs: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        è¿”å›:
            æ’åºåçš„æ–‡æ¡£åˆ—è¡¨åŠå…¶è¯¦ç»†åˆ†æ•°
        """
        if not docs:
            return []
        
        # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯
        keywords = [word for word in jieba.lcut_for_search(query) if len(word) > 1]
        if not keywords:
            keywords = [query]  # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªæŸ¥è¯¢ä½œä¸ºå…³é”®è¯
        
        # è®¡ç®—å„ç§æ’åºåˆ†æ•°
        doc_scores = []
        for doc in docs:
            content = doc.page_content.lower()
            
            # 1. å…³é”®è¯é¢‘ç‡åˆ†æ•°
            freq_count = sum(content.count(keyword.lower()) for keyword in keywords)
            freq_score = freq_count / max(1, len(content))
            
            # 2. å…³é”®è¯ä½ç½®åˆ†æ•°
            positions = []
            for keyword in keywords:
                pos = content.find(keyword.lower())
                if pos != -1:
                    positions.append(pos)
            min_pos = min(positions) if positions else len(content)
            position_score = np.exp(-min_pos / max(1, len(content)))
            
            # 3. å…³é”®è¯è¦†ç›–åº¦åˆ†æ•°
            covered_keywords = sum(1 for keyword in keywords if keyword.lower() in content)
            coverage_score = covered_keywords / max(1, len(keywords))
            
            # 4. ç»„åˆåˆ†æ•°
            combined_score = (
                self.weight_keyword_freq * freq_score +
                self.weight_keyword_pos * position_score +
                self.weight_keyword_coverage * coverage_score
            )
            
            # å­˜å‚¨æ–‡æ¡£å’Œè¯¦ç»†åˆ†æ•°
            scores = {
                "frequency_score": freq_score,
                "position_score": position_score,
                "coverage_score": coverage_score,
                "combined_score": combined_score
            }
            doc_scores.append((doc, scores))
        
        # æŒ‰ç»„åˆåˆ†æ•°é™åºæ’åº
        doc_scores.sort(key=lambda x: x[1]["combined_score"], reverse=True)
        
        return doc_scores