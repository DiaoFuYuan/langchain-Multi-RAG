# -*- coding: utf-8 -*-

import os
import sys
import time
from typing import List, Dict, Any, Optional, Tuple
from langchain.schema import Document
from langchain.vectorstores.base import VectorStore
from langchain.embeddings.base import Embeddings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from .base import BaseRetrieverService
from .utils import EnhancedTokenizer

# å¯¼å…¥æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨
try:
    from .query_decomposer import IntelligentQueryDecomposer, MultiPathRetriever
    QUERY_DECOMPOSER_AVAILABLE = True
    print("âœ… æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨å¯¼å…¥å¤±è´¥: {e}")
    QUERY_DECOMPOSER_AVAILABLE = False

# å¯¼å…¥å¢å¼ºæ£€ç´¢æ¨¡å—
try:
    from .hierarchical_modules.keyword_processor import KeywordProcessor
    from .hierarchical_modules.hierarchical_config import HierarchicalConfig
    ENHANCED_MODULES_AVAILABLE = True
    print("âœ… å¢å¼ºæ£€ç´¢æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ å¢å¼ºæ£€ç´¢æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    ENHANCED_MODULES_AVAILABLE = False

class HierarchicalRetrieverService(BaseRetrieverService):
    """åˆ†å±‚æ£€ç´¢å™¨æœåŠ¡ - é›†æˆæ™ºèƒ½æŸ¥è¯¢åˆ†è§£å’Œå¢å¼ºæ£€ç´¢"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.summary_vectorstore: Optional[VectorStore] = None
        self.chunk_vectorstore: Optional[VectorStore] = None
        self.summary_top_k = kwargs.get('summary_top_k', 5)
        self.chunk_top_k = kwargs.get('chunk_top_k', 10)
        self.summary_score_threshold = kwargs.get('summary_score_threshold', 0.3)
        self.chunk_score_threshold = kwargs.get('chunk_score_threshold', 0.3)
        self.context_window = kwargs.get('context_window', 4000)
        self.enable_summary_fallback = kwargs.get('enable_summary_fallback', True)
        self.enhanced_tokenizer = EnhancedTokenizer()
        
        # æ™ºèƒ½æŸ¥è¯¢åˆ†è§£é…ç½®
        self.enable_intelligent_decomposition = kwargs.get('enable_intelligent_decomposition', True)
        self.enable_multi_path_retrieval = kwargs.get('enable_multi_path_retrieval', True)
        self.complexity_threshold = kwargs.get('complexity_threshold', 0.3)
        
        # å¢å¼ºæ£€ç´¢é…ç½®
        self.enable_enhanced_second_layer = kwargs.get('enable_enhanced_second_layer', True)
        self.enable_entity_matching = kwargs.get('enable_entity_matching', True)
        self.enable_semantic_expansion = kwargs.get('enable_semantic_expansion', True)
        
        # åˆå§‹åŒ–æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨
        if QUERY_DECOMPOSER_AVAILABLE and self.enable_intelligent_decomposition:
            try:
                self.query_decomposer = IntelligentQueryDecomposer()
                self.multi_path_retriever = None  # å»¶è¿Ÿåˆå§‹åŒ–
                print("âœ… æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.query_decomposer = None
                self.multi_path_retriever = None
        else:
            self.query_decomposer = None
            self.multi_path_retriever = None
        
        # åˆå§‹åŒ–å¢å¼ºæ£€ç´¢æ¨¡å—
        if ENHANCED_MODULES_AVAILABLE:
            try:
                self.config = HierarchicalConfig.create_accurate_config()
                self.keyword_processor = KeywordProcessor(self.config)
                print("âœ… å¢å¼ºæ£€ç´¢æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å¢å¼ºæ£€ç´¢æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                self.keyword_processor = None
        else:
            self.keyword_processor = None
        
    def do_init(self, **kwargs):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        pass
        
    @classmethod
    def from_vectorstore(
        cls,
        vectorstore: VectorStore,
        **kwargs
    ) -> "HierarchicalRetrieverService":
        """ä»å‘é‡å­˜å‚¨åˆ›å»ºåˆ†å±‚æ£€ç´¢å™¨"""
        instance = cls(**kwargs)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åˆ†å±‚ç»“æ„
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„vectorstore_pathï¼Œå¦åˆ™å°è¯•ä»vectorstoreè·å–
        vectorstore_path = kwargs.get('vectorstore_path') or getattr(vectorstore, 'persist_directory', None)
        print(f"ğŸ” æ£€æŸ¥åˆ†å±‚ç»“æ„ï¼Œå‘é‡å­˜å‚¨è·¯å¾„: {vectorstore_path}")
        
        if vectorstore_path:
            # ä¿®æ­£è·¯å¾„æ£€æµ‹é€»è¾‘ï¼šæ­£ç¡®æ„å»ºåˆ†å±‚å‘é‡å­˜å‚¨è·¯å¾„
            # vectorstore_pathé€šå¸¸æŒ‡å‘ data/knowledge_base/PMS/vector_store
            # åˆ†å±‚ç»“æ„åº”è¯¥åœ¨ data/knowledge_base/PMS/hierarchical_vector_store
            
            # è·å–çŸ¥è¯†åº“æ ¹ç›®å½•ï¼ˆå»æ‰vector_storeéƒ¨åˆ†ï¼‰
            if vectorstore_path.endswith('vector_store'):
                kb_root_path = os.path.dirname(vectorstore_path)
            else:
                kb_root_path = vectorstore_path
            
            # æ„å»ºåˆ†å±‚å‘é‡å­˜å‚¨è·¯å¾„
            hierarchical_base_path = os.path.join(kb_root_path, 'hierarchical_vector_store')
            summary_path = os.path.join(hierarchical_base_path, 'summary_vector_store')
            chunk_path = os.path.join(hierarchical_base_path, 'chunk_vector_store')
            
            print(f"ğŸ“‹ æ£€æŸ¥æ‘˜è¦å‘é‡å­˜å‚¨è·¯å¾„: {summary_path}")
            print(f"ğŸ“„ æ£€æŸ¥å—å‘é‡å­˜å‚¨è·¯å¾„: {chunk_path}")
            print(f"ğŸ“‹ æ‘˜è¦è·¯å¾„å­˜åœ¨: {os.path.exists(summary_path)}")
            print(f"ğŸ“„ å—è·¯å¾„å­˜åœ¨: {os.path.exists(chunk_path)}")
            
            if os.path.exists(summary_path) and os.path.exists(chunk_path):
                # å­˜åœ¨åˆ†å±‚ç»“æ„ï¼Œè®¾ç½®åˆ†å±‚å‘é‡å­˜å‚¨
                try:
                    from langchain_community.vectorstores import FAISS
                    print(f"ğŸ”„ åŠ è½½æ‘˜è¦å‘é‡å­˜å‚¨: {summary_path}")
                    # è·å–åµŒå…¥å‡½æ•°ï¼Œå°è¯•å¤šç§å±æ€§å
                    embedding_function = None
                    for attr_name in ['_embedding_function', 'embedding_function', 'embeddings']:
                        if hasattr(vectorstore, attr_name):
                            embedding_function = getattr(vectorstore, attr_name)
                            print(f"âœ… æ‰¾åˆ°åµŒå…¥å‡½æ•°å±æ€§: {attr_name}")
                            break
                    
                    if embedding_function is None:
                        raise AttributeError("æ— æ³•æ‰¾åˆ°å‘é‡å­˜å‚¨çš„åµŒå…¥å‡½æ•°")
                    
                    instance.summary_vectorstore = FAISS.load_local(
                        summary_path,
                        embedding_function,
                        allow_dangerous_deserialization=True
                    )
                    print(f"ğŸ”„ åŠ è½½å—å‘é‡å­˜å‚¨: {chunk_path}")
                    instance.chunk_vectorstore = FAISS.load_local(
                        chunk_path,
                        embedding_function,
                        allow_dangerous_deserialization=True
                    )
                    print(f"âœ… æˆåŠŸåŠ è½½åˆ†å±‚å‘é‡å­˜å‚¨")
                except Exception as e:
                    print(f"âŒ åŠ è½½åˆ†å±‚å‘é‡å­˜å‚¨å¤±è´¥: {e}")
                    print(f"ğŸ”„ å›é€€åˆ°æ™®é€šå‘é‡å­˜å‚¨")
                    instance.chunk_vectorstore = vectorstore
            else:
                # ä¸å­˜åœ¨åˆ†å±‚ç»“æ„ï¼Œä½¿ç”¨æ™®é€šå‘é‡å­˜å‚¨
                print(f"âš ï¸ åˆ†å±‚ç»“æ„ä¸å®Œæ•´ï¼Œä½¿ç”¨æ™®é€šå‘é‡å­˜å‚¨")
                instance.chunk_vectorstore = vectorstore
        else:
            print(f"âš ï¸ æ— å‘é‡å­˜å‚¨è·¯å¾„ï¼Œä½¿ç”¨æ™®é€šå‘é‡å­˜å‚¨")
            instance.chunk_vectorstore = vectorstore
        
        # åˆå§‹åŒ–å¤šè·¯æ£€ç´¢å™¨
        if QUERY_DECOMPOSER_AVAILABLE and instance.enable_multi_path_retrieval:
            try:
                instance.multi_path_retriever = MultiPathRetriever(instance)
                print("âœ… å¤šè·¯æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å¤šè·¯æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                instance.multi_path_retriever = None
            
        return instance
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """è·å–ç›¸å…³æ–‡æ¡£ - é›†æˆæ™ºèƒ½æŸ¥è¯¢åˆ†è§£å’Œå¢å¼ºæ£€ç´¢"""
        try:
            print(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ£€ç´¢ï¼ŒæŸ¥è¯¢: '{query}'")
            
            # 1. æ™ºèƒ½æŸ¥è¯¢åˆ†æå’Œè¯­ä¹‰ç†è§£
            enhanced_query_info = self._analyze_and_enhance_query(query)
            
            # 2. åˆ¤æ–­æ£€ç´¢ç­–ç•¥
            use_intelligent_search = self._should_use_intelligent_search(query, enhanced_query_info)
            
            if use_intelligent_search and self.multi_path_retriever:
                print("ğŸ§  ä½¿ç”¨æ™ºèƒ½å¤šè·¯æ£€ç´¢")
                return self.multi_path_retriever.multi_path_search(query)
            elif self.summary_vectorstore and self.chunk_vectorstore:
                print("ğŸ“Š ä½¿ç”¨å¢å¼ºåˆ†å±‚æ£€ç´¢")
                return self._enhanced_hierarchical_search(query, enhanced_query_info)
            elif self.chunk_vectorstore:
                print("ğŸ“ ä½¿ç”¨å¢å¼ºç®€å•æ£€ç´¢")
                return self._enhanced_simple_search(query, enhanced_query_info)
            else:
                print("âŒ æ— å¯ç”¨å‘é‡å­˜å‚¨")
                return []
        except Exception as e:
            print(f"âŒ æ™ºèƒ½æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _analyze_and_enhance_query(self, query: str) -> Dict[str, Any]:
        """åˆ†æå’Œå¢å¼ºæŸ¥è¯¢"""
        enhanced_info = {
            "entities": {},
            "keywords": [],
            "query_intent": "general",
            "complexity_score": 0.0
        }
        
        try:
            # 1. ä½¿ç”¨å¢å¼ºåˆ†è¯å™¨æå–å®ä½“å’Œå…³é”®è¯
            if self.enhanced_tokenizer:
                # æå–äººå‘˜å§“å
                person_names = self.enhanced_tokenizer.get_person_names(query)
                if person_names:
                    enhanced_info["entities"]["persons"] = person_names
                
                # æå–ç»„ç»‡æœºæ„
                organizations = self.enhanced_tokenizer.get_organizations(query)
                if organizations:
                    enhanced_info["entities"]["organizations"] = organizations
                
                # æå–å…³é”®è¯
                keywords = self.enhanced_tokenizer.extract_keywords(query, top_k=10)
                enhanced_info["keywords"] = keywords
            
            # 2. åˆ†ææŸ¥è¯¢æ„å›¾
            enhanced_info["query_intent"] = self._analyze_query_intent(query, enhanced_info)
            
            # 3. ç”Ÿæˆè¯­ä¹‰æ‰©å±•
            if self.enable_semantic_expansion:
                semantic_expansions = self._generate_semantic_expansions(query, enhanced_info)
                enhanced_info["semantic_expansions"] = semantic_expansions
            
            # 4. è®¡ç®—å¤æ‚åº¦è¯„åˆ†
            complexity_score = len(enhanced_info["entities"].get("persons", [])) * 0.2
            complexity_score += len(enhanced_info["keywords"]) * 0.1
            enhanced_info["complexity_score"] = min(complexity_score, 1.0)
            
            print(f"ğŸ” æŸ¥è¯¢å¢å¼ºå®Œæˆ: å®ä½“={enhanced_info['entities']}, æ„å›¾={enhanced_info['query_intent']}")
            
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢å¢å¼ºå¤±è´¥: {e}")
        
        return enhanced_info
    
    def _analyze_query_intent(self, query: str, enhanced_info: Dict) -> str:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()
        
        # æŠ•è¯‰ç›¸å…³æŸ¥è¯¢
        if any(word in query_lower for word in ['æŠ•è¯‰', 'ä¸¾æŠ¥', 'åæ˜ ', 'ç”³è¯‰']):
            if any(word in query_lower for word in ['å†…å®¹', 'ä»€ä¹ˆ', 'è¯¦æƒ…']):
                return "complaint_content"
            elif any(word in query_lower for word in ['å¤„ç†', 'ç»“æœ', 'å›å¤']):
                return "complaint_handling"
            else:
                return "complaint_general"
        
        # äººå‘˜ç›¸å…³æŸ¥è¯¢
        if enhanced_info.get("entities", {}).get("persons"):
            return "person_related"
        
        # äº‹ä»¶ç›¸å…³æŸ¥è¯¢
        if any(word in query_lower for word in ['äº‹ä»¶', 'æƒ…å†µ', 'é—®é¢˜']):
            return "event_related"
        
        return "general"
    
    def _generate_semantic_expansions(self, query: str, enhanced_info: Dict) -> List[str]:
        """ç”Ÿæˆè¯­ä¹‰æ‰©å±•"""
        expansions = []
        
        # åŸºäºæŸ¥è¯¢æ„å›¾ç”Ÿæˆæ‰©å±•
        intent = enhanced_info.get("query_intent", "general")
        
        if intent == "complaint_content":
            expansions.extend(["æŠ•è¯‰è¯¦æƒ…", "åæ˜ å†…å®¹", "ä¸¾æŠ¥äº‹é¡¹", "ç”³è¯‰åŸå› "])
        elif intent == "complaint_handling":
            expansions.extend(["å¤„ç†ç»“æœ", "å›å¤æƒ…å†µ", "è§£å†³æ–¹æ¡ˆ", "è°ƒæŸ¥ç»“è®º"])
        elif intent == "person_related":
            expansions.extend(["ç›¸å…³äººå‘˜", "å½“äº‹äºº", "è´£ä»»äºº"])
        
        # åŸºäºå®ä½“ç”Ÿæˆæ‰©å±•
        entities = enhanced_info.get("entities", {})
        for entity_type, entity_list in entities.items():
            for entity in entity_list[:2]:  # æ¯ç§å®ä½“ç±»å‹å–å‰2ä¸ª
                if entity_type == "persons":
                    expansions.append(f"{entity}ç›¸å…³")
                    expansions.append(f"å…³äº{entity}")
        
        return expansions[:5]  # é™åˆ¶æ‰©å±•æ•°é‡
    
    def _should_use_intelligent_search(self, query: str, enhanced_info: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æœç´¢"""
        # å¦‚æœæŸ¥è¯¢å¤æ‚åº¦é«˜ï¼Œä½¿ç”¨æ™ºèƒ½æœç´¢
        if enhanced_info.get("complexity_score", 0) > self.complexity_threshold:
            return True
        
        # å¦‚æœåŒ…å«äººå‘˜å®ä½“ï¼Œä½¿ç”¨æ™ºèƒ½æœç´¢
        if enhanced_info.get("entities", {}).get("persons"):
            return True
        
        # å¦‚æœæ˜¯æŠ•è¯‰ç›¸å…³æŸ¥è¯¢ï¼Œä½¿ç”¨æ™ºèƒ½æœç´¢
        if enhanced_info.get("query_intent", "").startswith("complaint"):
            return True
        
        return False
    
    def _enhanced_hierarchical_search(self, query: str, enhanced_info: Dict) -> List[Document]:
        """å¢å¼ºåˆ†å±‚æœç´¢"""
        try:
            print(f"ğŸ” å¼€å§‹å¢å¼ºåˆ†å±‚æ£€ç´¢ï¼ŒæŸ¥è¯¢: '{query}'")
            
            # æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å¯ç”¨
            if not self.summary_vectorstore:
                print("âŒ æ‘˜è¦å‘é‡å­˜å‚¨ä¸å¯ç”¨")
                if self.enable_summary_fallback:
                    return self._enhanced_simple_search(query, enhanced_info)
                return []
            
            if not self.chunk_vectorstore:
                print("âŒ å—å‘é‡å­˜å‚¨ä¸å¯ç”¨")
                return []
            
            # ç¬¬ä¸€å±‚ï¼šåœ¨æ‘˜è¦ä¸­æœç´¢ï¼ˆä½¿ç”¨å¢å¼ºæŸ¥è¯¢ï¼‰
            print(f"ğŸ“‹ ç¬¬ä¸€å±‚ï¼šåœ¨æ‘˜è¦å‘é‡å­˜å‚¨ä¸­æœç´¢ï¼Œtop_k={self.summary_top_k}")
            summary_docs = self._enhanced_summary_search(query, enhanced_info)
            
            if not summary_docs:
                print("âš ï¸ æ‘˜è¦æœç´¢æ— ç»“æœï¼Œå¯ç”¨å›é€€æœç´¢")
                if self.enable_summary_fallback:
                    return self._enhanced_simple_search(query, enhanced_info)
                return []
            
            # æå–ç›¸å…³æ–‡æ¡£IDï¼ˆä½¿ç”¨å¢å¼ºåŒ¹é…ï¼‰
            relevant_doc_ids = self._extract_relevant_doc_ids(summary_docs, enhanced_info)
            
            if not relevant_doc_ids:
                print("âš ï¸ æ— ç›¸å…³æ–‡æ¡£IDï¼Œå¯ç”¨å›é€€æœç´¢")
                if self.enable_summary_fallback:
                    return self._enhanced_simple_search(query, enhanced_info)
                return []
            
            # ç¬¬äºŒå±‚ï¼šåœ¨ç›¸å…³æ–‡æ¡£çš„å—ä¸­æœç´¢ï¼ˆä½¿ç”¨å¢å¼ºæ£€ç´¢ï¼‰
            print(f"ğŸ“„ ç¬¬äºŒå±‚ï¼šä½¿ç”¨å¢å¼ºæ£€ç´¢åœ¨å—å‘é‡å­˜å‚¨ä¸­æœç´¢")
            chunk_docs = self._enhanced_chunk_search(query, enhanced_info, relevant_doc_ids)
            
            print(f"ğŸ¯ å¢å¼ºåˆ†å±‚æ£€ç´¢å®Œæˆï¼Œæœ€ç»ˆè¿”å›æ–‡æ¡£æ•°é‡: {len(chunk_docs)}")
            return chunk_docs
            
        except Exception as e:
            print(f"å¢å¼ºåˆ†å±‚æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            if self.enable_summary_fallback:
                return self._enhanced_simple_search(query, enhanced_info)
            return []
    
    def _enhanced_summary_search(self, query: str, enhanced_info: Dict) -> List[Tuple[Document, float]]:
        """å¢å¼ºæ‘˜è¦æœç´¢"""
        all_results = {}
        
        try:
            # æ£€æŸ¥æ‘˜è¦å‘é‡å­˜å‚¨çŠ¶æ€
            if not hasattr(self.summary_vectorstore, 'similarity_search_with_score'):
                print("âŒ æ‘˜è¦å‘é‡å­˜å‚¨ä¸æ”¯æŒç›¸ä¼¼åº¦æœç´¢")
                return []
            
            # 1. åŸå§‹æŸ¥è¯¢æœç´¢
            print(f"ğŸ” æ‰§è¡ŒåŸå§‹æŸ¥è¯¢æœç´¢: '{query}'")
            original_results = self.summary_vectorstore.similarity_search_with_score(
                    query, k=self.summary_top_k
                )
            print(f"ğŸ“‹ åŸå§‹æŸ¥è¯¢ç»“æœæ•°é‡: {len(original_results)}")
            
            for doc, score in original_results:
                doc_key = hash(doc.page_content)
                if doc_key not in all_results or all_results[doc_key][1] > score:
                    all_results[doc_key] = (doc, score)
                    print(f"âœ… åŸå§‹æŸ¥è¯¢åŒ¹é…: score={score:.4f}, doc_id={doc.metadata.get('doc_id', 'N/A')}")
            
            # 2. å®ä½“æŸ¥è¯¢æœç´¢
            entities = enhanced_info.get("entities", {})
            for entity_type, entity_list in entities.items():
                for entity in entity_list[:2]:  # æ¯ç§å®ä½“ç±»å‹æœç´¢å‰2ä¸ª
                    try:
                        entity_results = self.summary_vectorstore.similarity_search_with_score(
                            entity, k=3
                        )
                        for doc, score in entity_results:
                            # å®ä½“åŒ¹é…ç»™äºˆæ›´é«˜æƒé‡
                            weighted_score = score * 0.8
                            doc_key = hash(doc.page_content)
                            if doc_key not in all_results or all_results[doc_key][1] > weighted_score:
                                all_results[doc_key] = (doc, weighted_score)
                                print(f"âœ… å®ä½“æœç´¢åŒ¹é…: '{entity}' -> score={weighted_score:.4f}")
                    except Exception as e:
                        print(f"âš ï¸ å®ä½“ '{entity}' æœç´¢å¤±è´¥: {e}")
            
            # 3. å…³é”®è¯æŸ¥è¯¢æœç´¢
            keywords = enhanced_info.get("keywords", [])
            for keyword, pos, weight in keywords[:3]:  # å‰3ä¸ªå…³é”®è¯
                if weight > 1.0:  # åªæœç´¢é‡è¦å…³é”®è¯
                    try:
                        keyword_results = self.summary_vectorstore.similarity_search_with_score(
                            keyword, k=2
                        )
                        for doc, score in keyword_results:
                            weighted_score = score / weight  # æƒé‡è¶Šé«˜ï¼Œåˆ†æ•°è¶Šä½ï¼ˆæ›´ç›¸å…³ï¼‰
                            doc_key = hash(doc.page_content)
                            if doc_key not in all_results or all_results[doc_key][1] > weighted_score:
                                all_results[doc_key] = (doc, weighted_score)
                                print(f"âœ… å…³é”®è¯æœç´¢åŒ¹é…: '{keyword}' -> score={weighted_score:.4f}")
                    except Exception as e:
                        print(f"âš ï¸ å…³é”®è¯ '{keyword}' æœç´¢å¤±è´¥: {e}")
            
            # æ’åºå¹¶è¿”å›ç»“æœ
            sorted_results = sorted(all_results.values(), key=lambda x: x[1])
            final_results = sorted_results[:self.summary_top_k * 2]
            
            print(f"ğŸ“‹ å¢å¼ºæ‘˜è¦æœç´¢ç»“æœæ•°é‡: {len(final_results)}")
            for i, (doc, score) in enumerate(final_results):
                print(f"  æ‘˜è¦ {i+1}: score={score:.4f}, doc_id={doc.metadata.get('doc_id', 'N/A')}, content_preview={doc.page_content[:100]}...")
            
            return final_results
            
        except Exception as e:
            print(f"âŒ å¢å¼ºæ‘˜è¦æœç´¢å¤±è´¥: {e}")
            return []
            
    def _extract_relevant_doc_ids(self, summary_docs: List[Tuple[Document, float]], enhanced_info: Dict) -> set:
        """æå–ç›¸å…³æ–‡æ¡£IDï¼ˆä½¿ç”¨å¢å¼ºåŒ¹é…ï¼‰"""
        relevant_doc_ids = set()
        
        # 1. åŸºäºåˆ†æ•°é˜ˆå€¼çš„åŒ¹é…
        for doc, score in summary_docs:
            if score >= self.summary_score_threshold:
                doc_id = doc.metadata.get('doc_id')
                if doc_id:
                    relevant_doc_ids.add(doc_id)
                    print(f"âœ… æ‘˜è¦é€šè¿‡é˜ˆå€¼æ£€æŸ¥: doc_id={doc_id}, score={score:.4f}")
            else:
                print(f"âŒ æ‘˜è¦æœªé€šè¿‡é˜ˆå€¼æ£€æŸ¥: score={score:.4f} < {self.summary_score_threshold}")
            
        # 2. åŸºäºå®ä½“åŒ¹é…çš„å¢å¼ºè¿‡æ»¤
        if self.enable_entity_matching:
            entities = enhanced_info.get("entities", {})
            all_entities = []
            for entity_list in entities.values():
                all_entities.extend(entity_list)
            
            if all_entities:
                print(f"ğŸ” ä½¿ç”¨å®ä½“åŒ¹é…å¢å¼ºè¿‡æ»¤: {all_entities}")
                for doc, score in summary_docs:
                    doc_content = doc.page_content.lower()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å®ä½“
                    for entity in all_entities:
                        if entity.lower() in doc_content:
                            doc_id = doc.metadata.get('doc_id')
                            if doc_id:
                                relevant_doc_ids.add(doc_id)
                                print(f"âœ… å®ä½“åŒ¹é…é€šè¿‡: doc_id={doc_id}, entity='{entity}', score={score:.4f}")
                                break
        
        # 3. å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ–‡æ¡£ï¼Œé™ä½é˜ˆå€¼é‡æ–°åŒ¹é…
        if not relevant_doc_ids and summary_docs:
            print("âš ï¸ æ— åŒ¹é…æ–‡æ¡£ï¼Œé™ä½é˜ˆå€¼é‡æ–°åŒ¹é…...")
            relaxed_threshold = self.summary_score_threshold * 1.5
            for doc, score in summary_docs:
                if score <= relaxed_threshold:  # æ³¨æ„ï¼šåˆ†æ•°è¶Šä½è¶Šç›¸å…³
                    doc_id = doc.metadata.get('doc_id')
                    if doc_id:
                        relevant_doc_ids.add(doc_id)
                        print(f"ğŸ”„ æ”¾å®½é˜ˆå€¼é€šè¿‡: doc_id={doc_id}, score={score:.4f}")
        
        print(f"ğŸ“ ç›¸å…³æ–‡æ¡£IDé›†åˆ: {relevant_doc_ids}")
        return relevant_doc_ids
    
    def _enhanced_chunk_search(self, query: str, enhanced_info: Dict, relevant_doc_ids: set) -> List[Document]:
        """å¢å¼ºå—æœç´¢ - ç²¾ç¡®å®šä½åŒ…å«å…³é”®å­—çš„æ–‡æ¡£å—ï¼Œå¹¶èšåˆåŒä¸€å®ä½“çš„æ‰€æœ‰ç›¸å…³å—"""
        try:
            print(f"ğŸ“„ å¼€å§‹å¢å¼ºå—æœç´¢ï¼Œç›¸å…³æ–‡æ¡£IDæ•°é‡: {len(relevant_doc_ids)}")
            
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“å’Œå…³é”®è¯
            entities = enhanced_info.get("entities", {})
            keywords = enhanced_info.get("keywords", [])
            
            # æ„å»ºæœç´¢å…³é”®è¯åˆ—è¡¨
            search_terms = []
            for entity_list in entities.values():
                search_terms.extend(entity_list)
            search_terms.extend([kw[0] for kw in keywords if kw[2] > 1.0])  # åªå–é‡è¦å…³é”®è¯
            
            print(f"ğŸ” æœç´¢å…³é”®è¯: {search_terms}")
            
            if self.enable_enhanced_second_layer:
                print("ğŸ”§ ä½¿ç”¨å®ä½“èšåˆæ£€ç´¢è¿›è¡Œç¬¬äºŒå±‚æ£€ç´¢")
                
                # 1. æ‰§è¡Œå¤§èŒƒå›´å‘é‡æ£€ç´¢è·å–å€™é€‰æ–‡æ¡£å—
                # å¤§å¹…å¢åŠ å€™é€‰æ•°é‡ï¼Œç¡®ä¿ä¸é—æ¼ç›¸å…³å—
                candidate_k = max(self.chunk_top_k * 10, 100)  # è‡³å°‘æ£€ç´¢100ä¸ªå€™é€‰
                candidate_results = self.chunk_vectorstore.similarity_search_with_score(
                    query, k=candidate_k
                )
                
                print(f"ğŸ“„ å‘é‡æ£€ç´¢å€™é€‰ç»“æœæ•°é‡: {len(candidate_results)}")
                
                # 2. å®ä½“èšåˆåˆ†æ - æŒ‰å®ä½“åˆ†ç»„æ”¶é›†æ‰€æœ‰ç›¸å…³å—
                entity_blocks = {}  # å®ä½“å -> ç›¸å…³å—åˆ—è¡¨
                keyword_matched_chunks = []
                other_relevant_chunks = []
                
                for doc, score in candidate_results:
                    doc_id = doc.metadata.get('doc_id', '')
                    
                    # å¦‚æœdoc_idä¸ºç©ºï¼Œå°è¯•ç”Ÿæˆ
                    if not doc_id or doc_id == 'N/A':
                        doc_id = self._generate_doc_id_for_chunk(doc, 0)
                    
                    source_doc_id = self._extract_source_doc_id(doc_id)
                    content = doc.page_content.lower()
                    
                    # æ£€æŸ¥æ˜¯å¦å±äºç›¸å…³æ–‡æ¡£
                    is_relevant_doc = self._is_doc_in_relevant_set(source_doc_id, relevant_doc_ids)
                    
                    # æ£€æŸ¥å…³é”®å­—åŒ¹é…å’Œå®ä½“è¯†åˆ«
                    keyword_matches = []
                    matched_entities = []
                    keyword_score = 0
                    
                    for term in search_terms:
                        if term.lower() in content:
                            keyword_matches.append(term)
                            matched_entities.append(term)
                            # è®¡ç®—å…³é”®å­—åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®å’Œé¢‘æ¬¡
                            positions = self._find_keyword_positions(content, term.lower())
                            keyword_score += len(positions) * 0.1  # é¢‘æ¬¡åŠ åˆ†
                            
                            # å¦‚æœå…³é”®å­—åœ¨æ–‡æ¡£å¼€å¤´ï¼Œç»™äºˆé¢å¤–åŠ åˆ†
                            if positions and positions[0] < 100:
                                keyword_score += 0.2
                    
                    # ç»¼åˆè¯„åˆ†ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®å­—åŒ¹é… + æ–‡æ¡£ç›¸å…³æ€§
                    final_score = score
                    if keyword_matches:
                        final_score = score * 0.6 + keyword_score * 0.4  # å¢åŠ å…³é”®å­—åŒ¹é…æƒé‡
                        
                    if is_relevant_doc:
                        final_score *= 0.7  # ç›¸å…³æ–‡æ¡£ç»™äºˆæ›´é«˜ä¼˜å…ˆçº§ï¼ˆåˆ†æ•°è¶Šä½è¶Šå¥½ï¼‰
                    
                    chunk_info = {
                        'doc': doc,
                        'score': final_score,
                        'vector_score': score,
                        'keyword_score': keyword_score,
                        'keyword_matches': keyword_matches,
                        'matched_entities': matched_entities,
                        'is_relevant_doc': is_relevant_doc,
                        'doc_id': doc_id,
                        'source_doc_id': source_doc_id
                    }
                    
                    # æŒ‰å®ä½“åˆ†ç»„æ”¶é›†å—
                    for entity in matched_entities:
                        if entity not in entity_blocks:
                            entity_blocks[entity] = []
                        entity_blocks[entity].append(chunk_info)
                    
                    if keyword_matches:
                        keyword_matched_chunks.append(chunk_info)
                        print(f"âœ… å…³é”®å­—åŒ¹é…å—: doc_id={doc_id}, åŒ¹é…è¯={keyword_matches}, ç»¼åˆåˆ†æ•°={final_score:.4f}")
                    elif is_relevant_doc:
                        other_relevant_chunks.append(chunk_info)
                        print(f"ğŸ“‹ æ–‡æ¡£ç›¸å…³å—: doc_id={doc_id}, å‘é‡åˆ†æ•°={score:.4f}")
                
                # 3. å®ä½“èšåˆç­–ç•¥ - ç¡®ä¿æ¯ä¸ªåŒ¹é…å®ä½“çš„æ‰€æœ‰ç›¸å…³å—éƒ½è¢«åŒ…å«
                final_chunks = []
                used_doc_ids = set()
                
                print(f"ğŸ¯ å®ä½“èšåˆåˆ†æï¼Œå‘ç°å®ä½“: {list(entity_blocks.keys())}")
                
                # ä¸ºæ¯ä¸ªåŒ¹é…çš„å®ä½“æ”¶é›†æ‰€æœ‰ç›¸å…³å—
                for entity, chunks in entity_blocks.items():
                    print(f"ğŸ“Š å¤„ç†å®ä½“ '{entity}': æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³å—")
                    
                    # æŒ‰åˆ†æ•°æ’åºï¼Œä½†ç¡®ä¿åŒä¸€å®ä½“çš„å¤šä¸ªå—éƒ½è¢«åŒ…å«
                    chunks.sort(key=lambda x: x['score'])
                    
                    # ä¸ºæ¯ä¸ªå®ä½“è‡³å°‘ä¿ç•™å‰Nä¸ªæœ€ç›¸å…³çš„å—
                    entity_limit = min(len(chunks), max(3, self.chunk_top_k // len(entity_blocks)))
                    
                    for chunk in chunks[:entity_limit]:
                        if chunk['doc_id'] not in used_doc_ids:
                            final_chunks.append(chunk['doc'])
                            used_doc_ids.add(chunk['doc_id'])
                            print(f"  âœ… æ·»åŠ å®ä½“å—: {chunk['doc_id']}")
                
                # 4. å¦‚æœç»“æœä¸å¤Ÿï¼Œè¡¥å……å…¶ä»–å…³é”®å­—åŒ¹é…çš„å—
                if len(final_chunks) < self.chunk_top_k:
                    print(f"ğŸ“ ç»“æœä¸è¶³({len(final_chunks)})ï¼Œè¡¥å……å…¶ä»–å…³é”®å­—åŒ¹é…å—")
                    keyword_matched_chunks.sort(key=lambda x: x['score'])
                    
                    for chunk in keyword_matched_chunks:
                        if len(final_chunks) >= self.chunk_top_k:
                            break
                        if chunk['doc_id'] not in used_doc_ids:
                            final_chunks.append(chunk['doc'])
                            used_doc_ids.add(chunk['doc_id'])
                            print(f"  âœ… è¡¥å……å…³é”®å­—å—: {chunk['doc_id']}")
                
                # 5. å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œè¡¥å……ç›¸å…³æ–‡æ¡£çš„å—
                if len(final_chunks) < self.chunk_top_k:
                    print(f"ğŸ“‹ ç»“æœä»ä¸è¶³({len(final_chunks)})ï¼Œè¡¥å……ç›¸å…³æ–‡æ¡£å—")
                    other_relevant_chunks.sort(key=lambda x: x['score'])
                    
                    for chunk in other_relevant_chunks:
                        if len(final_chunks) >= self.chunk_top_k:
                            break
                        if chunk['doc_id'] not in used_doc_ids:
                            final_chunks.append(chunk['doc'])
                            used_doc_ids.add(chunk['doc_id'])
                            print(f"  âœ… è¡¥å……ç›¸å…³å—: {chunk['doc_id']}")
                
                # 6. ä¸ºæ¯ä¸ªé€‰ä¸­çš„å—æ·»åŠ å…³é”®å­—é«˜äº®ä¿¡æ¯
                for i, doc in enumerate(final_chunks):
                    highlighted_content = self._highlight_keywords_in_content(
                        doc.page_content, search_terms
                    )
                    # å°†é«˜äº®ä¿¡æ¯æ·»åŠ åˆ°metadataä¸­
                    doc.metadata['highlighted_content'] = highlighted_content
                    doc.metadata['search_terms'] = search_terms
                
                print(f"ğŸ¯ å®ä½“èšåˆæ£€ç´¢å®Œæˆï¼Œè¿”å›æ–‡æ¡£å—æ•°é‡: {len(final_chunks)}")
                print(f"ğŸ“Š å®ä½“åˆ†å¸ƒ: {[(entity, len(chunks)) for entity, chunks in entity_blocks.items()]}")
                return final_chunks
                
            else:
                print("âš ï¸ å¢å¼ºæ£€ç´¢æœªå¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†å—æœç´¢")
                return self._standard_chunk_search(query, enhanced_info, relevant_doc_ids)
                
        except Exception as e:
            print(f"âŒ å¢å¼ºå—æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _find_keyword_positions(self, content: str, keyword: str) -> List[int]:
        """æŸ¥æ‰¾å…³é”®å­—åœ¨å†…å®¹ä¸­çš„ä½ç½®"""
        positions = []
        start = 0
        while True:
            pos = content.find(keyword, start)
            if pos == -1:
                break
            positions.append(pos)
            start = pos + 1
        return positions
    
    def _highlight_keywords_in_content(self, content: str, search_terms: List[str]) -> str:
        """åœ¨å†…å®¹ä¸­é«˜äº®å…³é”®å­—"""
        highlighted_content = content
        for term in search_terms:
            if term.lower() in content.lower():
                # ä½¿ç”¨ç®€å•çš„æ ‡è®°æ–¹å¼é«˜äº®
                highlighted_content = highlighted_content.replace(
                    term, f"**{term}**"
                )
        return highlighted_content

    def _extract_source_doc_id(self, doc_id: str) -> str:
        """ä»å—æ–‡æ¡£IDä¸­æå–æºæ–‡æ¡£ID"""
        if not doc_id:
            return ""
        
        # å¤„ç†ç±»ä¼¼ "ä¿¡æ¯ç§‘ï¼ˆæŠ•è¯‰ï¼‰_row_28" çš„æ ¼å¼
        if "_row_" in doc_id:
            parts = doc_id.split("_row_")
            if len(parts) >= 2:
                return parts[0]
        
        # å¤„ç†å…¶ä»–æ ¼å¼
        return doc_id
    
    def _is_doc_in_relevant_set(self, source_doc_id: str, relevant_doc_ids: set) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦åœ¨ç›¸å…³æ–‡æ¡£é›†åˆä¸­"""
        if not source_doc_id:
            return False
        
        # æ ‡å‡†åŒ–è·¯å¾„æ ¼å¼
        normalized_source = source_doc_id.replace('\\', '/').lower()
        
        for relevant_id in relevant_doc_ids:
            normalized_relevant = relevant_id.replace('\\', '/').lower()
            
            # 1. ç²¾ç¡®åŒ¹é…
            if normalized_source == normalized_relevant:
                return True
            
            # 2. åŒ…å«åŒ¹é…
            if normalized_source in normalized_relevant or normalized_relevant in normalized_source:
                return True
            
            # 3. æ–‡ä»¶ååŒ¹é…ï¼ˆå»æ‰è·¯å¾„å‰ç¼€ï¼‰
            source_basename = normalized_source.split('/')[-1]
            relevant_basename = normalized_relevant.split('/')[-1]
            
            if source_basename == relevant_basename:
                return True
            
            # 4. åŸºç¡€æ–‡ä»¶ååŒ¹é…ï¼ˆå»æ‰_row_éƒ¨åˆ†ï¼‰
            if '_row_' in source_basename and '_row_' in relevant_basename:
                source_base = source_basename.split('_row_')[0]
                relevant_base = relevant_basename.split('_row_')[0]
                if source_base == relevant_base:
                    return True
        
        return False
    
    def _standard_chunk_search(self, query: str, enhanced_info: Dict, relevant_doc_ids: set) -> List[Document]:
        """æ ‡å‡†å—æœç´¢æ–¹æ³•"""
        try:
            print("ğŸ”§ ä½¿ç”¨æ ‡å‡†å‘é‡æ£€ç´¢")
            enhanced_results = self.chunk_vectorstore.similarity_search_with_score(
                query, k=self.chunk_top_k * 3
            )
            
            print(f"ğŸ“„ æ ‡å‡†å—æœç´¢åŸå§‹ç»“æœæ•°é‡: {len(enhanced_results)}")
            
            # è¿‡æ»¤å¹¶æ’åºç»“æœ
            filtered_docs = []
            high_score_docs = []
            entity_matched_docs = []
            
            # æå–å®ä½“ç”¨äºåŒ¹é…
            entities = enhanced_info.get("entities", {})
            all_entities = []
            for entity_list in entities.values():
                all_entities.extend(entity_list)
            
            for i, (doc, score) in enumerate(enhanced_results):
                # æ£€æŸ¥åˆ†æ•°é˜ˆå€¼
                if score > self.chunk_score_threshold * 2:  # æ”¾å®½é˜ˆå€¼
                    continue
                
                doc_id = doc.metadata.get('doc_id')
                
                # å¦‚æœæ–‡æ¡£æ²¡æœ‰doc_idï¼Œå°è¯•ç”Ÿæˆä¸€ä¸ª
                if not doc_id:
                    doc_id = self._generate_doc_id_for_chunk(doc, i)
                
                # 1. ç²¾ç¡®æ–‡æ¡£IDåŒ¹é…
                if doc_id in relevant_doc_ids:
                    filtered_docs.append((doc, score))
                    print(f"âœ… å—é€šè¿‡ç²¾ç¡®åŒ¹é…: doc_id={doc_id}, score={score:.4f}")
                    continue
                
                # 2. æ¨¡ç³Šæ–‡æ¡£IDåŒ¹é…
                matched_id = self._fuzzy_match_doc_id(doc_id, relevant_doc_ids)
                if matched_id:
                    filtered_docs.append((doc, score))
                    print(f"âœ… å—é€šè¿‡æ¨¡ç³ŠåŒ¹é…: doc_id={doc_id} â†’ {matched_id}, score={score:.4f}")
                    continue
                
                # 3. å®ä½“å†…å®¹åŒ¹é…
                if self.enable_entity_matching and all_entities:
                    doc_content = doc.page_content.lower()
                    entity_matched = False
                    for entity in all_entities:
                        if entity.lower() in doc_content:
                            entity_matched_docs.append((doc, score, entity))
                            print(f"âœ… å—é€šè¿‡å®ä½“åŒ¹é…: entity='{entity}', score={score:.4f}")
                            entity_matched = True
                            break
                    if entity_matched:
                        continue
                
                # 4. æ”¶é›†é«˜åˆ†æ–‡æ¡£
                if score <= 1.5:  # ç›¸å¯¹è¾ƒé«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°
                    high_score_docs.append((doc, score, doc_id))
            
            # åˆå¹¶ç»“æœ
            final_docs = []
            
            # æ·»åŠ ç²¾ç¡®åŒ¹é…çš„æ–‡æ¡£
            filtered_docs.sort(key=lambda x: x[1])
            final_docs.extend([doc for doc, _ in filtered_docs[:self.chunk_top_k]])
            
            # å¦‚æœç»“æœä¸å¤Ÿï¼Œæ·»åŠ å®ä½“åŒ¹é…çš„æ–‡æ¡£
            if len(final_docs) < self.chunk_top_k and entity_matched_docs:
                entity_matched_docs.sort(key=lambda x: x[1])
                needed = self.chunk_top_k - len(final_docs)
                for doc, score, entity in entity_matched_docs[:needed]:
                    final_docs.append(doc)
                    print(f"ğŸ”„ æ·»åŠ å®ä½“åŒ¹é…æ–‡æ¡£: entity='{entity}', score={score:.4f}")
            
            # å¦‚æœç»“æœä»ç„¶ä¸å¤Ÿï¼Œæ·»åŠ é«˜åˆ†æ–‡æ¡£
            if len(final_docs) < self.chunk_top_k and high_score_docs:
                high_score_docs.sort(key=lambda x: x[1])
                needed = self.chunk_top_k - len(final_docs)
                for doc, score, doc_id in high_score_docs[:needed]:
                    final_docs.append(doc)
                    print(f"ğŸ”„ æ·»åŠ é«˜åˆ†ç›¸å…³æ–‡æ¡£: doc_id={doc_id}, score={score:.4f}")
            
            print(f"ğŸ¯ æ ‡å‡†å—æœç´¢æœ€ç»ˆè¿”å›æ–‡æ¡£æ•°é‡: {len(final_docs)}")
            return final_docs
            
        except Exception as e:
            print(f"âŒ æ ‡å‡†å—æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _generate_doc_id_for_chunk(self, doc: Document, index: int) -> str:
        """ä¸ºå—æ–‡æ¡£ç”Ÿæˆdoc_id"""
        source = doc.metadata.get('source', f'doc_{index}')
        row = doc.metadata.get('row', index)
        
        # å¯¹äºExcelç­‰ç»“æ„åŒ–æ•°æ®ï¼Œä½¿ç”¨æ–‡ä»¶å+è¡Œå·ä½œä¸ºå”¯ä¸€ID
        if 'row' in doc.metadata:
            base_name = source.split('/')[-1].split('.')[0] if '/' in source or '.' in source else source
            # å¤„ç†è·¯å¾„åˆ†éš”ç¬¦
            base_name = base_name.replace('\\', '/').split('/')[-1]
            return f"{base_name}_row_{row}"
        else:
            # å¯¹äºå…¶ä»–æ–‡æ¡£ï¼Œä½¿ç”¨æ–‡ä»¶å+ç´¢å¼•
            base_name = source.split('/')[-1].split('.')[0] if '/' in source or '.' in source else source
            base_name = base_name.replace('\\', '/').split('/')[-1]
            return f"{base_name}_{index}"
    
    def _fuzzy_match_doc_id(self, doc_id: str, relevant_doc_ids: set) -> str:
        """æ¨¡ç³ŠåŒ¹é…doc_id"""
        if not doc_id or not relevant_doc_ids:
            return None
        
        # æå–åŸºç¡€åç§°ï¼ˆå»æ‰è·¯å¾„å‰ç¼€ï¼‰
        base_doc_id = doc_id.replace('\\', '/').split('/')[-1]
        
        for relevant_id in relevant_doc_ids:
            base_relevant_id = relevant_id.replace('\\', '/').split('/')[-1]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„åŸºç¡€åç§°
            if base_doc_id == base_relevant_id:
                return relevant_id
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€æ–‡ä»¶çš„ä¸åŒéƒ¨åˆ†
            if '_row_' in base_doc_id and '_row_' in base_relevant_id:
                doc_file_part = base_doc_id.split('_row_')[0]
                relevant_file_part = base_relevant_id.split('_row_')[0]
                if doc_file_part == relevant_file_part:
                    return relevant_id
        
        return None

    def _enhanced_simple_search(self, query: str, enhanced_info: Dict) -> List[Document]:
        """å¢å¼ºç®€å•æœç´¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            print(f"ğŸ”„ å¯ç”¨å¢å¼ºç®€å•æœç´¢å›é€€æ–¹æ¡ˆï¼ŒæŸ¥è¯¢: '{query}'")
            
            if not self.chunk_vectorstore:
                print("âŒ å—å‘é‡å­˜å‚¨ä¸å­˜åœ¨")
                return []
            
            # ä½¿ç”¨å¢å¼ºæ£€ç´¢æ–¹æ³•
            if self.keyword_processor and self.enable_enhanced_second_layer:
                print("ğŸ”§ ä½¿ç”¨å…³é”®è¯å¤„ç†å™¨è¿›è¡Œå¢å¼ºæ£€ç´¢")
                enhanced_results = self.keyword_processor.enhanced_search_with_scores(
                    query, self.chunk_vectorstore
                )
            else:
                print("ğŸ”§ ä½¿ç”¨æ ‡å‡†å‘é‡æ£€ç´¢")
                enhanced_results = self.chunk_vectorstore.similarity_search_with_score(
                    query, k=self.chunk_top_k * 2
                )
            
            print(f"ğŸ“„ å¢å¼ºç®€å•æœç´¢ç»“æœæ•°é‡: {len(enhanced_results)}")
            
            # è¿‡æ»¤ç»“æœ
            filtered_docs = []
            entities = enhanced_info.get("entities", {})
            all_entities = []
            for entity_list in entities.values():
                all_entities.extend(entity_list)
            
            for i, (doc, score) in enumerate(enhanced_results):
                # åŸºæœ¬åˆ†æ•°è¿‡æ»¤
                if score <= self.chunk_score_threshold * 1.5:  # æ”¾å®½é˜ˆå€¼
                    filtered_docs.append(doc)
                    print(f"âœ… å¢å¼ºç®€å•æœç´¢é€šè¿‡: score={score:.4f}, doc_id={doc.metadata.get('doc_id', 'N/A')}")
                # å®ä½“åŒ¹é…è¿‡æ»¤
                elif self.enable_entity_matching and all_entities:
                    doc_content = doc.page_content.lower()
                    for entity in all_entities:
                        if entity.lower() in doc_content:
                            filtered_docs.append(doc)
                            print(f"âœ… å®ä½“åŒ¹é…é€šè¿‡: entity='{entity}', score={score:.4f}")
                            break
            
            final_docs = filtered_docs[:self.chunk_top_k]
            print(f"ğŸ¯ å¢å¼ºç®€å•æœç´¢æœ€ç»ˆè¿”å›æ–‡æ¡£æ•°é‡: {len(final_docs)}")
            return final_docs
            
        except Exception as e:
            print(f"å¢å¼ºç®€å•æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _simple_search(self, query: str) -> List[Document]:
        """ç®€å•æœç´¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            print(f"ğŸ”„ å¯ç”¨ç®€å•æœç´¢å›é€€æ–¹æ¡ˆï¼ŒæŸ¥è¯¢: '{query}'")
            if self.chunk_vectorstore:
                try:
                    docs = self.chunk_vectorstore.similarity_search_with_score(
                        query, k=self.chunk_top_k
                    )
                except Exception as e:
                    print(f"âŒ ç®€å•æœç´¢å‘é‡å­˜å‚¨æœç´¢å¤±è´¥: {e}")
                    return []
                print(f"ğŸ“„ ç®€å•æœç´¢ç»“æœæ•°é‡: {len(docs)}")
                
                filtered_docs = []
                for i, (doc, score) in enumerate(docs):
                    if score >= self.chunk_score_threshold:
                        filtered_docs.append(doc)
                        print(f"âœ… ç®€å•æœç´¢é€šè¿‡: score={score:.4f}, doc_id={doc.metadata.get('doc_id', 'N/A')}, content_preview={doc.page_content[:50]}...")
                    else:
                        print(f"âŒ ç®€å•æœç´¢æœªé€šè¿‡: score={score:.4f} < {self.chunk_score_threshold}")
                
                print(f"ğŸ¯ ç®€å•æœç´¢æœ€ç»ˆè¿”å›æ–‡æ¡£æ•°é‡: {len(filtered_docs)}")
                return filtered_docs
            else:
                print("âŒ å—å‘é‡å­˜å‚¨ä¸å­˜åœ¨")
            return []
        except Exception as e:
            print(f"ç®€å•æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _is_same_document_group(self, doc_id: str, relevant_doc_ids: set) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å±äºåŒä¸€æ–‡æ¡£ç»„ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰"""
        if not doc_id or not relevant_doc_ids:
            return False
        
        # æå–åŸºç¡€åç§°ï¼ˆå»æ‰è·¯å¾„å‰ç¼€ï¼‰
        base_doc_id = doc_id.replace('\\', '/').split('/')[-1]
        
        # æå–æ–‡ä»¶åéƒ¨åˆ†ï¼ˆå»æ‰è¡Œå·ï¼‰
        if '_row_' in base_doc_id:
            doc_file_part = base_doc_id.split('_row_')[0]
            try:
                doc_row = int(base_doc_id.split('_row_')[1])
            except (ValueError, IndexError):
                return False
        else:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒä¸€æ–‡ä»¶çš„å…¶ä»–è¡Œ
        for relevant_id in relevant_doc_ids:
            base_relevant_id = relevant_id.replace('\\', '/').split('/')[-1]
            
            if '_row_' in base_relevant_id:
                relevant_file_part = base_relevant_id.split('_row_')[0]
                try:
                    relevant_row = int(base_relevant_id.split('_row_')[1])
                except (ValueError, IndexError):
                    continue
                
                # å¦‚æœæ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼Œä¸”è¡Œå·ç›¸è¿‘ï¼ˆå·®è·åœ¨5è¡Œä»¥å†…ï¼‰ï¼Œè®¤ä¸ºæ˜¯ç›¸å…³çš„
                if (doc_file_part == relevant_file_part and 
                    abs(doc_row - relevant_row) <= 5):
                    print(f"ğŸ”— å‘ç°åŒç»„æ–‡æ¡£: {doc_id} ä¸ {relevant_id} (è¡Œå·å·®è·: {abs(doc_row - relevant_row)})")
                    return True
        
        return False


class HierarchicalIndexBuilder:
    """åˆ†å±‚ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, embeddings: Embeddings):
        self.embeddings = embeddings
        self.enhanced_tokenizer = EnhancedTokenizer()
    
    def build_hierarchical_index(
        self,
        documents: List[Document],
        vectorstore_path: str,
        **kwargs
    ) -> Tuple[VectorStore, VectorStore]:
        """æ„å»ºåˆ†å±‚ç´¢å¼• - ä»…ä½¿ç”¨FAISS"""
        try:
            from langchain_community.vectorstores import FAISS
            
            print("ğŸ”§ ä½¿ç”¨FAISSæ„å»ºåˆ†å±‚ç´¢å¼•...")
            print(f"ğŸ“„ è¾“å…¥æ–‡æ¡£æ•°é‡: {len(documents)}")
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸ºç©º
            if not documents:
                raise ValueError("æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ„å»ºåˆ†å±‚ç´¢å¼•")
            
            # åˆ›å»ºæ‘˜è¦æ–‡æ¡£
            summary_docs = self._create_summary_documents(documents)
            print(f"ğŸ“‹ ç”Ÿæˆæ‘˜è¦æ–‡æ¡£æ•°é‡: {len(summary_docs)}")
            
            # æ£€æŸ¥æ‘˜è¦æ–‡æ¡£æ˜¯å¦ä¸ºç©º
            if not summary_docs:
                raise ValueError("æ‘˜è¦æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ„å»ºåˆ†å±‚ç´¢å¼•")
            
            # æ„å»ºæ‘˜è¦å‘é‡å­˜å‚¨
            print("ğŸ“‹ æ„å»ºæ‘˜è¦å‘é‡å­˜å‚¨...")
            summary_vectorstore = FAISS.from_documents(
                documents=summary_docs,
                embedding=self.embeddings
            )
            
            # æ„å»ºå—å‘é‡å­˜å‚¨
            print("ğŸ“„ æ„å»ºå—å‘é‡å­˜å‚¨...")
            chunk_vectorstore = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )
            
            # ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜
            summary_path = os.path.join(vectorstore_path, '..', 'hierarchical_vector_store', 'summary_vector_store')
            chunk_path = os.path.join(vectorstore_path, '..', 'hierarchical_vector_store', 'chunk_vector_store')
            
            os.makedirs(os.path.dirname(summary_path), exist_ok=True)
            os.makedirs(os.path.dirname(chunk_path), exist_ok=True)
            
            print(f"ğŸ’¾ ä¿å­˜æ‘˜è¦å‘é‡å­˜å‚¨åˆ°: {summary_path}")
            summary_vectorstore.save_local(summary_path)
            
            print(f"ğŸ’¾ ä¿å­˜å—å‘é‡å­˜å‚¨åˆ°: {chunk_path}")
            chunk_vectorstore.save_local(chunk_path)
            
            print(f"âœ… åˆ†å±‚ç´¢å¼•å·²ä¿å­˜åˆ°: {os.path.dirname(summary_path)}")
            
            return summary_vectorstore, chunk_vectorstore
            
        except Exception as e:
            print(f"âŒ æ„å»ºåˆ†å±‚ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def _create_summary_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ›å»ºæ‘˜è¦æ–‡æ¡£ï¼ˆæ”¯æŒæ™ºèƒ½åˆå¹¶ç›¸ä¼¼æ–‡æ¡£ï¼‰"""
        print(f"ğŸ” å¼€å§‹åˆ›å»ºæ‘˜è¦æ–‡æ¡£ï¼Œè¾“å…¥æ–‡æ¡£æ•°é‡: {len(documents)}")
        
        if not documents:
            print("âš ï¸ è¾“å…¥æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        summary_docs = []
        doc_groups = {}
        
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ–‡æ¡£IDåˆ†ç»„
        for i, doc in enumerate(documents):
            doc_id = doc.metadata.get('doc_id')
            if not doc_id:
                # å¦‚æœæ²¡æœ‰doc_idï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå”¯ä¸€ID
                source = doc.metadata.get('source', f'doc_{i}')
                row = doc.metadata.get('row', i)
                
                # å¯¹äºExcelç­‰ç»“æ„åŒ–æ•°æ®ï¼Œä½¿ç”¨æ–‡ä»¶å+è¡Œå·ä½œä¸ºå”¯ä¸€ID
                if 'row' in doc.metadata:
                    base_name = source.split('/')[-1].split('.')[0] if '/' in source or '.' in source else source
                    doc_id = f"{base_name}_row_{row}"
                else:
                    # å¯¹äºå…¶ä»–æ–‡æ¡£ï¼Œä½¿ç”¨æ–‡ä»¶å+ç´¢å¼•
                    doc_id = f"{source.split('/')[-1].split('.')[0] if '/' in source or '.' in source else source}_{i}"
                
                print(f"âš ï¸ æ–‡æ¡£ {i} ç¼ºå°‘doc_idï¼Œä½¿ç”¨é»˜è®¤ID: {doc_id}")
            
            if doc_id not in doc_groups:
                doc_groups[doc_id] = []
            doc_groups[doc_id].append(doc)
        
        print(f"ğŸ“Š åˆå§‹æ–‡æ¡£åˆ†ç»„ç»“æœ: {len(doc_groups)} ä¸ªç»„")
        
        # ç¬¬äºŒæ­¥ï¼šæ£€æµ‹å¹¶åˆå¹¶ç›¸ä¼¼æ–‡æ¡£
        merged_groups = self._merge_similar_documents(doc_groups)
        print(f"ğŸ“Š åˆå¹¶åæ–‡æ¡£åˆ†ç»„ç»“æœ: {len(merged_groups)} ä¸ªç»„")
        
        # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ¯ä¸ªæ–‡æ¡£ç»„åˆ›å»ºæ‘˜è¦
        for group_id, doc_list in merged_groups.items():
            try:
                # åˆå¹¶æ–‡æ¡£å†…å®¹
                combined_content = '\n'.join([doc.page_content for doc in doc_list if doc.page_content])
                
                if not combined_content.strip():
                    print(f"âš ï¸ æ–‡æ¡£ç»„ {group_id} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # åˆ›å»ºæ™ºèƒ½æ‘˜è¦
                summary = self._create_intelligent_summary(combined_content, doc_list)
                
                # æå–å…³é”®è¯
                try:
                    keywords = self.enhanced_tokenizer.extract_keywords(combined_content)
                except Exception as e:
                    print(f"âš ï¸ å…³é”®è¯æå–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºå…³é”®è¯")
                    keywords = []
                
                # æ”¶é›†æ‰€æœ‰æºæ–‡ä»¶ä¿¡æ¯
                sources = list(set([doc.metadata.get('source', 'unknown') for doc in doc_list]))
                
                # åˆ›å»ºæ‘˜è¦æ–‡æ¡£
                summary_doc = Document(
                    page_content=summary,
                    metadata={
                        'doc_id': group_id,
                        'type': 'summary',
                        'keywords': str(keywords),
                        'chunk_count': len(doc_list),
                        'sources': sources,  # å¤šä¸ªæºæ–‡ä»¶
                        'source': sources[0] if sources else 'unknown',  # ä¸»è¦æºæ–‡ä»¶
                        'merged_docs': len([doc_id for doc_id in doc_groups.keys() if doc_id.startswith(group_id.split('_merged')[0])])
                    }
                )
                summary_docs.append(summary_doc)
                print(f"âœ… åˆ›å»ºæ‘˜è¦æ–‡æ¡£: {group_id} (å†…å®¹é•¿åº¦: {len(summary)}, åˆå¹¶äº† {len(doc_list)} ä¸ªå—)")
                
            except Exception as e:
                print(f"âŒ åˆ›å»ºæ‘˜è¦æ–‡æ¡£å¤±è´¥ {group_id}: {e}")
                continue
        
        print(f"ğŸ“‹ æœ€ç»ˆç”Ÿæˆæ‘˜è¦æ–‡æ¡£æ•°é‡: {len(summary_docs)}")
        return summary_docs
    
    def _merge_similar_documents(self, doc_groups: dict) -> dict:
        """åˆå¹¶ç›¸ä¼¼æ–‡æ¡£"""
        print("ğŸ”„ å¼€å§‹æ£€æµ‹å¹¶åˆå¹¶ç›¸ä¼¼æ–‡æ¡£...")
        
        merged_groups = {}
        processed_groups = set()
        
        group_items = list(doc_groups.items())
        
        for i, (doc_id1, docs1) in enumerate(group_items):
            if doc_id1 in processed_groups:
                continue
                
            # å½“å‰ç»„ä½œä¸ºåŸºç¡€ç»„
            current_group_id = doc_id1
            current_docs = docs1.copy()
            merged_doc_ids = [doc_id1]
            
            # æ£€æŸ¥æ˜¯å¦ä¸å…¶ä»–ç»„ç›¸ä¼¼
            for j, (doc_id2, docs2) in enumerate(group_items[i+1:], i+1):
                if doc_id2 in processed_groups:
                    continue
                    
                if self._are_documents_similar(docs1, docs2, doc_id1, doc_id2):
                    print(f"ğŸ”— åˆå¹¶ç›¸ä¼¼æ–‡æ¡£: {doc_id1} + {doc_id2}")
                    current_docs.extend(docs2)
                    merged_doc_ids.append(doc_id2)
                    processed_groups.add(doc_id2)
            
            # å¦‚æœåˆå¹¶äº†å¤šä¸ªæ–‡æ¡£ï¼Œæ›´æ–°ç»„ID
            if len(merged_doc_ids) > 1:
                current_group_id = f"{doc_id1}_merged_{len(merged_doc_ids)}"
                print(f"ğŸ“‹ åˆ›å»ºåˆå¹¶ç»„: {current_group_id} (åŒ…å«: {', '.join(merged_doc_ids)})")
            
            merged_groups[current_group_id] = current_docs
            processed_groups.add(doc_id1)
        
        return merged_groups
    
    def _are_documents_similar(self, docs1: List[Document], docs2: List[Document], 
                              doc_id1: str, doc_id2: str) -> bool:
        """åˆ¤æ–­ä¸¤ç»„æ–‡æ¡£æ˜¯å¦ç›¸ä¼¼"""
        
        # 1. æ£€æŸ¥æ–‡ä»¶åç›¸ä¼¼æ€§ï¼ˆå¤„ç†"å‰ä»¶ä½œåºŸ"ç­‰æƒ…å†µï¼‰
        if self._check_filename_similarity(doc_id1, doc_id2):
            return True
        
        # 2. æ£€æŸ¥å†…å®¹ç›¸ä¼¼æ€§
        content1 = ' '.join([doc.page_content for doc in docs1 if doc.page_content])
        content2 = ' '.join([doc.page_content for doc in docs2 if doc.page_content])
        
        if self._check_content_similarity(content1, content2):
            return True
        
        # 3. æ£€æŸ¥ä¸»é¢˜ç›¸ä¼¼æ€§
        if self._check_topic_similarity(content1, content2):
            return True
            
        return False
    
    def _check_filename_similarity(self, doc_id1: str, doc_id2: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶åç›¸ä¼¼æ€§"""
        # æå–åŸºç¡€æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
        name1 = doc_id1.split('/')[-1].split('.')[0].lower()
        name2 = doc_id2.split('/')[-1].split('.')[0].lower()
        
        # ç§»é™¤å¸¸è§çš„ç‰ˆæœ¬æ ‡è¯†è¯
        version_keywords = ['å‰ä»¶ä½œåºŸ', 'ä»¥æ­¤ä»¶ä¸ºå‡†', 'ä¿®è®¢ç‰ˆ', 'æœ€æ–°ç‰ˆ', 'æ›´æ–°ç‰ˆ', 'v1', 'v2', 'v3', 
                           'ç¬¬ä¸€ç‰ˆ', 'ç¬¬äºŒç‰ˆ', 'ç¬¬ä¸‰ç‰ˆ', 'åˆç¨¿', 'ç»ˆç¨¿', 'æ­£å¼ç‰ˆ']
        
        for keyword in version_keywords:
            name1 = name1.replace(keyword, '').strip()
            name2 = name2.replace(keyword, '').strip()
        
        # ç§»é™¤æ‹¬å·å†…å®¹ï¼ˆé€šå¸¸æ˜¯ç‰ˆæœ¬è¯´æ˜ï¼‰
        import re
        name1 = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', name1).strip()
        name2 = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', name2).strip()
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = self._calculate_string_similarity(name1, name2)
        
        if similarity > 0.8:  # 80%ä»¥ä¸Šç›¸ä¼¼åº¦è®¤ä¸ºæ˜¯åŒä¸€æ–‡æ¡£çš„ä¸åŒç‰ˆæœ¬
            print(f"ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶åç›¸ä¼¼: '{name1}' vs '{name2}' (ç›¸ä¼¼åº¦: {similarity:.2f})")
            return True
            
        return False
    
    def _check_content_similarity(self, content1: str, content2: str) -> bool:
        """æ£€æŸ¥å†…å®¹ç›¸ä¼¼æ€§"""
        if not content1 or not content2:
            return False
            
        # ç®€åŒ–çš„å†…å®¹ç›¸ä¼¼æ€§æ£€æŸ¥
        # æå–å‰200å­—ç¬¦è¿›è¡Œæ¯”è¾ƒ
        sample1 = content1[:200].lower()
        sample2 = content2[:200].lower()
        
        similarity = self._calculate_string_similarity(sample1, sample2)
        
        if similarity > 0.7:  # 70%ä»¥ä¸Šç›¸ä¼¼åº¦
            print(f"ğŸ“„ æ£€æµ‹åˆ°å†…å®¹ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {similarity:.2f})")
            return True
            
        return False
    
    def _check_topic_similarity(self, content1: str, content2: str) -> bool:
        """æ£€æŸ¥ä¸»é¢˜ç›¸ä¼¼æ€§"""
        try:
            # æå–å…³é”®è¯è¿›è¡Œä¸»é¢˜æ¯”è¾ƒ
            keywords1 = set(self.enhanced_tokenizer.extract_keywords(content1)[:10])
            keywords2 = set(self.enhanced_tokenizer.extract_keywords(content2)[:10])
            
            if keywords1 and keywords2:
                # è®¡ç®—å…³é”®è¯é‡å åº¦
                intersection = keywords1.intersection(keywords2)
                union = keywords1.union(keywords2)
                
                if union:
                    similarity = len(intersection) / len(union)
                    if similarity > 0.5:  # 50%ä»¥ä¸Šå…³é”®è¯é‡å 
                        print(f"ğŸ·ï¸ æ£€æµ‹åˆ°ä¸»é¢˜ç›¸ä¼¼ (å…³é”®è¯é‡å åº¦: {similarity:.2f})")
                        print(f"   å…±åŒå…³é”®è¯: {list(intersection)}")
                        return True
        except Exception as e:
            print(f"âš ï¸ ä¸»é¢˜ç›¸ä¼¼æ€§æ£€æŸ¥å¤±è´¥: {e}")
            
        return False
    
    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆç¼–è¾‘è·ç¦»ï¼‰"""
        if not str1 or not str2:
            return 0.0
            
        # ä½¿ç”¨ç®€åŒ–çš„Jaccardç›¸ä¼¼åº¦
        set1 = set(str1)
        set2 = set(str2)
        
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _create_intelligent_summary(self, combined_content: str, doc_list: List[Document]) -> str:
        """åˆ›å»ºæ™ºèƒ½æ‘˜è¦ï¼Œç¡®ä¿é‡è¦ä¿¡æ¯ä¸ä¸¢å¤±"""
        # å¦‚æœå†…å®¹è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
        if len(combined_content) <= 500:
            return combined_content
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæŠ•è¯‰ç±»æ–‡æ¡£
        is_complaint_doc = any('æŠ•è¯‰' in doc.metadata.get('source', '') for doc in doc_list)
        
        if is_complaint_doc:
            # å¯¹äºæŠ•è¯‰ç±»æ–‡æ¡£ï¼Œä½¿ç”¨ä¸“é—¨çš„æ‘˜è¦ç”Ÿæˆé€»è¾‘
            return self._create_complaint_summary(combined_content, doc_list)
        
        # å¯¹äºå…¶ä»–ç±»å‹æ–‡æ¡£ï¼Œä½¿ç”¨é€šç”¨æ‘˜è¦é€»è¾‘
        lines = combined_content.split('\n')
        
        # æå–é‡è¦ä¿¡æ¯
        important_lines = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®ä¿¡æ¯çš„è¡Œ
            if any(keyword in line for keyword in ['é€šæŠ¥', 'é€šçŸ¥', 'äº‹ä»¶', 'æƒ…å†µ', 'è¦æ±‚', 'æªæ–½', 'å¤„ç†']):
                important_lines.append(line)
            elif len(line) > 20:  # ä¿ç•™è¾ƒé•¿çš„æè¿°æ€§å†…å®¹
                important_lines.append(line)
                
            # é™åˆ¶æ‘˜è¦é•¿åº¦
            if len('\n'.join(important_lines)) > 400:
                break
        
        summary = '\n'.join(important_lines)
        
        # å¦‚æœåˆå¹¶äº†å¤šä¸ªæ–‡æ¡£ï¼Œæ·»åŠ è¯´æ˜
        if len(doc_list) > 1:
            sources = list(set([doc.metadata.get('source', '').split('/')[-1] for doc in doc_list]))
            summary += f"\n\n[åˆå¹¶æ–‡æ¡£: {', '.join(sources[:3])}{'ç­‰' if len(sources) > 3 else ''}]"
        
        return summary
    
    def _create_complaint_summary(self, combined_content: str, doc_list: List[Document]) -> str:
        """ä¸ºæŠ•è¯‰ç±»æ–‡æ¡£åˆ›å»ºä¸“é—¨çš„æ‘˜è¦"""
        # æå–å…³é”®ä¿¡æ¯
        people_names = set()
        companies = set()
        problem_types = set()
        locations = set()
        complaint_contents = []
        
        lines = combined_content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æå–äººå‘˜å§“å
            if line.startswith('æä¾›æ–¹å§“å:'):
                name = line.split(':', 1)[1].strip()
                if name and name != '':
                    people_names.add(name)
            
            # æå–ä¼ä¸šåç§°
            elif line.startswith('ä¼ä¸šåç§°:'):
                company = line.split(':', 1)[1].strip()
                if company and company != '':
                    companies.add(company)
            
            # æå–é—®é¢˜ç±»åˆ«
            elif line.startswith('é—®é¢˜ç±»åˆ«:'):
                problem = line.split(':', 1)[1].strip()
                if problem and problem != '':
                    problem_types.add(problem)
            
            # æå–äº‹å‘åœ°
            elif line.startswith('äº‹å‘åœ°:'):
                location = line.split(':', 1)[1].strip()
                if location and location != '':
                    locations.add(location)
            
            # æå–å…·ä½“é—®é¢˜æè¿°
            elif line.startswith('å…·ä½“é—®é¢˜:'):
                content = line.split(':', 1)[1].strip()
                if content and content != '' and len(content) > 10:
                    complaint_contents.append(content[:200] + '...' if len(content) > 200 else content)
        
        # æ„å»ºç»“æ„åŒ–æ‘˜è¦
        summary_parts = []
        
        # æ·»åŠ äººå‘˜ä¿¡æ¯ï¼ˆæœ€é‡è¦ï¼‰
        if people_names:
            summary_parts.append(f"æŠ•è¯‰äººå‘˜: {', '.join(people_names)}")
        
        # æ·»åŠ ä¼ä¸šä¿¡æ¯
        if companies:
            company_list = list(companies)[:3]  # åªå–å‰3ä¸ª
            summary_parts.append(f"æ¶‰åŠä¼ä¸š: {', '.join(company_list)}")
        
        # æ·»åŠ é—®é¢˜ç±»å‹
        if problem_types:
            problem_list = list(problem_types)[:3]  # åªå–å‰3ä¸ª
            summary_parts.append(f"é—®é¢˜ç±»å‹: {', '.join(problem_list)}")
        
        # æ·»åŠ äº‹å‘åœ°
        if locations:
            summary_parts.append(f"äº‹å‘åœ°åŒº: {', '.join(locations)}")
        
        # æ·»åŠ æŠ•è¯‰å†…å®¹ç¤ºä¾‹
        if complaint_contents:
            summary_parts.append(f"æŠ•è¯‰å†…å®¹ç¤ºä¾‹: {complaint_contents[0]}")
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        summary_parts.append(f"æŠ•è¯‰è®°å½•æ•°é‡: {len(doc_list)}")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å†…å®¹çš„å‰éƒ¨åˆ†
        if not summary_parts:
            summary_parts.append(combined_content[:400] + '...' if len(combined_content) > 400 else combined_content)
        
        return '\n'.join(summary_parts)