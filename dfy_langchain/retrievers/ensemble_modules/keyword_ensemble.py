from __future__ import annotations

import jieba
import jieba.posseg as pseg
import re
from ..utils.enhanced_tokenizer import get_enhanced_tokenizer
from langchain.retrievers import EnsembleRetriever
from langchain.vectorstores import VectorStore
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever

from ..base import BaseRetrieverService
from ..utils.relevance_ranking import RelevanceRanker


class KeywordEnsembleRetrieverService(BaseRetrieverService):
    """
    å…³é”®è¯åŒ¹é…é¢„ç­›é€‰ + ç»„åˆæ£€ç´¢çš„æ£€ç´¢å™¨æœåŠ¡
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨jiebaåˆ†è¯å¯¹æŸ¥è¯¢è¿›è¡Œå¤„ç†ï¼Œæå–å…³é”®è¯
    2. ä½¿ç”¨å…³é”®è¯åœ¨æ–‡æ¡£ä¸­è¿›è¡ŒåŒ¹é…ï¼Œç­›é€‰å‡ºåŒ…å«å…³é”®è¯çš„æ–‡æ¡£
    3. å¯¹ç­›é€‰å‡ºçš„æ–‡æ¡£è¿›è¡Œå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢ï¼Œå¹¶ç»„åˆç»“æœ
    4. å¢å¼ºè¿”å›ç»“æœï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    5. ä½¿ç”¨ç›¸å…³æ€§æ’åºå¯¹ç»“æœè¿›è¡Œé‡æ–°æ’åº
    """
    
    def do_init(
        self,
        retriever: BaseRetriever = None,
        vectorstore: VectorStore = None,
        top_k: int = 5,
        keyword_match_threshold: int = 1,  # è‡³å°‘åŒ…å«å¤šå°‘ä¸ªå…³é”®è¯æ‰ç®—åŒ¹é…
        context_window: int = 50,  # ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        enable_ranking: bool = True,  # æ˜¯å¦å¯ç”¨ç›¸å…³æ€§æ’åº
        weight_keyword_freq: float = 0.4,  # å…³é”®è¯é¢‘ç‡æƒé‡
        weight_keyword_pos: float = 0.3,  # å…³é”®è¯ä½ç½®æƒé‡
        weight_keyword_coverage: float = 0.3,  # å…³é”®è¯è¦†ç›–åº¦æƒé‡
    ):
        self.vs = vectorstore
        self.top_k = top_k
        self.retriever = retriever
        self.keyword_match_threshold = keyword_match_threshold
        self.context_window = context_window
        self.enable_ranking = enable_ranking
        self.all_docs = list(vectorstore.docstore._dict.values()) if vectorstore else []
        # åˆ›å»ºæ–‡æ¡£IDåˆ°åŸå§‹æ–‡æ¡£çš„æ˜ å°„ï¼Œç”¨äºåç»­è·å–ä¸Šä¸‹æ–‡
        self.doc_id_map = {doc.metadata.get('source', ''): doc for doc in self.all_docs}
        # åˆ›å»ºç›¸å…³æ€§æ’åºå™¨
        self.ranker = RelevanceRanker(
            weight_keyword_freq=weight_keyword_freq,
            weight_keyword_pos=weight_keyword_pos,
            weight_keyword_coverage=weight_keyword_coverage
        )
    
    @staticmethod
    def from_vectorstore(
        vectorstore: VectorStore,
        top_k: int,
        score_threshold: int | float,
        keyword_match_threshold: int = 1,
        context_window: int = 100,
        enable_ranking: bool = True,
        weight_keyword_freq: float = 0.4,
        weight_keyword_pos: float = 0.3,
        weight_keyword_coverage: float = 0.3,
    ):
        # åˆ›å»ºFAISSæ£€ç´¢å™¨
        faiss_retriever = vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"score_threshold": score_threshold, "k": top_k},
        )
        
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = list(vectorstore.docstore._dict.values())
        
        # åˆ›å»ºBM25æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever.from_documents(
            all_docs,
            preprocess_func=jieba.lcut_for_search,
        )
        bm25_retriever.k = top_k
        
        # åˆ›å»ºç»„åˆæ£€ç´¢å™¨
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
        )
        
        return KeywordEnsembleRetrieverService(
            retriever=ensemble_retriever, 
            vectorstore=vectorstore,
            top_k=top_k,
            keyword_match_threshold=keyword_match_threshold,
            context_window=context_window,
            enable_ranking=enable_ranking,
            weight_keyword_freq=weight_keyword_freq,
            weight_keyword_pos=weight_keyword_pos,
            weight_keyword_coverage=weight_keyword_coverage
        )
    
    def _extract_keywords(self, query: str, top_n: int = 5):
        """ä½¿ç”¨å¢å¼ºåˆ†è¯å™¨æå–å…³é”®è¯"""
        try:
            # è·å–å¢å¼ºåˆ†è¯å™¨
            tokenizer = get_enhanced_tokenizer()
            
            # æå–å…³é”®è¯ï¼ˆåªè¿”å›è¯æ±‡ï¼Œä¸åŒ…å«æƒé‡ï¼‰
            keyword_tuples = tokenizer.extract_keywords(query, top_k=20, min_word_len=2)
            keywords = [word for word, pos, weight in keyword_tuples]
            
            # æ·»åŠ ä¸“é—¨å®ä½“
            person_names = tokenizer.get_person_names(query)
            organizations = tokenizer.get_organizations(query)
            locations = tokenizer.get_locations(query)
            
            # åˆå¹¶æ‰€æœ‰å…³é”®è¯
            all_keywords = keywords + person_names + organizations + locations
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            unique_keywords = []
            seen = set()
            for keyword in all_keywords:
                if keyword not in seen:
                    unique_keywords.append(keyword)
                    seen.add(keyword)
            
            print(f"ğŸ¯ æå–çš„å…³é”®è¯: {unique_keywords}")
            final_keywords = unique_keywords[:top_n] if top_n and unique_keywords else unique_keywords
            return final_keywords if final_keywords else [query]
            
        except Exception as e:
            print(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            # å›é€€åˆ°åŸå§‹jiebaåˆ†è¯
            words = jieba.lcut_for_search(query)
            keywords = [word for word in words if len(word) > 1]
            return keywords[:top_n] if top_n else keywords
    
    def _filter_docs_by_keywords(self, keywords, docs, threshold=1):
        """æ ¹æ®å…³é”®è¯ç­›é€‰æ–‡æ¡£"""
        if not keywords:
            return docs
        
        filtered_docs = []
        for doc in docs:
            content = doc.page_content.lower()
            # è®¡ç®—æ–‡æ¡£ä¸­åŒ…å«çš„å…³é”®è¯æ•°é‡
            match_count = sum(1 for keyword in keywords if keyword.lower() in content)
            if match_count >= threshold:
                filtered_docs.append(doc)
        
        return filtered_docs
    
    def _enrich_context(self, doc: Document, keywords: list) -> Document:
        """
        å¢å¼ºæ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ å…³é”®è¯å‘¨å›´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # å¦‚æœcontext_windowä¸º0ï¼Œåˆ™ç¦ç”¨ä¸Šä¸‹æ–‡å¢å¼ºï¼Œç›´æ¥è¿”å›åŸæ–‡æ¡£
        if self.context_window <= 0:
            return doc
            
        source = doc.metadata.get('source', '')
        content = doc.page_content
        
        # å¦‚æœå†…å®¹å·²ç»å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›åŸæ–‡æ¡£
        if len(content) <= self.context_window * 3:
            return doc
        
        # æŸ¥æ‰¾æ‰€æœ‰å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        positions = []
        for keyword in keywords:
            # æ‰¾åˆ°æ‰€æœ‰å…³é”®è¯å‡ºç°çš„ä½ç½®
            for match in re.finditer(re.escape(keyword.lower()), content.lower()):
                positions.append((match.start(), match.end()))
        
        if not positions:
            return doc
        
        # æŒ‰ä½ç½®æ’åº
        positions.sort()
        
        # åˆå¹¶é‡å æˆ–æ¥è¿‘çš„ä½ç½®
        merged_positions = []
        current_start, current_end = positions[0]
        
        for start, end in positions[1:]:
            # å¦‚æœå½“å‰ä½ç½®ä¸ä¸Šä¸€ä¸ªä½ç½®çš„è·ç¦»å°äºä¸Šä¸‹æ–‡çª—å£çš„2å€ï¼Œåˆå¹¶å®ƒä»¬
            if start <= current_end + self.context_window * 2:
                current_end = max(current_end, end)
            else:
                merged_positions.append((current_start, current_end))
                current_start, current_end = start, end
        
        merged_positions.append((current_start, current_end))
        
        # ä¸ºæ¯ä¸ªåˆå¹¶åçš„ä½ç½®æå–ä¸Šä¸‹æ–‡
        contexts = []
        for start, end in merged_positions:
            # æ‰©å±•ä¸Šä¸‹æ–‡çª—å£
            context_start = max(0, start - self.context_window)
            context_end = min(len(content), end + self.context_window)
            
            # æå–ä¸Šä¸‹æ–‡
            context = content[context_start:context_end]
            
            # æ·»åŠ çœç•¥å·è¡¨ç¤ºä¸Šä¸‹æ–‡è¢«æˆªæ–­
            prefix = "..." if context_start > 0 else ""
            suffix = "..." if context_end < len(content) else ""
            
            contexts.append(f"{prefix}{context}{suffix}")
        
        # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡
        enriched_content = "\n...\n".join(contexts)
        
        # åˆ›å»ºæ–°çš„æ–‡æ¡£ï¼ŒåŒ…å«åŸå§‹å…ƒæ•°æ®å’Œå¢å¼ºçš„å†…å®¹
        enriched_doc = Document(
            page_content=enriched_content,
            metadata={
                **doc.metadata,
                "original_content": content[:100] + "..." if len(content) > 100 else content,
                "context_enriched": True
            }
        )
        
        return enriched_doc
    
    def get_relevant_documents(self, query: str):
        """
        å…ˆä½¿ç”¨å…³é”®è¯åŒ¹é…ç­›é€‰æ–‡æ¡£ï¼Œç„¶åå¯¹ç­›é€‰å‡ºçš„æ–‡æ¡£è¿›è¡Œç»„åˆæ£€ç´¢ï¼Œæœ€åå¢å¼ºä¸Šä¸‹æ–‡å¹¶è¿›è¡Œç›¸å…³æ€§æ’åº
        """
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(query)
        
        if not keywords:
            # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œç›´æ¥ä½¿ç”¨ç»„åˆæ£€ç´¢
            results = self.retriever.get_relevant_documents(query)[:self.top_k]
            
            # ä¸ºç»“æœå¢åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.context_window > 0:
                enriched_results = [self._enrich_context(doc, [query]) for doc in results]
            else:
                enriched_results = results
            
            # å¦‚æœå¯ç”¨äº†ç›¸å…³æ€§æ’åºï¼Œåˆ™å¯¹ç»“æœè¿›è¡Œæ’åº
            if self.enable_ranking:
                return self.ranker.rank_documents(enriched_results, query)
            else:
                return enriched_results
        
        # ä½¿ç”¨å…³é”®è¯ç­›é€‰æ–‡æ¡£
        filtered_docs = self._filter_docs_by_keywords(
            keywords, 
            self.all_docs, 
            self.keyword_match_threshold
        )
        
        if not filtered_docs:
            # å¦‚æœæ²¡æœ‰ç­›é€‰å‡ºæ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨ç»„åˆæ£€ç´¢
            results = self.retriever.get_relevant_documents(query)[:self.top_k]
            
            # ä¸ºç»“æœå¢åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.context_window > 0:
                enriched_results = [self._enrich_context(doc, [query]) for doc in results]
            else:
                enriched_results = results
            
            # å¦‚æœå¯ç”¨äº†ç›¸å…³æ€§æ’åºï¼Œåˆ™å¯¹ç»“æœè¿›è¡Œæ’åº
            if self.enable_ranking:
                return self.ranker.rank_documents(enriched_results, query)
            else:
                return enriched_results
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å‘é‡å­˜å‚¨ï¼ŒåªåŒ…å«ç­›é€‰å‡ºçš„æ–‡æ¡£
        filtered_vectorstore = self.vs.from_documents(
            filtered_docs,
            self.vs.embedding_function,
        )
        
        # å¯¹ç­›é€‰å‡ºçš„æ–‡æ¡£åˆ›å»ºæ£€ç´¢å™¨
        filtered_faiss_retriever = filtered_vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"score_threshold": 0.0, "k": self.top_k * 2},  # è®¾ç½®è¾ƒä½çš„é˜ˆå€¼ï¼Œç¡®ä¿èƒ½å¤Ÿè¿”å›ç»“æœ
        )
        
        # åˆ›å»ºBM25æ£€ç´¢å™¨
        filtered_bm25_retriever = BM25Retriever.from_documents(
            filtered_docs,
            preprocess_func=jieba.lcut_for_search,
        )
        filtered_bm25_retriever.k = self.top_k * 2
        
        # åˆ›å»ºç»„åˆæ£€ç´¢å™¨
        filtered_ensemble_retriever = EnsembleRetriever(
            retrievers=[filtered_bm25_retriever, filtered_faiss_retriever], 
            weights=[0.5, 0.5]
        )
        
        # ä½¿ç”¨ç»„åˆæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
        results = filtered_ensemble_retriever.get_relevant_documents(query)
        
        # ä¸ºæ¯ä¸ªç»“æœå¢å¼ºä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.context_window > 0:
            enriched_results = [self._enrich_context(doc, keywords) for doc in results[:self.top_k * 2]]
        else:
            enriched_results = results[:self.top_k * 2]
        
        # å¦‚æœå¯ç”¨äº†ç›¸å…³æ€§æ’åºï¼Œåˆ™å¯¹ç»“æœè¿›è¡Œæ’åº
        if self.enable_ranking:
            ranked_results = self.ranker.rank_documents(enriched_results, query)
            return ranked_results[:self.top_k]
        else:
            return enriched_results[:self.top_k]
    
    def get_relevant_documents_with_scores(self, query: str):
        """
        è·å–ç›¸å…³æ–‡æ¡£åŠå…¶ç›¸å…³æ€§åˆ†æ•°
        """
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(query)
        
        if not keywords:
            # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œç›´æ¥ä½¿ç”¨ç»„åˆæ£€ç´¢
            results = self.retriever.get_relevant_documents(query)[:self.top_k]
            
            # ä¸ºç»“æœå¢åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.context_window > 0:
                enriched_results = [self._enrich_context(doc, [query]) for doc in results]
            else:
                enriched_results = results
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            return self.ranker.rank_documents_with_scores(enriched_results, query)
        
        # ä½¿ç”¨å…³é”®è¯ç­›é€‰æ–‡æ¡£
        filtered_docs = self._filter_docs_by_keywords(
            keywords, 
            self.all_docs, 
            self.keyword_match_threshold
        )
        
        if not filtered_docs:
            # å¦‚æœæ²¡æœ‰ç­›é€‰å‡ºæ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨ç»„åˆæ£€ç´¢
            results = self.retriever.get_relevant_documents(query)[:self.top_k]
            
            # ä¸ºç»“æœå¢åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.context_window > 0:
                enriched_results = [self._enrich_context(doc, [query]) for doc in results]
            else:
                enriched_results = results
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            return self.ranker.rank_documents_with_scores(enriched_results, query)
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å‘é‡å­˜å‚¨ï¼ŒåªåŒ…å«ç­›é€‰å‡ºçš„æ–‡æ¡£
        filtered_vectorstore = self.vs.from_documents(
            filtered_docs,
            self.vs.embedding_function,
        )
        
        # å¯¹ç­›é€‰å‡ºçš„æ–‡æ¡£åˆ›å»ºæ£€ç´¢å™¨
        filtered_faiss_retriever = filtered_vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"score_threshold": 0.0, "k": self.top_k * 2},  # è®¾ç½®è¾ƒä½çš„é˜ˆå€¼ï¼Œç¡®ä¿èƒ½å¤Ÿè¿”å›ç»“æœ
        )
        
        # åˆ›å»ºBM25æ£€ç´¢å™¨
        filtered_bm25_retriever = BM25Retriever.from_documents(
            filtered_docs,
            preprocess_func=jieba.lcut_for_search,
        )
        filtered_bm25_retriever.k = self.top_k * 2
        
        # åˆ›å»ºç»„åˆæ£€ç´¢å™¨
        filtered_ensemble_retriever = EnsembleRetriever(
            retrievers=[filtered_bm25_retriever, filtered_faiss_retriever], 
            weights=[0.5, 0.5]
        )
        
        # ä½¿ç”¨ç»„åˆæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
        results = filtered_ensemble_retriever.invoke(query)
        
        # ä¸ºæ¯ä¸ªç»“æœå¢å¼ºä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.context_window > 0:
            enriched_results = [self._enrich_context(doc, keywords) for doc in results[:self.top_k * 2]]
        else:
            enriched_results = results[:self.top_k * 2]
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶æ’åº
        scored_results = self.ranker.rank_documents_with_scores(enriched_results, query)
        
        return scored_results[:self.top_k]