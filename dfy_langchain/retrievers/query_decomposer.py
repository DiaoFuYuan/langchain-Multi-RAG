#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨
å®ç°å¤æ‚æŸ¥è¯¢çš„æ™ºèƒ½åˆ†è§£ã€å®ä½“æå–å’Œå¤šè·¯æ£€ç´¢ç­–ç•¥
ç°åœ¨çœŸæ­£ä½¿ç”¨æ•°æ®åº“ä¸­é…ç½®çš„AIå¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ
"""

import json
import re
import os
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
from langchain_core.documents import Document

# å¯¼å…¥AIæŸ¥è¯¢åˆ†æå™¨
try:
    from .ai_query_analyzer import get_ai_query_analyzer, AIQueryAnalyzer
    AI_ANALYZER_AVAILABLE = True
    print("âœ… AIæŸ¥è¯¢åˆ†æå™¨å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ AIæŸ¥è¯¢åˆ†æå™¨å¯¼å…¥å¤±è´¥: {e}")
    AI_ANALYZER_AVAILABLE = False

class IntelligentQueryDecomposer:
    """æ™ºèƒ½æŸ¥è¯¢åˆ†è§£å™¨ - é›†æˆAIå¤§æ¨¡å‹åˆ†æ"""
    
    def __init__(self, knowledge_path: Optional[str] = None, model_config_id: Optional[int] = None):
        """
        åˆå§‹åŒ–æŸ¥è¯¢åˆ†è§£å™¨
        
        Args:
            knowledge_path: å…ˆéªŒçŸ¥è¯†åº“è·¯å¾„
            model_config_id: æ•°æ®åº“ä¸­çš„AIæ¨¡å‹é…ç½®ID
        """
        self.knowledge_path = knowledge_path or self._get_default_knowledge_path()
        self.prior_knowledge = self._load_prior_knowledge()
        self.model_config_id = model_config_id
        
        # åˆå§‹åŒ–AIæŸ¥è¯¢åˆ†æå™¨
        if AI_ANALYZER_AVAILABLE:
            try:
                self.ai_analyzer = get_ai_query_analyzer(model_config_id)
                if self.ai_analyzer.is_available():
                    print(f"âœ… AIæŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ: {self.ai_analyzer.get_model_info()}")
                else:
                    print("âš ï¸ AIæŸ¥è¯¢åˆ†æå™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†æ")
                    self.ai_analyzer = None
            except Exception as e:
                print(f"âš ï¸ AIæŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.ai_analyzer = None
        else:
            self.ai_analyzer = None
        
    def _get_default_knowledge_path(self) -> str:
        """è·å–é»˜è®¤å…ˆéªŒçŸ¥è¯†åº“è·¯å¾„"""
        current_dir = Path(__file__).parent
        return str(current_dir / "knowledge" / "prior_knowledge.json")
    
    def _load_prior_knowledge(self) -> Dict[str, Any]:
        """åŠ è½½å…ˆéªŒçŸ¥è¯†åº“"""
        try:
            if os.path.exists(self.knowledge_path):
                with open(self.knowledge_path, 'r', encoding='utf-8') as f:
                    knowledge = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½å…ˆéªŒçŸ¥è¯†åº“: {self.knowledge_path}")
                return knowledge
            else:
                print(f"âš ï¸ å…ˆéªŒçŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.knowledge_path}")
                return {}
        except Exception as e:
            print(f"âŒ åŠ è½½å…ˆéªŒçŸ¥è¯†åº“å¤±è´¥: {e}")
            return {}
    
    def decompose_query(self, query: str) -> Dict[str, Any]:
        """æ™ºèƒ½åˆ†è§£æŸ¥è¯¢ - ä½¿ç”¨AIå¤§æ¨¡å‹è¿›è¡Œåˆ†æ"""
        print(f"ğŸ§  å¼€å§‹AIæ™ºèƒ½æŸ¥è¯¢åˆ†è§£: '{query}'")
        
        # 1. ä½¿ç”¨AIå¤§æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢åˆ†æ
        ai_analysis = self._analyze_with_ai(query)
        
        # 2. ç»“åˆå…ˆéªŒçŸ¥è¯†åº“è¿›è¡Œç­–ç•¥è¯†åˆ«
        decomposition_strategy = self._identify_decomposition_strategy(query, ai_analysis)
        
        # 3. æ•´åˆå®ä½“æå–ç»“æœ
        entities = self._integrate_entity_extraction(query, ai_analysis)
        
        # 4. ç”Ÿæˆè¯­ä¹‰æ‰©å±•
        expanded_terms = self._generate_semantic_expansions(query, ai_analysis)
        
        # 5. ç”Ÿæˆå­æŸ¥è¯¢
        sub_queries = self._generate_sub_queries(query, decomposition_strategy, entities, ai_analysis)
        
        # 6. ç¡®å®šå¤šè·¯æ£€ç´¢ç­–ç•¥
        multi_path_strategies = self._determine_multi_path_strategies(query, entities, ai_analysis)
        
        result = {
            "original_query": query,
            "decomposition_strategy": decomposition_strategy,
            "entities": entities,
            "expanded_terms": expanded_terms,
            "sub_queries": sub_queries,
            "multi_path_strategies": multi_path_strategies,
            "complexity_score": ai_analysis.get("complexity_score", 0.3),
            "ai_confidence": ai_analysis.get("confidence", 0.0),
            "query_intent": ai_analysis.get("query_intent", "general"),
            "ai_model_info": self.ai_analyzer.get_model_info() if self.ai_analyzer else {}
        }
        
        print(f"ğŸ“Š AIæ™ºèƒ½æŸ¥è¯¢åˆ†è§£ç»“æœ:")
        print(f"  ç­–ç•¥: {decomposition_strategy}")
        print(f"  å®ä½“æ•°é‡: {sum(len(v) for v in entities.values())}")
        print(f"  å­æŸ¥è¯¢æ•°é‡: {len(sub_queries)}")
        print(f"  å¤æ‚åº¦è¯„åˆ†: {result['complexity_score']}")
        print(f"  AIç½®ä¿¡åº¦: {result['ai_confidence']}")
        print(f"  æŸ¥è¯¢æ„å›¾: {result['query_intent']}")
        
        return result
    
    def _analyze_with_ai(self, query: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIå¤§æ¨¡å‹åˆ†ææŸ¥è¯¢"""
        if self.ai_analyzer and self.ai_analyzer.is_available():
            print(f"ğŸ¤– ä½¿ç”¨AIå¤§æ¨¡å‹åˆ†ææŸ¥è¯¢...")
            try:
                ai_result = self.ai_analyzer.analyze_query_with_ai(query)
                print(f"âœ… AIåˆ†ææˆåŠŸï¼Œç½®ä¿¡åº¦: {ai_result.get('confidence', 0.0):.2f}")
                return ai_result
            except Exception as e:
                print(f"âš ï¸ AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ: {e}")
        else:
            print(f"ğŸ”„ AIåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
        
        # å›é€€åˆ°åŸºç¡€åˆ†æ
        return self._basic_analysis(query)
    
    def _basic_analysis(self, query: str) -> Dict[str, Any]:
        """åŸºç¡€åˆ†ææ–¹æ³•ï¼ˆå½“AIä¸å¯ç”¨æ—¶ï¼‰"""
        entities = {
            "persons": [],
            "organizations": [],
            "locations": [],
            "times": [],
            "topics": []
        }
        
        # åŸºç¡€å®ä½“æå–
        person_names = self._extract_person_names_with_regex(query)
        entities["persons"].extend(person_names)
        
        # åŸºç¡€ä¸»é¢˜è¯æå–
        topic_keywords = ['æŠ•è¯‰', 'ä¸¾æŠ¥', 'åæ˜ ', 'ç”³è¯‰', 'æ„è§', 'å»ºè®®', 'é—®é¢˜', 'äº‹ä»¶', 'æƒ…å†µ', 'å¤„ç†', 'è°ƒæŸ¥', 'æ£€æŸ¥']
        for keyword in topic_keywords:
            if keyword in query:
                entities["topics"].append(keyword)
        
        # åŸºç¡€æ„å›¾åˆ†æ
        query_intent = "general"
        if "æŠ•è¯‰" in query:
            if "å†…å®¹" in query or "ä»€ä¹ˆ" in query:
                query_intent = "complaint_content"
            elif "å¤„ç†" in query:
                query_intent = "complaint_handling"
            else:
                query_intent = "complaint_general"
        elif entities["persons"]:
            query_intent = "person_related"
        
        return {
            "entities": entities,
            "query_intent": query_intent,
            "keywords": [word for word in ['æŠ•è¯‰', 'å¤„ç†', 'å†…å®¹', 'æƒ…å†µ'] if word in query],
            "semantic_expansions": [],
            "complexity_score": 0.3,
            "confidence": 0.6
        }
    
    def _identify_decomposition_strategy(self, query: str, ai_analysis: Dict) -> str:
        """è¯†åˆ«åˆ†è§£ç­–ç•¥ - ç»“åˆAIåˆ†æå’Œå…ˆéªŒçŸ¥è¯†"""
        # ä¼˜å…ˆä½¿ç”¨AIåˆ†æçš„æŸ¥è¯¢æ„å›¾
        query_intent = ai_analysis.get("query_intent", "general")
        
        # æ˜ å°„æŸ¥è¯¢æ„å›¾åˆ°åˆ†è§£ç­–ç•¥
        intent_to_strategy = {
            "complaint_content": "person_complaint_content",
            "complaint_handling": "process_oriented",
            "person_related": "person_complaint_general",
            "event_related": "cause_analysis",
            "general": "simple"
        }
        
        strategy = intent_to_strategy.get(query_intent, "simple")
        
        # ä½¿ç”¨å…ˆéªŒçŸ¥è¯†åº“è¿›è¡ŒéªŒè¯å’Œè¡¥å……
        if self.prior_knowledge:
            patterns = self.prior_knowledge.get("query_decomposition_rules", {}).get("complex_patterns", [])
            
            for pattern_info in patterns:
                pattern = pattern_info.get("pattern", "")
                if re.search(pattern, query):
                    prior_strategy = pattern_info.get("decomposition_strategy", "simple")
                    print(f"ğŸ¯ å…ˆéªŒçŸ¥è¯†åŒ¹é…: '{pattern}' -> {prior_strategy}")
                    # å¦‚æœå…ˆéªŒçŸ¥è¯†æ›´å…·ä½“ï¼Œåˆ™ä½¿ç”¨å…ˆéªŒçŸ¥è¯†çš„ç­–ç•¥
                    if prior_strategy != "simple":
                        strategy = prior_strategy
                    break
        
        print(f"ğŸ“‹ ç¡®å®šåˆ†è§£ç­–ç•¥: {strategy} (åŸºäºAIæ„å›¾: {query_intent})")
        return strategy
    
    def _integrate_entity_extraction(self, query: str, ai_analysis: Dict) -> Dict[str, List[str]]:
        """æ•´åˆå®ä½“æå–ç»“æœ"""
        # ä»AIåˆ†æè·å–å®ä½“
        ai_entities = ai_analysis.get("entities", {})
        
        # ä½¿ç”¨å¢å¼ºåˆ†è¯å™¨è¿›è¡Œè¡¥å……æå–
        enhanced_entities = self._extract_entities_with_enhanced_tokenizer(query)
        
        # æ•´åˆç»“æœ
        integrated_entities = {
            "persons": [],
            "organizations": [],
            "locations": [],
            "times": [],
            "topics": []
        }
        
        # åˆå¹¶AIæå–çš„å®ä½“
        for entity_type, entity_list in ai_entities.items():
            if entity_type in integrated_entities:
                integrated_entities[entity_type].extend(entity_list)
        
        # åˆå¹¶å¢å¼ºåˆ†è¯å™¨æå–çš„å®ä½“
        for entity_type, entity_list in enhanced_entities.items():
            if entity_type in integrated_entities:
                for entity in entity_list:
                    if entity not in integrated_entities[entity_type]:
                        integrated_entities[entity_type].append(entity)
        
        # å»é‡å¹¶æ¸…ç†
        for key in integrated_entities:
            integrated_entities[key] = list(set([entity.strip() for entity in integrated_entities[key] if entity.strip()]))
        
        if any(integrated_entities.values()):
            print(f"ğŸ·ï¸ æ•´åˆå®ä½“æå–ç»“æœ: {integrated_entities}")
        
        return integrated_entities
    
    def _extract_entities_with_enhanced_tokenizer(self, query: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨å¢å¼ºåˆ†è¯å™¨æå–å®ä½“"""
        entities = {
            "persons": [],
            "organizations": [],
            "locations": [],
            "times": [],
            "topics": []
        }
        
        try:
            from .utils.enhanced_tokenizer import get_enhanced_tokenizer
            tokenizer = get_enhanced_tokenizer()
            
            # æå–äººå‘˜å§“å
            person_names = tokenizer.get_person_names(query)
            entities["persons"].extend(person_names)
            
            # æå–ç»„ç»‡æœºæ„
            organizations = tokenizer.get_organizations(query)
            entities["organizations"].extend(organizations)
            
            # æå–åœ°ç‚¹
            locations = tokenizer.get_locations(query)
            entities["locations"].extend(locations)
            
        except Exception as e:
            print(f"âš ï¸ å¢å¼ºåˆ†è¯å™¨å®ä½“æå–å¤±è´¥: {e}")
            # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æå–
            person_names = self._extract_person_names_with_regex(query)
            entities["persons"].extend(person_names)
        
        # æå–ä¸»é¢˜è¯
        topic_keywords = ['æŠ•è¯‰', 'ä¸¾æŠ¥', 'åæ˜ ', 'ç”³è¯‰', 'æ„è§', 'å»ºè®®', 'é—®é¢˜', 'äº‹ä»¶', 'æƒ…å†µ', 'å¤„ç†', 'è°ƒæŸ¥', 'æ£€æŸ¥']
        for keyword in topic_keywords:
            if keyword in query:
                entities["topics"].append(keyword)
        
        return entities
    
    def _generate_semantic_expansions(self, query: str, ai_analysis: Dict) -> Dict[str, List[str]]:
        """ç”Ÿæˆè¯­ä¹‰æ‰©å±• - ç»“åˆAIå’Œå…ˆéªŒçŸ¥è¯†"""
        expanded_terms = {
            "synonyms": [],
            "related_terms": [],
            "contextual_terms": []
        }
        
        # 1. ä½¿ç”¨AIç”Ÿæˆè¯­ä¹‰æ‰©å±•
        if self.ai_analyzer and self.ai_analyzer.is_available():
            try:
                ai_expansions = self.ai_analyzer.generate_semantic_expansions_with_ai(query)
                expanded_terms["synonyms"].extend(ai_expansions)
            except Exception as e:
                print(f"âš ï¸ AIè¯­ä¹‰æ‰©å±•å¤±è´¥: {e}")
        
        # 2. ä½¿ç”¨å…ˆéªŒçŸ¥è¯†åº“è¿›è¡Œæ‰©å±•
        if self.prior_knowledge:
            semantic_expansion = self.prior_knowledge.get("query_expansion", {}).get("semantic_expansion", {})
            
            # åŸºäºAIæå–çš„å…³é”®è¯è¿›è¡Œæ‰©å±•
            ai_keywords = ai_analysis.get("keywords", [])
            for keyword in ai_keywords:
                if keyword in semantic_expansion:
                    synonyms = semantic_expansion[keyword]
                    expanded_terms["synonyms"].extend(synonyms)
        
        # å»é‡
        for key in expanded_terms:
            expanded_terms[key] = list(set(expanded_terms[key]))
        
        if any(expanded_terms.values()):
            print(f"ğŸ” è¯­ä¹‰æ‰©å±•ç»“æœ: {expanded_terms}")
        
        return expanded_terms
    
    def _extract_person_names_with_regex(self, query: str) -> List[str]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–äººå‘˜å§“å"""
        person_names = []
        
        # ä¼˜åŒ–çš„äººå‘˜å§“åè¯†åˆ«æ¨¡å¼
        patterns = [
            # å¸¸è§å§“æ° + åå­— + ç§°è°“ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            r'([ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå‘¨å´å¾å­™æœ±é©¬èƒ¡éƒ­æ—ä½•é«˜æ¢éƒ‘ç½—å®‹è°¢å”éŸ©æ›¹è®¸é‚“è§å†¯æ›¾ç¨‹è”¡å½­æ½˜è¢äºè‘£ä½™è‹å¶å•é­è’‹ç”°æœä¸æ²ˆå§œèŒƒæ±Ÿå‚…é’Ÿå¢æ±ªæˆ´å´”ä»»é™†å»–å§šæ–¹é‡‘é‚±å¤è°­éŸ¦è´¾é‚¹çŸ³ç†Šå­Ÿç§¦é˜è–›ä¾¯é›·ç™½é¾™æ®µéƒå­”é‚µå²æ¯›å¸¸ä¸‡é¡¾èµ–æ­¦åº·è´ºä¸¥å°¹é’±æ–½ç‰›æ´ªé¾šæ±¤é™¶é»æ¸©è«æ˜“æ¨Šä¹”æ–‡å®‰æ®·é¢œåº„ç« é²å€ªåºé‚¢ä¿ç¿Ÿè“è‚ä¸›å²³é½æ²¿][ä¸€-é¾¥]{1,3})(å…ˆç”Ÿ|å¥³å£«|åŒå¿—|è€å¸ˆ|åŒ»ç”Ÿ|æŠ¤å£«|ä¸»ä»»|ç»ç†|æ€»ç›‘|ä¸“å®¶|å±€é•¿|å¤„é•¿|ç§‘é•¿|ä¸»ç®¡|å‘˜å·¥|å®¢æˆ·|æ‚£è€…|å­¦ç”Ÿ|å®¶é•¿)',
            
            # 2-4ä¸ªæ±‰å­— + ç§°è°“ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            r'([ä¸€-é¾¥]{2,4})(å…ˆç”Ÿ|å¥³å£«|åŒå¿—|è€å¸ˆ|åŒ»ç”Ÿ|æŠ¤å£«|ä¸»ä»»|ç»ç†|æ€»ç›‘|ä¸“å®¶|å±€é•¿|å¤„é•¿|ç§‘é•¿|ä¸»ç®¡|å‘˜å·¥|å®¢æˆ·|æ‚£è€…|å­¦ç”Ÿ|å®¶é•¿)',
            
            # æŠ•è¯‰äºº/ä¸¾æŠ¥äººç­‰æ¨¡å¼ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            r'(?:æŠ•è¯‰äºº|ä¸¾æŠ¥äºº|ç”³è¯‰äºº|åæ˜ äºº)[:ï¼š]?\s*([ä¸€-é¾¥]{2,4})',
            
            # å§“ååœ¨å¥é¦–çš„æ¨¡å¼ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            r'^([ä¸€-é¾¥]{2,4})(?:æŠ•è¯‰|åæ˜ |ä¸¾æŠ¥|ç”³è¯‰)',
            
            # å§“å + çš„ + æŠ•è¯‰ç›¸å…³è¯ï¼ˆé¿å…æå–"é’Ÿå¥³å£«çš„"ï¼‰
            r'([ä¸€-é¾¥]{2,4})(?=çš„(?:æŠ•è¯‰|åæ˜ |ä¸¾æŠ¥|ç”³è¯‰))',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, query)
            for match in matches:
                if isinstance(match, tuple):
                    # å¤„ç†å¤šä¸ªæ•è·ç»„çš„æƒ…å†µ
                    name = None
                    for group in match:
                        if group and self._is_valid_person_name(group):
                            name = group
                            break
                    if name:
                        person_names.append(name)
                        print(f"ğŸ·ï¸ æå–åˆ°äººå‘˜å§“å: '{name}'")
                else:
                    if self._is_valid_person_name(match):
                        person_names.append(match)
                        print(f"ğŸ·ï¸ æå–åˆ°äººå‘˜å§“å: '{match}'")
        
        # åå¤„ç†ï¼šæ¸…ç†é”™è¯¯çš„åç¼€
        cleaned_names = []
        for name in person_names:
            # ç§»é™¤å¸¸è§çš„é”™è¯¯åç¼€
            cleaned_name = re.sub(r'(çš„|æŠ•è¯‰|åæ˜ |ä¸¾æŠ¥|ç”³è¯‰|ç›¸å…³|å¤„ç†|æƒ…å†µ)$', '', name)
            if cleaned_name and self._is_valid_person_name(cleaned_name) and cleaned_name not in cleaned_names:
                cleaned_names.append(cleaned_name)
        
        return cleaned_names
    
    def _is_valid_person_name(self, name: str) -> bool:
        """éªŒè¯å§“åæœ‰æ•ˆæ€§"""
        if not name or len(name) < 2 or len(name) > 4:
            return False
        
        # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯å§“åçš„è¯æ±‡
        invalid_names = {
            'æŠ•è¯‰', 'å¤„ç†', 'æƒ…å†µ', 'å†…å®¹', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 
            'å“ªé‡Œ', 'ä»€ä¹ˆæ—¶å€™', 'é—®é¢˜', 'äº‹ä»¶', 'åæ˜ ', 'ä¸¾æŠ¥', 'ç”³è¯‰',
            'å…ˆç”Ÿ', 'å¥³å£«', 'åŒå¿—', 'è€å¸ˆ', 'åŒ»ç”Ÿ', 'æŠ¤å£«', 'ä¸»ä»»', 'ç»ç†',
            'æœ‰å…³', 'å…³äº', 'ç›¸å…³', 'æ‰€æœ‰', 'è®°å½•', 'è¯¦ç»†', 'ä¿¡æ¯'
        }
        
        if name in invalid_names:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
        if any(char.isdigit() or not char.isalnum() for char in name):
            return False
        
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯æ±‰å­—
        if not all('\u4e00' <= char <= '\u9fff' for char in name):
            return False
        
        return True
        
    def _generate_sub_queries(self, query: str, strategy: str, entities: Dict[str, List[str]], ai_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå­æŸ¥è¯¢"""
        sub_queries = []
        
        if strategy == "simple":
            return [query]  # ç®€å•æŸ¥è¯¢ä¸åˆ†è§£
        
        # åŸºäºç­–ç•¥ç”Ÿæˆå­æŸ¥è¯¢
        if strategy in ["topic_detail", "process_oriented", "cause_analysis", "safety_domain", "crisis_management"]:
            sub_queries.extend(self._generate_strategy_based_sub_queries(query, strategy))
        
        # åŸºäºå®ä½“ç”Ÿæˆå­æŸ¥è¯¢
        sub_queries.extend(self._generate_entity_based_sub_queries(query, entities))
        
        # åŸºäºè¯­ä¹‰ç”Ÿæˆå­æŸ¥è¯¢
        sub_queries.extend(self._generate_semantic_based_sub_queries(query))
        
        # å»é‡å¹¶è¿‡æ»¤
        sub_queries = list(set(sub_queries))
        sub_queries = [q for q in sub_queries if q and len(q.strip()) > 2]
        
        # é™åˆ¶å­æŸ¥è¯¢æ•°é‡
        if len(sub_queries) > 8:
            sub_queries = sub_queries[:8]
        
        print(f"ğŸ“ ç”Ÿæˆå­æŸ¥è¯¢: {sub_queries}")
        return sub_queries
    
    def _generate_strategy_based_sub_queries(self, query: str, strategy: str) -> List[str]:
        """åŸºäºç­–ç•¥ç”Ÿæˆå­æŸ¥è¯¢"""
        sub_queries = []
        
        if not self.prior_knowledge:
            return sub_queries
        
        patterns = self.prior_knowledge.get("query_decomposition_rules", {}).get("complex_patterns", [])
        
        for pattern_info in patterns:
            if pattern_info.get("decomposition_strategy") == strategy:
                template_queries = pattern_info.get("sub_queries", [])
                
                # æå–ä¸»é¢˜è¯è¿›è¡Œæ¨¡æ¿æ›¿æ¢
                topic_match = re.search(r"å…³äº(.+?)çš„", query)
                if topic_match:
                    topic = topic_match.group(1)
                    for template in template_queries:
                        sub_query = template.replace("{topic}", topic)
                        sub_queries.append(sub_query)
                else:
                    # å¦‚æœæ²¡æœ‰æ˜ç¡®ä¸»é¢˜ï¼Œä½¿ç”¨å…³é”®è¯
                    keywords = pattern_info.get("keywords", [])
                    for keyword in keywords:
                        if keyword in query:
                            sub_queries.append(keyword)
                break
        
        return sub_queries
    
    def _generate_entity_based_sub_queries(self, query: str, entities: Dict[str, List[str]]) -> List[str]:
        """åŸºäºå®ä½“ç”Ÿæˆå­æŸ¥è¯¢"""
        sub_queries = []
        
        # åŸºäºäººå‘˜å®ä½“
        for person in entities.get("persons", []):
            sub_queries.append(f"{person}ç›¸å…³ä¿¡æ¯")
            sub_queries.append(f"{person}æŠ•è¯‰")
        
        # åŸºäºç»„ç»‡æœºæ„
        for org in entities.get("organizations", []):
            sub_queries.append(f"{org}å¤„ç†æƒ…å†µ")
            sub_queries.append(f"{org}æ£€æŸ¥")
        
        # åŸºäºåœ°ç‚¹
        for location in entities.get("locations", []):
            sub_queries.append(f"{location}äº‹ä»¶")
            sub_queries.append(f"{location}å®‰å…¨")
        
        # åŸºäºæ—¶é—´
        for time in entities.get("times", []):
            sub_queries.append(f"{time}å‘ç”Ÿ")
            sub_queries.append(f"{time}å¤„ç†")
        
        # åŸºäºä¸»é¢˜
        for topic in entities.get("topics", []):
            sub_queries.append(f"{topic}äº‹ä»¶")
            sub_queries.append(f"{topic}å¤„ç†")
            sub_queries.append(f"{topic}è§„å®š")
        
        return sub_queries
    
    def _generate_semantic_based_sub_queries(self, query: str) -> List[str]:
        """åŸºäºè¯­ä¹‰ç”Ÿæˆå­æŸ¥è¯¢"""
        sub_queries = []
        
        # æå–æ ¸å¿ƒåŠ¨è¯å’Œåè¯
        action_words = ["å¤„ç†", "è°ƒæŸ¥", "æ£€æŸ¥", "å¤„ç½®", "åº”å¯¹", "è§£å†³"]
        object_words = ["æŠ•è¯‰", "äº‹ä»¶", "é—®é¢˜", "æƒ…å†µ", "æ¡ˆä»¶", "èˆ†æƒ…"]
        
        for action in action_words:
            if action in query:
                for obj in object_words:
                    if obj in query:
                        sub_queries.append(f"{obj}{action}")
                        sub_queries.append(f"{action}{obj}")
        
        return sub_queries
    
    def _determine_multi_path_strategies(self, query: str, entities: Dict[str, List[str]], ai_analysis: Dict) -> List[str]:
        """ç¡®å®šå¤šè·¯æ£€ç´¢ç­–ç•¥"""
        strategies = []
        
        # åŸºäºå®ä½“æ•°é‡ç¡®å®šç­–ç•¥
        entity_count = sum(len(v) for v in entities.values())
        
        if entity_count > 3:
            strategies.append("entity_based")
        
        if len(query) > 15:
            strategies.append("semantic_based")
        
        if entities.get("topics"):
            strategies.append("topic_based")
        
        if entities.get("times"):
            strategies.append("time_based")
        
        # é»˜è®¤ç­–ç•¥
        if not strategies:
            strategies.append("semantic_based")
        
        return strategies


class MultiPathRetriever:
    """å¤šè·¯æ£€ç´¢å™¨ - å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self, base_retriever):
        """åˆå§‹åŒ–å¤šè·¯æ£€ç´¢å™¨"""
        self.base_retriever = base_retriever
        self.query_decomposer = IntelligentQueryDecomposer()
    
    def multi_path_search(self, query: str) -> List[Document]:
        """å¤šè·¯æ£€ç´¢ - ä½¿ç”¨å¢å¼ºæ£€ç´¢æ–¹æ³•"""
        print(f"ğŸš€ å¼€å§‹å¢å¼ºå¤šè·¯æ£€ç´¢: '{query}'")
        
        # 1. æŸ¥è¯¢åˆ†è§£å’Œè¯­ä¹‰ç†è§£
        decomposition_result = self.query_decomposer.decompose_query(query)
        
        # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šè·¯æ£€ç´¢
        if decomposition_result["complexity_score"] < 0.3:
            print("ğŸ“ æŸ¥è¯¢å¤æ‚åº¦è¾ƒä½ï¼Œä½¿ç”¨å¢å¼ºå•è·¯æ£€ç´¢")
            # ä½¿ç”¨å¢å¼ºçš„åˆ†å±‚æ£€ç´¢
            enhanced_query_info = {
                "entities": decomposition_result.get("entities", {}),
                "keywords": [],
                "query_intent": self._analyze_query_intent(query, decomposition_result),
                "complexity_score": decomposition_result["complexity_score"]
            }
            return self.base_retriever._enhanced_hierarchical_search(query, enhanced_query_info)
        
        # 3. æ‰§è¡Œå¤šè·¯æ£€ç´¢
        all_results = []
        search_paths = []
        
        # ä¸»æŸ¥è¯¢è·¯å¾„ - ä½¿ç”¨å¢å¼ºæ£€ç´¢
        print("ğŸ” æ‰§è¡Œä¸»æŸ¥è¯¢è·¯å¾„...")
        enhanced_query_info = {
            "entities": decomposition_result.get("entities", {}),
            "keywords": [],
            "query_intent": self._analyze_query_intent(query, decomposition_result),
            "complexity_score": decomposition_result["complexity_score"]
        }
        main_results = self.base_retriever._enhanced_hierarchical_search(query, enhanced_query_info)
        search_paths.append(("main_query", query, main_results))
        all_results.extend(main_results)
        
        # å®ä½“æŸ¥è¯¢è·¯å¾„ - é’ˆå¯¹æå–åˆ°çš„å®ä½“è¿›è¡Œä¸“é—¨æ£€ç´¢
        entities = decomposition_result.get("entities", {})
        self._execute_entity_search_paths(entities, all_results, search_paths)
        
        # å­æŸ¥è¯¢è·¯å¾„ - ä½¿ç”¨å¢å¼ºæ£€ç´¢
        sub_queries = decomposition_result["sub_queries"][:3]  # é™åˆ¶å­æŸ¥è¯¢æ•°é‡
        for i, sub_query in enumerate(sub_queries):
            try:
                print(f"ğŸ” æ‰§è¡Œå­æŸ¥è¯¢ {i+1}: '{sub_query}'")
                sub_enhanced_info = {
                    "entities": entities,
                    "keywords": [],
                    "query_intent": "general",
                    "complexity_score": 0.2
                }
                sub_results = self.base_retriever._enhanced_hierarchical_search(sub_query, sub_enhanced_info)
                search_paths.append((f"sub_query_{i+1}", sub_query, sub_results))
                all_results.extend(sub_results)
                print(f"ğŸ” å­æŸ¥è¯¢ {i+1}: '{sub_query}' -> {len(sub_results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"âš ï¸ å­æŸ¥è¯¢ {i+1} å¤±è´¥: {e}")
        
        # æ‰©å±•è¯æŸ¥è¯¢è·¯å¾„
        expanded_terms = decomposition_result.get("expanded_terms", {})
        if expanded_terms.get("synonyms"):
            for synonym in expanded_terms["synonyms"][:2]:  # é™åˆ¶åŒä¹‰è¯æ•°é‡
                try:
                    synonym_query = query.replace(query.split()[0], synonym, 1)
                    print(f"ğŸ”„ æ‰§è¡ŒåŒä¹‰è¯æŸ¥è¯¢: '{synonym_query}'")
                    synonym_enhanced_info = {
                        "entities": entities,
                        "keywords": [],
                        "query_intent": "general",
                        "complexity_score": 0.2
                    }
                    synonym_results = self.base_retriever._enhanced_hierarchical_search(synonym_query, synonym_enhanced_info)
                    search_paths.append(("synonym", synonym_query, synonym_results))
                    all_results.extend(synonym_results)
                    print(f"ğŸ”„ åŒä¹‰è¯æŸ¥è¯¢: '{synonym_query}' -> {len(synonym_results)} ä¸ªç»“æœ")
                except Exception as e:
                    print(f"âš ï¸ åŒä¹‰è¯æŸ¥è¯¢å¤±è´¥: {e}")
        
        # 4. ç»“æœèåˆ
        fused_results = self._fuse_results(all_results, search_paths, decomposition_result)
        
        print(f"ğŸ¯ å¢å¼ºå¤šè·¯æ£€ç´¢å®Œæˆï¼Œèåˆåç»“æœæ•°é‡: {len(fused_results)}")
        return fused_results
    
    def _analyze_query_intent(self, query: str, decomposition_result: Dict) -> str:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()
        
        # æŠ•è¯‰ç›¸å…³æŸ¥è¯¢
        if "æŠ•è¯‰" in query_lower:
            if "å†…å®¹" in query_lower or "ä»€ä¹ˆ" in query_lower:
                return "complaint_content"
            elif "å¤„ç†" in query_lower:
                return "complaint_handling"
            else:
                return "complaint_general"
        
        # äººå‘˜ç›¸å…³æŸ¥è¯¢
        entities = decomposition_result.get("entities", {})
        if entities.get("persons"):
            return "person_related"
        
        # äº‹ä»¶ç›¸å…³æŸ¥è¯¢
        if "äº‹ä»¶" in query_lower or "æƒ…å†µ" in query_lower:
            return "event_related"
        
        return "general"
    
    def _execute_entity_search_paths(self, entities: Dict[str, List[str]], all_results: List, search_paths: List):
        """æ‰§è¡Œå®ä½“æœç´¢è·¯å¾„"""
        # äººå‘˜å®ä½“æœç´¢
        persons = entities.get("persons", [])
        for person in persons[:2]:  # é™åˆ¶äººå‘˜æ•°é‡
            try:
                print(f"ğŸ‘¤ æ‰§è¡Œäººå‘˜å®ä½“æœç´¢: '{person}'")
                
                # æ„å»ºäººå‘˜ç›¸å…³çš„æŸ¥è¯¢
                person_queries = [
                    f"{person}æŠ•è¯‰",
                    f"{person}ç›¸å…³",
                    f"{person}åæ˜ ",
                    person  # ç›´æ¥æœç´¢äººå‘˜å§“å
                ]
                
                for person_query in person_queries:
                    try:
                        person_enhanced_info = {
                            "entities": {"persons": [person]},
                            "keywords": [],
                            "query_intent": "person_related",
                            "complexity_score": 0.3
                        }
                        person_results = self.base_retriever._enhanced_hierarchical_search(person_query, person_enhanced_info)
                        if person_results:
                            search_paths.append(("person_entity", person_query, person_results))
                            all_results.extend(person_results)
                            print(f"ğŸ‘¤ äººå‘˜æŸ¥è¯¢: '{person_query}' -> {len(person_results)} ä¸ªç»“æœ")
                            break  # æ‰¾åˆ°ç»“æœå°±åœæ­¢
                    except Exception as e:
                        print(f"âš ï¸ äººå‘˜æŸ¥è¯¢ '{person_query}' å¤±è´¥: {e}")
                        
            except Exception as e:
                print(f"âš ï¸ äººå‘˜å®ä½“æœç´¢å¤±è´¥: {e}")
        
        # ç»„ç»‡æœºæ„å®ä½“æœç´¢
        organizations = entities.get("organizations", [])
        for org in organizations[:1]:  # é™åˆ¶ç»„ç»‡æ•°é‡
            try:
                print(f"ğŸ¢ æ‰§è¡Œç»„ç»‡å®ä½“æœç´¢: '{org}'")
                org_enhanced_info = {
                    "entities": {"organizations": [org]},
                    "keywords": [],
                    "query_intent": "organization_related",
                    "complexity_score": 0.2
                }
                org_results = self.base_retriever._enhanced_hierarchical_search(org, org_enhanced_info)
                if org_results:
                    search_paths.append(("organization_entity", org, org_results))
                    all_results.extend(org_results)
                    print(f"ğŸ¢ ç»„ç»‡æŸ¥è¯¢: '{org}' -> {len(org_results)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"âš ï¸ ç»„ç»‡å®ä½“æœç´¢å¤±è´¥: {e}")
    
    def _fuse_results(self, all_results: List[Document], search_paths: List[Tuple], decomposition_result: Dict) -> List[Document]:
        """èåˆå¤šè·¯æ£€ç´¢ç»“æœ - å¢å¼ºç‰ˆæœ¬"""
        if not all_results:
            return []
        
        # å»é‡ï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼åº¦å’Œå®ä½“åŒ¹é…ï¼‰
        unique_results = self._enhanced_deduplicate_results(all_results, decomposition_result)
        
        # é‡æ–°è¯„åˆ†ï¼ˆè€ƒè™‘å®ä½“åŒ¹é…ï¼‰
        scored_results = self._enhanced_rescore_results(unique_results, decomposition_result)
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ª
        scored_results.sort(key=lambda x: x[1], reverse=True)
        final_results = [doc for doc, score in scored_results[:self.base_retriever.chunk_top_k]]
        
        return final_results
    
    def _enhanced_deduplicate_results(self, results: List[Document], decomposition_result: Dict) -> List[Document]:
        """å¢å¼ºå»é‡ç»“æœ - è€ƒè™‘å®ä½“åŒ¹é…"""
        unique_results = []
        seen_contents = set()
        seen_doc_ids = set()
        
        # æå–å®ä½“ç”¨äºåŒ¹é…
        entities = decomposition_result.get("entities", {})
        all_entities = []
        for entity_list in entities.values():
            all_entities.extend(entity_list)
        
        for doc in results:
            # ä½¿ç”¨doc_idè¿›è¡Œå»é‡
            doc_id = doc.metadata.get('doc_id')
            if doc_id and doc_id in seen_doc_ids:
                continue
            
            # ä½¿ç”¨å†…å®¹çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå»é‡æ ‡è¯†
            content_key = doc.page_content[:100].strip()
            if content_key in seen_contents:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å®ä½“ï¼ˆä¼˜å…ˆä¿ç•™åŒ…å«å®ä½“çš„æ–‡æ¡£ï¼‰
            doc_content = doc.page_content.lower()
            contains_entity = any(entity.lower() in doc_content for entity in all_entities)
            
            if contains_entity or len(unique_results) < self.base_retriever.chunk_top_k:
                seen_contents.add(content_key)
                if doc_id:
                    seen_doc_ids.add(doc_id)
                unique_results.append(doc)
                
                if contains_entity:
                    print(f"ğŸ¯ ä¿ç•™åŒ…å«å®ä½“çš„æ–‡æ¡£: doc_id={doc_id}")
        
        print(f"ğŸ”„ å¢å¼ºå»é‡: {len(results)} -> {len(unique_results)}")
        return unique_results
    
    def _enhanced_rescore_results(self, results: List[Document], decomposition_result: Dict) -> List[Tuple[Document, float]]:
        """å¢å¼ºé‡æ–°è¯„åˆ†ç»“æœ - è€ƒè™‘å®ä½“åŒ¹é…å’ŒæŸ¥è¯¢æ„å›¾"""
        scored_results = []
        
        # æå–å…³é”®è¯å’Œå®ä½“ç”¨äºè¯„åˆ†
        entities = decomposition_result.get("entities", {})
        all_entities = []
        for entity_list in entities.values():
            all_entities.extend(entity_list)
        
        query_intent = decomposition_result.get("query_intent", "general")
        original_query = decomposition_result["original_query"]
        
        for doc in results:
            score = self._calculate_enhanced_relevance_score(
                doc, all_entities, original_query, query_intent
            )
            scored_results.append((doc, score))
        
        return scored_results
    
    def _calculate_enhanced_relevance_score(self, doc: Document, entities: List[str], 
                                          original_query: str, query_intent: str) -> float:
        """è®¡ç®—å¢å¼ºç›¸å…³æ€§è¯„åˆ† - è€ƒè™‘å®ä½“åŒ¹é…å’ŒæŸ¥è¯¢æ„å›¾"""
        content = doc.page_content.lower()
        query_lower = original_query.lower()
        
        score = 0.0
        
        # 1. åŸæŸ¥è¯¢åŒ¹é…åº¦ (40%)
        query_words = query_lower.split()
        query_matches = sum(1 for word in query_words if word in content)
        score += (query_matches / len(query_words)) * 0.4
        
        # 2. å®ä½“åŒ¹é…åº¦ (35%)
        if entities:
            entity_matches = sum(1 for entity in entities if entity.lower() in content)
            entity_score = (entity_matches / len(entities)) * 0.35
            score += entity_score
            
            # å®ä½“ç²¾ç¡®åŒ¹é…åŠ åˆ†
            for entity in entities:
                if entity.lower() in content:
                    score += 0.1  # æ¯ä¸ªåŒ¹é…çš„å®ä½“é¢å¤–åŠ åˆ†
        
        # 3. æŸ¥è¯¢æ„å›¾åŒ¹é…åº¦ (15%)
        intent_keywords = {
            "complaint_content": ["æŠ•è¯‰", "å†…å®¹", "åæ˜ ", "ä¸¾æŠ¥", "ç”³è¯‰"],
            "complaint_handling": ["å¤„ç†", "åŠç†", "è§£å†³", "å›å¤", "ç­”å¤"],
            "person_related": ["å§“å", "è”ç³»", "ç”µè¯", "åœ°å€", "èº«ä»½"],
            "event_related": ["äº‹ä»¶", "æƒ…å†µ", "ç»è¿‡", "è¯¦æƒ…", "è¿‡ç¨‹"]
        }
        
        if query_intent in intent_keywords:
            intent_words = intent_keywords[query_intent]
            intent_matches = sum(1 for word in intent_words if word in content)
            score += (intent_matches / len(intent_words)) * 0.15
        
        # 4. æ–‡æ¡£è´¨é‡åŠ åˆ† (10%)
        if doc.metadata.get('source'):
            score += 0.05
        if doc.metadata.get('doc_id'):
            score += 0.05
        
        # 5. æ–‡æ¡£é•¿åº¦æƒ©ç½šï¼ˆé¿å…è¿‡é•¿æ–‡æ¡£ï¼‰
        length_penalty = min(len(content) / 1000.0, 1.0) * 0.05
        score -= length_penalty
        
        return max(score, 0.0) 