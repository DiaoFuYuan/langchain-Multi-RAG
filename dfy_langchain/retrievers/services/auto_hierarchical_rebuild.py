#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åˆ†å±‚ç´¢å¼•é‡å»ºæœåŠ¡

åœ¨æ–‡ä»¶ä¸Šä¼ åè‡ªåŠ¨æ£€æŸ¥çŸ¥è¯†åº“è§„æ¨¡ï¼Œå¦‚æœæ»¡è¶³æ¡ä»¶åˆ™è‡ªåŠ¨é‡å»ºåˆ†å±‚ç´¢å¼•ã€‚
ç¡®ä¿æ–°æ–‡æ¡£èƒ½å¤Ÿè¢«åˆ†å±‚æ£€ç´¢å‘ç°ã€‚
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoHierarchicalRebuildService:
    """è‡ªåŠ¨åˆ†å±‚ç´¢å¼•é‡å»ºæœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.threshold_docs = 0    # åˆ†å±‚ç´¢å¼•é˜ˆå€¼ (è®¾ä¸º0ï¼Œæ‰€æœ‰çŸ¥è¯†åº“éƒ½è‡ªåŠ¨æ„å»ºåˆ†å±‚ç´¢å¼•)
        self.min_groups = 0        # æœ€å°æ–‡æ¡£ç»„æ•° (è®¾ä¸º0ï¼Œæ‰€æœ‰çŸ¥è¯†åº“éƒ½å¯ä»¥æ„å»º)
        
        # ç¡®ä¿å¯ä»¥å¯¼å…¥RAGç›¸å…³æ¨¡å—
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        if parent_dir not in sys.path:
            sys.path.append(parent_dir)
    
    def should_rebuild_hierarchical_index(self, kb_name: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºåˆ†å±‚ç´¢å¼•
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        try:
            # åŠ è½½çŸ¥è¯†åº“å‘é‡å­˜å‚¨
            vectorstore = self._load_vectorstore(kb_name)
            if not vectorstore:
                return {
                    "should_rebuild": False,
                    "reason": "æ— æ³•åŠ è½½çŸ¥è¯†åº“å‘é‡å­˜å‚¨",
                    "doc_count": 0,
                    "group_count": 0
                }
            
            # åˆ†ææ–‡æ¡£è§„æ¨¡å’Œç»“æ„
            analysis = self._analyze_knowledge_base(vectorstore)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åˆ†å±‚ç´¢å¼•æ¡ä»¶ï¼ˆåªè¦æœ‰æ–‡æ¡£å°±å¯ä»¥æ„å»ºåˆ†å±‚ç´¢å¼•ï¼‰
            should_rebuild = (
                analysis["doc_count"] > 0 and
                analysis["doc_count"] >= self.threshold_docs and 
                analysis["group_count"] >= self.min_groups
            )
            
            # æ£€æŸ¥å½“å‰åˆ†å±‚ç´¢å¼•æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
            hierarchical_exists = self._check_hierarchical_index_exists(kb_name)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ–°æ–‡æ¡£æ•°é‡ï¼‰
            needs_update = self._check_needs_update(kb_name, analysis["doc_count"])
            
            return {
                "should_rebuild": should_rebuild,
                "reason": self._get_rebuild_reason(should_rebuild, hierarchical_exists, needs_update, analysis),
                "doc_count": analysis["doc_count"],
                "group_count": analysis["group_count"],
                "hierarchical_exists": hierarchical_exists,
                "needs_update": needs_update,
                "analysis": analysis
            }
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥åˆ†å±‚ç´¢å¼•é‡å»ºéœ€æ±‚å¤±è´¥: {str(e)}")
            return {
                "should_rebuild": False,
                "reason": f"æ£€æŸ¥å¤±è´¥: {str(e)}",
                "doc_count": 0,
                "group_count": 0
            }
    
    def auto_rebuild_if_needed(self, kb_name: str) -> Dict[str, Any]:
        """
        è‡ªåŠ¨æ£€æŸ¥å¹¶é‡å»ºåˆ†å±‚ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            é‡å»ºç»“æœ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
            check_result = self.should_rebuild_hierarchical_index(kb_name)
            
            if not check_result["should_rebuild"]:
                logger.info(f"çŸ¥è¯†åº“ {kb_name} ä¸éœ€è¦é‡å»ºåˆ†å±‚ç´¢å¼•: {check_result['reason']}")
                return {
                    "success": True,
                    "action": "no_rebuild_needed",
                    "message": check_result["reason"],
                    "check_result": check_result
                }
            
            # æ‰§è¡Œé‡å»º
            logger.info(f"å¼€å§‹ä¸ºçŸ¥è¯†åº“ {kb_name} é‡å»ºåˆ†å±‚ç´¢å¼•...")
            rebuild_result = self._rebuild_hierarchical_index(kb_name)
            
            if rebuild_result["success"]:
                # æ›´æ–°é‡å»ºè®°å½•
                self._update_rebuild_record(kb_name, check_result["doc_count"])
                
                logger.info(f"çŸ¥è¯†åº“ {kb_name} åˆ†å±‚ç´¢å¼•é‡å»ºæˆåŠŸ")
                return {
                    "success": True,
                    "action": "rebuilt",
                    "message": "åˆ†å±‚ç´¢å¼•é‡å»ºæˆåŠŸ",
                    "check_result": check_result,
                    "rebuild_result": rebuild_result
                }
            else:
                logger.error(f"çŸ¥è¯†åº“ {kb_name} åˆ†å±‚ç´¢å¼•é‡å»ºå¤±è´¥: {rebuild_result['message']}")
                return {
                    "success": False,
                    "action": "rebuild_failed",
                    "message": f"åˆ†å±‚ç´¢å¼•é‡å»ºå¤±è´¥: {rebuild_result['message']}",
                    "check_result": check_result,
                    "rebuild_result": rebuild_result
                }
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨é‡å»ºåˆ†å±‚ç´¢å¼•å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "action": "error",
                "message": f"è‡ªåŠ¨é‡å»ºå¤±è´¥: {str(e)}"
            }
    
    def _load_vectorstore(self, kb_name: str):
        """åŠ è½½çŸ¥è¯†åº“å‘é‡å­˜å‚¨"""
        try:
            from rag_retrievers import KnowledgeBaseManager, RetrieverServiceFactory
            
            factory = RetrieverServiceFactory()
            kb_manager = KnowledgeBaseManager(factory)
            
            # åŠ¨æ€ä»é…ç½®æ–‡ä»¶è·å–çŸ¥è¯†åº“ID
            kb_id = self._get_kb_id_by_name(kb_name)
            if not kb_id:
                logger.error(f"æ— æ³•æ‰¾åˆ°çŸ¥è¯†åº“ '{kb_name}' çš„ID")
                return None
            
            vectorstore = kb_manager.load_faiss_vectorstore(str(kb_id))
            
            return vectorstore
            
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            return None
    
    def _get_kb_id_by_name(self, kb_name: str) -> Optional[int]:
        """æ ¹æ®çŸ¥è¯†åº“åç§°è·å–ID"""
        try:
            import json
            # ä½¿ç”¨ç»å¯¹è·¯å¾„ - ä»dfy_langchain/retrievers/services/å‘ä¸Š4çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
            base_path = Path(__file__).parent.parent.parent.parent / "data" / "knowledge_base"
            config_file = base_path / "knowledge_bases.json"
            
            logger.info(f"ğŸ” æŸ¥æ‰¾çŸ¥è¯†åº“é…ç½®æ–‡ä»¶: {config_file}")
            logger.info(f"ğŸ” é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {config_file.exists()}")
            
            if not config_file.exists():
                logger.error(f"çŸ¥è¯†åº“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                return None
            
            with config_file.open("r", encoding="utf-8") as f:
                config = json.load(f)
            
            for kb in config.get("knowledge_bases", []):
                if kb.get("name") == kb_name:
                    return kb.get("id")
            
            logger.error(f"æœªæ‰¾åˆ°åç§°ä¸º '{kb_name}' çš„çŸ¥è¯†åº“")
            return None
            
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“IDå¤±è´¥: {str(e)}")
            return None
    
    def _analyze_knowledge_base(self, vectorstore) -> Dict[str, Any]:
        """åˆ†æçŸ¥è¯†åº“ç»“æ„"""
        try:
            doc_count = 0
            doc_groups = {}
            content_lengths = []
            
            if hasattr(vectorstore, 'docstore') and hasattr(vectorstore.docstore, '_dict'):
                docs = vectorstore.docstore._dict.values()
                doc_count = len(docs)
                
                for doc in docs:
                    source = doc.metadata.get('source', 'unknown')
                    if source not in doc_groups:
                        doc_groups[source] = []
                    doc_groups[source].append(doc)
                    content_lengths.append(len(doc.page_content))
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_content_length = sum(content_lengths) / len(content_lengths) if content_lengths else 0
            
            return {
                "doc_count": doc_count,
                "group_count": len(doc_groups),
                "avg_content_length": avg_content_length,
                "doc_groups": doc_groups
            }
            
        except Exception as e:
            logger.error(f"åˆ†æçŸ¥è¯†åº“ç»“æ„å¤±è´¥: {str(e)}")
            return {
                "doc_count": 0,
                "group_count": 0,
                "avg_content_length": 0,
                "doc_groups": {}
            }
    
    def _check_hierarchical_index_exists(self, kb_name: str) -> bool:
        """æ£€æŸ¥åˆ†å±‚ç´¢å¼•æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´"""
        try:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„
            base_path = Path(__file__).parent.parent.parent / "data" / "knowledge_base"
            hierarchical_path = base_path / kb_name / "hierarchical_vector_store"
            summary_path = hierarchical_path / "summary_vector_store"
            chunk_path = hierarchical_path / "chunk_vector_store"
            
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = ["index.faiss", "index.pkl"]
            
            summary_complete = all(
                (summary_path / f).exists() for f in required_files
            ) if summary_path.exists() else False
            
            chunk_complete = all(
                (chunk_path / f).exists() for f in required_files
            ) if chunk_path.exists() else False
            
            return summary_complete and chunk_complete
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥åˆ†å±‚ç´¢å¼•å­˜åœ¨æ€§å¤±è´¥: {str(e)}")
            return False
    
    def _check_needs_update(self, kb_name: str, current_doc_count: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆåŸºäºæ–‡æ¡£æ•°é‡å˜åŒ–ï¼‰"""
        try:
            # è¯»å–ä¸Šæ¬¡é‡å»ºè®°å½•ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
            base_path = Path(__file__).parent.parent.parent / "data" / "knowledge_base"
            record_file = base_path / kb_name / "hierarchical_rebuild_record.json"
            
            if not record_file.exists():
                return True  # æ²¡æœ‰è®°å½•ï¼Œéœ€è¦æ„å»º
            
            with record_file.open("r", encoding="utf-8") as f:
                record = json.load(f)
            
            last_doc_count = record.get("doc_count", 0)
            
            # å¦‚æœæ–‡æ¡£æ•°é‡å˜åŒ–è¶…è¿‡10%ï¼Œåˆ™éœ€è¦æ›´æ–°
            change_threshold = max(10, last_doc_count * 0.1)
            needs_update = abs(current_doc_count - last_doc_count) >= change_threshold
            
            return needs_update
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ›´æ–°éœ€æ±‚å¤±è´¥: {str(e)}")
            return True  # å‡ºé”™æ—¶é»˜è®¤éœ€è¦æ›´æ–°
    
    def _get_rebuild_reason(self, should_rebuild: bool, hierarchical_exists: bool, 
                           needs_update: bool, analysis: Dict[str, Any]) -> str:
        """è·å–é‡å»ºåŸå› è¯´æ˜"""
        if not should_rebuild:
            if analysis["doc_count"] == 0:
                return "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œæ— éœ€æ„å»ºåˆ†å±‚ç´¢å¼•"
            elif analysis["doc_count"] < self.threshold_docs:
                return f"æ–‡æ¡£æ•°é‡({analysis['doc_count']})æœªè¾¾åˆ°åˆ†å±‚ç´¢å¼•é˜ˆå€¼({self.threshold_docs})"
            elif analysis["group_count"] < self.min_groups:
                return f"æ–‡æ¡£ç»„æ•°é‡({analysis['group_count']})æœªè¾¾åˆ°æœ€å°è¦æ±‚({self.min_groups})"
            else:
                return "ä¸æ»¡è¶³åˆ†å±‚ç´¢å¼•æ„å»ºæ¡ä»¶"
        
        if not hierarchical_exists:
            return "åˆ†å±‚ç´¢å¼•ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º"
        elif needs_update:
            return "æ–‡æ¡£æ•°é‡å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦æ›´æ–°åˆ†å±‚ç´¢å¼•"
        else:
            return "åˆ†å±‚ç´¢å¼•éœ€è¦é‡å»º"
    
    def _rebuild_hierarchical_index(self, kb_name: str) -> Dict[str, Any]:
        """é‡å»ºåˆ†å±‚ç´¢å¼•"""
        try:
            start_time = time.time()
            
            # åŠ è½½å‘é‡å­˜å‚¨
            vectorstore = self._load_vectorstore(kb_name)
            if not vectorstore:
                return {
                    "success": False,
                    "message": "æ— æ³•åŠ è½½çŸ¥è¯†åº“å‘é‡å­˜å‚¨"
                }
            
            # è·å–çŸ¥è¯†åº“çš„åµŒå…¥æ¨¡å‹é…ç½®
            embeddings = self._get_kb_embeddings(kb_name)
            if not embeddings:
                return {
                    "success": False,
                    "message": "æ— æ³•è·å–çŸ¥è¯†åº“çš„åµŒå…¥æ¨¡å‹é…ç½®"
                }
            
            # åˆ›å»ºåˆ†å±‚ç´¢å¼•æ„å»ºå™¨
            from ..hierarchical_retriever import HierarchicalIndexBuilder
            
            builder = HierarchicalIndexBuilder(
                embeddings=embeddings
            )
            
            # è·å–å‘é‡å­˜å‚¨ä¸­çš„æ‰€æœ‰æ–‡æ¡£
            try:
                # ä»FAISSå‘é‡å­˜å‚¨è·å–æ‰€æœ‰æ–‡æ¡£
                all_docs = []
                
                if hasattr(vectorstore, 'docstore') and hasattr(vectorstore.docstore, '_dict'):
                    # FAISSå‘é‡å­˜å‚¨çš„æ­£ç¡®è·å–æ–¹æ³•
                    doc_dict = vectorstore.docstore._dict
                    all_docs = list(doc_dict.values())
                    logger.info(f"ä»FAISSå‘é‡å­˜å‚¨è·å–åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£")
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ–‡æ¡£çš„åŸºæœ¬ä¿¡æ¯
                    if all_docs:
                        sample_doc = all_docs[0]
                        logger.info(f"ç¤ºä¾‹æ–‡æ¡£å…ƒæ•°æ®: {sample_doc.metadata}")
                        logger.info(f"ç¤ºä¾‹æ–‡æ¡£å†…å®¹é•¿åº¦: {len(sample_doc.page_content)}")
                    
                elif hasattr(vectorstore, 'similarity_search'):
                    # å¤‡ç”¨æ–¹æ³•ï¼šé€šè¿‡æœç´¢è·å–æ–‡æ¡£
                    logger.warning("ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è·å–æ–‡æ¡£")
                    try:
                        # ä½¿ç”¨ä¸€ä¸ªé€šç”¨æŸ¥è¯¢æ¥è·å–æ–‡æ¡£
                        all_docs = vectorstore.similarity_search("", k=1000)  # è·å–å°½å¯èƒ½å¤šçš„æ–‡æ¡£
                        logger.info(f"é€šè¿‡æœç´¢è·å–åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£")
                    except Exception as search_error:
                        logger.error(f"æœç´¢æ–¹æ³•ä¹Ÿå¤±è´¥: {search_error}")
                        all_docs = []
                else:
                    logger.warning("æ— æ³•ä»å‘é‡å­˜å‚¨è·å–æ–‡æ¡£ï¼Œä½¿ç”¨ç©ºæ–‡æ¡£åˆ—è¡¨")
                    all_docs = []
                
                logger.info(f"æœ€ç»ˆè·å–åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£")
                
                # æ„å»ºåˆ†å±‚ç´¢å¼•ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                base_path = Path(__file__).parent.parent.parent.parent / "data" / "knowledge_base"
                vectorstore_path = str(base_path / kb_name / "vector_store")
                
                summary_vs, chunk_vs = builder.build_hierarchical_index(
                    documents=all_docs,
                    vectorstore_path=vectorstore_path
                )
                
            except Exception as doc_error:
                logger.error(f"è·å–æ–‡æ¡£æˆ–æ„å»ºç´¢å¼•å¤±è´¥: {doc_error}")
                raise doc_error
            
            build_time = time.time() - start_time
            
            # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„
            base_path = Path(__file__).parent.parent.parent.parent / "data" / "knowledge_base"
            output_dir = str(base_path / kb_name / "hierarchical_vector_store")
            
            if summary_vs and chunk_vs:
                return {
                    "success": True,
                    "message": "åˆ†å±‚ç´¢å¼•é‡å»ºæˆåŠŸ",
                    "build_time": build_time,
                    "output_dir": output_dir
                }
            else:
                return {
                    "success": False,
                    "message": "åˆ†å±‚ç´¢å¼•æ„å»ºå™¨è¿”å›ç©ºç»“æœ"
                }
                
        except Exception as e:
            logger.error(f"é‡å»ºåˆ†å±‚ç´¢å¼•å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {
                "success": False,
                "message": f"é‡å»ºå¤±è´¥: {str(e)}"
            }
    
    def _get_kb_embeddings(self, kb_name: str):
        """è·å–çŸ¥è¯†åº“çš„åµŒå…¥æ¨¡å‹"""
        try:
            # è·å–çŸ¥è¯†åº“ID
            kb_id = self._get_kb_id_by_name(kb_name)
            if not kb_id:
                logger.warning(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ {kb_name} çš„IDï¼Œä½¿ç”¨é»˜è®¤åµŒå…¥æ¨¡å‹")
                return self._get_default_embeddings()
            
            # åŠ¨æ€å¯¼å…¥çŸ¥è¯†åº“ç®¡ç†å™¨
            from rag_retrievers import KnowledgeBaseManager, RetrieverServiceFactory
            
            # åˆ›å»ºä¸´æ—¶å·¥å‚å’Œç®¡ç†å™¨
            temp_factory = RetrieverServiceFactory()
            temp_manager = KnowledgeBaseManager(temp_factory)
            
            # è·å–çŸ¥è¯†åº“çš„åµŒå…¥é…ç½®
            embedding_config = temp_manager.get_embedding_config_for_kb(str(kb_id))
            
            if embedding_config:
                logger.info(f"ä½¿ç”¨çŸ¥è¯†åº“ {kb_name} é…ç½®çš„åµŒå…¥æ¨¡å‹: {embedding_config['model_name']}")
                # åˆ›å»ºä½¿ç”¨è¯¥é…ç½®çš„å·¥å‚
                factory_with_config = RetrieverServiceFactory(embedding_config)
                return factory_with_config.embeddings
            else:
                logger.warning(f"çŸ¥è¯†åº“ {kb_name} æœªé…ç½®åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
                return self._get_default_embeddings()
                
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            return self._get_default_embeddings()
    
    def _get_default_embeddings(self):
        """è·å–é»˜è®¤åµŒå…¥æ¨¡å‹"""
        try:
            from rag_retrievers import RetrieverServiceFactory
            factory = RetrieverServiceFactory()
            return factory.embeddings
        except Exception as e:
            logger.error(f"è·å–é»˜è®¤åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _update_rebuild_record(self, kb_name: str, doc_count: int):
        """æ›´æ–°é‡å»ºè®°å½•"""
        try:
            record = {
                "kb_name": kb_name,
                "doc_count": doc_count,
                "last_rebuild_time": time.time(),
                "last_rebuild_datetime": time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ä½¿ç”¨ç»å¯¹è·¯å¾„
            base_path = Path(__file__).parent.parent.parent / "data" / "knowledge_base"
            record_file = base_path / kb_name / "hierarchical_rebuild_record.json"
            record_file.parent.mkdir(parents=True, exist_ok=True)
            
            with record_file.open("w", encoding="utf-8") as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
                
            logger.info(f"å·²æ›´æ–°é‡å»ºè®°å½•: {record_file}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°é‡å»ºè®°å½•å¤±è´¥: {str(e)}")

# å…¨å±€æœåŠ¡å®ä¾‹
auto_rebuild_service = AutoHierarchicalRebuildService()

def trigger_auto_rebuild(kb_name: str) -> Dict[str, Any]:
    """
    è§¦å‘è‡ªåŠ¨é‡å»ºï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰
    
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        
    Returns:
        é‡å»ºç»“æœ
    """
    return auto_rebuild_service.auto_rebuild_if_needed(kb_name)