import os
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Tuple, Dict, Any, List, Optional
import hashlib
import asyncio
import sys
import logging
import threading
import time
from queue import Queue, Empty

# è·¨å¹³å°æ–‡ä»¶é”æ”¯æŒ
if sys.platform == "win32":
    import msvcrt
else:
    import fcntl

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessingQueue:
    """æ–‡æ¡£å¤„ç†é˜Ÿåˆ—ç®¡ç†å™¨"""
    
    def __init__(self):
        self.queue = Queue()
        self.processing = False
        self.current_task = None
        self.processed_count = 0
        self.failed_count = 0
        self.worker_thread = None
        
    def add_task(self, task: Dict[str, Any]):
        """æ·»åŠ å¤„ç†ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        self.queue.put(task)
        logger.info(f"æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—: {task['filename']}")
        
        # å¦‚æœæ²¡æœ‰å·¥ä½œçº¿ç¨‹åœ¨è¿è¡Œï¼Œå¯åŠ¨ä¸€ä¸ª
        if not self.processing:
            self.start_worker()
    
    def start_worker(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        if self.worker_thread and self.worker_thread.is_alive():
            return
            
        self.processing = True
        self.worker_thread = threading.Thread(target=self._process_queue, daemon=True)
        self.worker_thread.start()
        logger.info("æ–‡æ¡£å¤„ç†å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
    
    def _process_queue(self):
        """å¤„ç†é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
        while self.processing:
            try:
                # è·å–ä»»åŠ¡ï¼Œè¶…æ—¶1ç§’
                task = self.queue.get(timeout=1)
                self.current_task = task
                
                logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {task['filename']}")
                
                # å¤„ç†æ–‡æ¡£
                success = self._process_single_document(task)
                
                if success:
                    self.processed_count += 1
                    logger.info(f"æ–‡æ¡£å¤„ç†æˆåŠŸ: {task['filename']}")
                else:
                    self.failed_count += 1
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {task['filename']}")
                
                self.queue.task_done()
                self.current_task = None
                
            except Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"å¤„ç†é˜Ÿåˆ—ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                if self.current_task:
                    self.failed_count += 1
                    self.queue.task_done()
                    self.current_task = None
        
        logger.info("æ–‡æ¡£å¤„ç†å·¥ä½œçº¿ç¨‹å·²åœæ­¢")
    
    def _process_single_document(self, task: Dict[str, Any]) -> bool:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            file_path = Path(task['file_path'])
            vector_store_dir = Path(task['vector_store_dir'])
            doc_id = task['doc_id']
            metadata_file = Path(task['metadata_file'])
            kb_name = task.get('kb_name')
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self._update_document_status(metadata_file, doc_id, "processing", False)
            
            # ç¡®ä¿å‘é‡å­˜å‚¨ç›®å½•å­˜åœ¨
            vector_store_dir.mkdir(parents=True, exist_ok=True)
            
            # åŠ¨æ€å¯¼å…¥RAGç®¡é“
            dfy_path = str(Path(__file__).parent.parent.parent / "dfy_langchain")
            if dfy_path not in sys.path:
                sys.path.append(dfy_path)
            
            from dfy_langchain.rag_pipeline import RAGPipeline
            
            # è·å–çŸ¥è¯†åº“çš„embeddingé…ç½®
            embedding_config = None
            embedding_config_id = None
            use_local_embedding = True  # é»˜è®¤ä½¿ç”¨æœ¬åœ°embedding
            
            if kb_name:
                try:
                    # è·å–çŸ¥è¯†åº“é…ç½®
                    from app.services.knowledge_base_service import KnowledgeBaseService
                    kb_service = KnowledgeBaseService()
                    knowledge_bases = kb_service.load_knowledge_bases()
                    
                    kb_config = None
                    for kb in knowledge_bases:
                        if kb.get("name") == kb_name:
                            kb_config = kb
                            break
                    
                    if kb_config:
                        embedding_model_id = kb_config.get("embedding_model_id")
                        if embedding_model_id:
                            # ä»æ•°æ®åº“è·å–embeddingæ¨¡å‹é…ç½®
                            from app.database import get_db
                            from app.services.model_config_service import ModelConfigService
                            
                            db = next(get_db())
                            try:
                                model_config = ModelConfigService.get_model_config_by_id(db, embedding_model_id)
                                if model_config:
                                    embedding_config = {
                                        "id": model_config.id,
                                        "provider": model_config.provider,
                                        "model_name": model_config.model_name,
                                        "api_key": model_config.api_key,
                                        "endpoint": model_config.endpoint,
                                        "model_type": model_config.model_type
                                    }
                                    embedding_config_id = embedding_model_id
                                    use_local_embedding = False
                                    logger.info(f"âœ… ä½¿ç”¨çŸ¥è¯†åº“é…ç½®çš„embeddingæ¨¡å‹: {model_config.model_name} (provider: {model_config.provider})")
                                else:
                                    logger.warning(f"æœªæ‰¾åˆ°embeddingæ¨¡å‹é…ç½® (ID: {embedding_model_id})ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                            finally:
                                db.close()
                        else:
                            logger.warning(f"çŸ¥è¯†åº“ '{kb_name}' æœªé…ç½®embeddingæ¨¡å‹ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                    else:
                        logger.warning(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ '{kb_name}' çš„é…ç½®ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                        
                except Exception as e:
                    logger.error(f"è·å–embeddingé…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹")
            
            # åˆ›å»ºRAGç®¡é“å®ä¾‹ï¼Œä½¿ç”¨é…ç½®çš„embeddingæ¨¡å‹
            if embedding_config:
                rag_pipeline = RAGPipeline(
                    file_path=str(file_path),
                    vector_store_path=str(vector_store_dir),
                    embedding_config=embedding_config,
                    use_local_embedding=False
                )
            else:
                rag_pipeline = RAGPipeline(
                    file_path=str(file_path),
                    vector_store_path=str(vector_store_dir),
                    use_local_embedding=True
                )
            
            # ğŸš€ ä¼˜å…ˆä½¿ç”¨æ™®é€šå‘é‡åŒ–å¤„ç†ï¼Œç¡®ä¿åŸºç¡€å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ
            try:
                logger.info(f"å¼€å§‹å‘é‡åŒ–å¤„ç†æ–‡æ¡£: {task['filename']}")
                result = rag_pipeline.load_single_document(str(file_path))
                
                # å¦‚æœæ™®é€šå‘é‡åŒ–æˆåŠŸï¼Œå†å°è¯•åˆ†å±‚ç´¢å¼•ä¼˜åŒ–
                if result:
                    logger.info(f"åŸºç¡€å‘é‡åŒ–æˆåŠŸï¼Œå°è¯•åˆ†å±‚ç´¢å¼•ä¼˜åŒ–: {task['filename']}")
                    try:
                        hierarchical_result = self._process_with_hierarchical_index(task, embedding_config, use_local_embedding)
                        if hierarchical_result:
                            logger.info(f"åˆ†å±‚ç´¢å¼•ä¼˜åŒ–æˆåŠŸ: {task['filename']}")
                        else:
                            logger.info(f"åˆ†å±‚ç´¢å¼•ä¼˜åŒ–è·³è¿‡ï¼ŒåŸºç¡€å‘é‡åŒ–å·²å®Œæˆ: {task['filename']}")
                    except Exception as hierarchical_error:
                        logger.warning(f"åˆ†å±‚ç´¢å¼•ä¼˜åŒ–å¤±è´¥ï¼Œä½†åŸºç¡€å‘é‡åŒ–å·²å®Œæˆ: {hierarchical_error}")
                else:
                    logger.error(f"å‘é‡åŒ–å¤„ç†å¤±è´¥: {task['filename']}")
            finally:
                # æ˜¾å­˜æ¸…ç†ï¼šåˆ é™¤RAGç®¡é“å®ä¾‹å¹¶å¼ºåˆ¶åƒåœ¾å›æ”¶
                del rag_pipeline
                
                # æ¸…ç†PyTorchç¼“å­˜
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        logger.info(f"å·²æ¸…ç†GPUç¼“å­˜ï¼Œæ–‡æ¡£: {task['filename']}")
                except Exception as cache_error:
                    logger.warning(f"æ¸…ç†GPUç¼“å­˜æ—¶å‡ºç°è­¦å‘Š: {str(cache_error)}")
                
                # å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
                import gc
                gc.collect()
            
            if result:
                # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆ
                self._update_document_status(metadata_file, doc_id, "completed", True)
                logger.info(f"æ–‡æ¡£å¤„ç†æˆåŠŸå¹¶å·²æ¸…ç†æ˜¾å­˜: {task['filename']}")
                logger.info(f"æ–‡æ¡£å‘é‡åŒ–æˆåŠŸ: {task['filename']}")
                
                return True
            else:
                # å¤„ç†å¤±è´¥
                self._update_document_status(metadata_file, doc_id, "error", False, "å‘é‡åŒ–å¤„ç†å¤±è´¥")
                logger.error(f"æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {task['filename']}")
                return False
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            # å³ä½¿å‡ºé”™ä¹Ÿè¦å°è¯•æ¸…ç†æ˜¾å­˜
            try:
                import torch
                import gc
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                gc.collect()
                logger.info(f"é”™è¯¯å¤„ç†åå·²æ¸…ç†æ˜¾å­˜: {task.get('filename', 'unknown')}")
            except Exception as cleanup_error:
                logger.warning(f"é”™è¯¯å¤„ç†åæ¸…ç†æ˜¾å­˜å¤±è´¥: {str(cleanup_error)}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºé”™è¯¯
            self._update_document_status(metadata_file, doc_id, "error", False, str(e))
            
            return False
    
    def _update_document_status(self, metadata_file: Path, doc_id: str, status: str, has_vector: bool, error_message: str = None):
        """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
        try:
            if metadata_file.exists():
                with metadata_file.open("r", encoding="utf-8") as f:
                    metadata = json.load(f)
            else:
                metadata = {"documents": {}}
            
            if doc_id in metadata["documents"]:
                metadata["documents"][doc_id]["vector_status"] = status
                metadata["documents"][doc_id]["has_vector"] = has_vector
                
                if status == "completed":
                    metadata["documents"][doc_id]["vector_time"] = datetime.now().isoformat()
                elif status == "error" and error_message:
                    metadata["documents"][doc_id]["error_message"] = error_message
                
                with metadata_file.open("w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
    
    def _get_kb_paths(self, kb_name: str) -> Dict[str, Path]:
        """è·å–çŸ¥è¯†åº“ç›¸å…³è·¯å¾„"""
        data_dir = Path("data/knowledge_base")
        kb_dir = data_dir / kb_name
        return {
            "kb_dir": kb_dir,
            "content_dir": kb_dir / "content",
            "vector_store_dir": kb_dir / "vector_store",
            "metadata_file": kb_dir / "content" / "documents_metadata.json"
        }
    
    def _process_with_hierarchical_index(self, task: Dict[str, Any], embedding_config: Dict = None, use_local_embedding: bool = True) -> bool:
        """ä½¿ç”¨åˆ†å±‚ç´¢å¼•å¤„ç†æ–‡æ¡£"""
        try:
            kb_name = task.get('kb_name')
            file_path = Path(task['file_path'])
            
            logger.info(f"å¼€å§‹åˆ†å±‚ç´¢å¼•å¤„ç†: {file_path.name}")
            
            # åŠ¨æ€å¯¼å…¥åˆ†å±‚ç´¢å¼•æœåŠ¡
            dfy_path = str(Path(__file__).parent.parent.parent / "dfy_langchain")
            if dfy_path not in sys.path:
                sys.path.append(dfy_path)
            
            from dfy_langchain.retrievers.services.auto_hierarchical_rebuild import trigger_auto_rebuild
            
            # è§¦å‘åˆ†å±‚ç´¢å¼•é‡å»ºï¼ˆåŒ…å«æ–°æ–‡æ¡£ï¼‰
            result = trigger_auto_rebuild(kb_name)
            
            if result["success"]:
                logger.info(f"åˆ†å±‚ç´¢å¼•å¤„ç†æˆåŠŸ: {file_path.name}")
                return True
            else:
                logger.error(f"åˆ†å±‚ç´¢å¼•å¤„ç†å¤±è´¥: {result.get('message', 'unknown error')}")
                return False
                
        except Exception as e:
            logger.error(f"åˆ†å±‚ç´¢å¼•å¤„ç†å¼‚å¸¸: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def _trigger_hierarchical_rebuild_if_needed(self, kb_name: str):
        """è§¦å‘åˆ†å±‚ç´¢å¼•è‡ªåŠ¨é‡å»ºï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        try:
            # åŠ¨æ€å¯¼å…¥è‡ªåŠ¨é‡å»ºæœåŠ¡
            dfy_path = str(Path(__file__).parent.parent.parent / "dfy_langchain")
            if dfy_path not in sys.path:
                sys.path.append(dfy_path)
            
            from dfy_langchain.retrievers.services.auto_hierarchical_rebuild import trigger_auto_rebuild
            
            # è§¦å‘è‡ªåŠ¨é‡å»º
            result = trigger_auto_rebuild(kb_name)
            
            if result["success"]:
                if result["action"] == "rebuilt":
                    logger.info(f"çŸ¥è¯†åº“ {kb_name} åˆ†å±‚ç´¢å¼•å·²è‡ªåŠ¨é‡å»º")
                elif result["action"] == "no_rebuild_needed":
                    logger.info(f"çŸ¥è¯†åº“ {kb_name} æ— éœ€é‡å»ºåˆ†å±‚ç´¢å¼•: {result['message']}")
            else:
                logger.warning(f"çŸ¥è¯†åº“ {kb_name} è‡ªåŠ¨é‡å»ºåˆ†å±‚ç´¢å¼•å¤±è´¥: {result['message']}")
                
        except Exception as e:
            logger.warning(f"è§¦å‘åˆ†å±‚ç´¢å¼•è‡ªåŠ¨é‡å»ºå¤±è´¥: {str(e)}")
            # è¿™æ˜¯ä¸€ä¸ªéå…³é”®åŠŸèƒ½ï¼Œå¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—çŠ¶æ€"""
        return {
            "queue_size": self.queue.qsize(),
            "processing": self.processing,
            "current_task": self.current_task,
            "processed_count": self.processed_count,
            "failed_count": self.failed_count
        }
    
    def stop(self):
        """åœæ­¢å¤„ç†é˜Ÿåˆ—"""
        self.processing = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5)

# å…¨å±€é˜Ÿåˆ—å®ä¾‹
processing_queue = DocumentProcessingQueue()

class DocumentUploadService:
    """æ–‡æ¡£ä¸Šä¼ å’Œå‘é‡åŒ–æœåŠ¡"""
    
    def __init__(self):
        self.data_dir = Path("data/knowledge_base")
    
    def _lock_file(self, file_handle, exclusive=False):
        """è·¨å¹³å°æ–‡ä»¶é”"""
        if sys.platform == "win32":
            # Windows ä½¿ç”¨ msvcrt
            try:
                if exclusive:
                    msvcrt.locking(file_handle.fileno(), msvcrt.LK_NBLCK, 1)
                else:
                    msvcrt.locking(file_handle.fileno(), msvcrt.LK_NBLCK, 1)
            except IOError:
                raise Exception("æ— æ³•è·å–æ–‡ä»¶é”")
        else:
            # Unix/Linux ä½¿ç”¨ fcntl
            lock_type = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH
            fcntl.flock(file_handle.fileno(), lock_type)
    
    def _unlock_file(self, file_handle):
        """è·¨å¹³å°æ–‡ä»¶è§£é”"""
        if sys.platform == "win32":
            # Windows ä½¿ç”¨ msvcrt
            try:
                msvcrt.locking(file_handle.fileno(), msvcrt.LK_UNLCK, 1)
            except IOError:
                pass  # å¿½ç•¥è§£é”é”™è¯¯
        else:
            # Unix/Linux ä½¿ç”¨ fcntl
            fcntl.flock(file_handle.fileno(), fcntl.LOCK_UN)
    
    def _get_kb_paths(self, kb_name: str) -> Dict[str, Path]:
        """è·å–çŸ¥è¯†åº“ç›¸å…³è·¯å¾„"""
        kb_dir = self.data_dir / kb_name
        return {
            "kb_dir": kb_dir,
            "content_dir": kb_dir / "content",
            "vector_store_dir": kb_dir / "vector_store",
            "metadata_file": kb_dir / "content" / "documents_metadata.json"
        }
    
    def _ensure_directories(self, paths: Dict[str, Path]) -> None:
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        paths["content_dir"].mkdir(parents=True, exist_ok=True)
        paths["vector_store_dir"].mkdir(parents=True, exist_ok=True)
    
    def _load_metadata(self, metadata_file: Path) -> Dict[str, Any]:
        """åŠ è½½æ–‡æ¡£å…ƒæ•°æ®ï¼ˆå¸¦æ–‡ä»¶é”ï¼‰"""
        if not metadata_file.exists():
            return {"documents": {}}
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with metadata_file.open("r", encoding="utf-8") as f:
                    # è·å–å…±äº«é”ï¼ˆè¯»é”ï¼‰
                    self._lock_file(f, False)
                    try:
                        content = f.read()
                        if content.strip():
                            return json.loads(content)
                        else:
                            return {"documents": {}}
                    finally:
                        self._unlock_file(f)
            except (json.JSONDecodeError, FileNotFoundError) as e:
                logger.warning(f"å…ƒæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    logger.warning("å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåˆ›å»ºæ–°çš„å…ƒæ•°æ®")
                    return {"documents": {}}
                time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
            except Exception as e:
                logger.error(f"è¯»å–å…ƒæ•°æ®æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
                if attempt == max_retries - 1:
                    return {"documents": {}}
                time.sleep(0.1)
        
        return {"documents": {}}
    
    def _save_metadata(self, metadata_file: Path, metadata: Dict[str, Any]) -> None:
        """ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®ï¼ˆå¸¦æ–‡ä»¶é”å’ŒåŸå­å†™å…¥ï¼‰"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                metadata_file.parent.mkdir(parents=True, exist_ok=True)
                
                # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­å†™å…¥
                temp_file = metadata_file.with_suffix('.tmp')
                
                with temp_file.open("w", encoding="utf-8") as f:
                    # è·å–æ’ä»–é”ï¼ˆå†™é”ï¼‰
                    self._lock_file(f, True)
                    try:
                        json.dump(metadata, f, ensure_ascii=False, indent=2)
                        f.flush()
                        os.fsync(f.fileno())  # å¼ºåˆ¶å†™å…¥ç£ç›˜
                    finally:
                        self._unlock_file(f)
                
                # åŸå­æ€§åœ°æ›¿æ¢åŸæ–‡ä»¶
                temp_file.replace(metadata_file)
                logger.info(f"å…ƒæ•°æ®ä¿å­˜æˆåŠŸ: {metadata_file}")
                return
                
            except Exception as e:
                logger.error(f"ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    raise Exception(f"ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
                time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file.exists():
                    try:
                        temp_file.unlink()
                    except:
                        pass
    
    def _safe_update_metadata(self, metadata_file: Path, doc_id: str, doc_metadata: Dict[str, Any]) -> None:
        """å®‰å…¨åœ°æ›´æ–°å…ƒæ•°æ®ï¼ˆé˜²æ­¢å¹¶å‘å†²çªï¼‰"""
        max_retries = 5
        for attempt in range(max_retries):
            try:
                # é‡æ–°åŠ è½½æœ€æ–°çš„å…ƒæ•°æ®
                current_metadata = self._load_metadata(metadata_file)
                
                # æ·»åŠ æ–°æ–‡æ¡£
                current_metadata["documents"][doc_id] = doc_metadata
                
                # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
                self._save_metadata(metadata_file, current_metadata)
                logger.info(f"æ–‡æ¡£å…ƒæ•°æ®æ›´æ–°æˆåŠŸ: {doc_id}")
                return
                
            except Exception as e:
                logger.error(f"æ›´æ–°å…ƒæ•°æ®å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    raise Exception(f"æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                time.sleep(0.2)  # ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _format_file_size(self, size_in_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        units = ['B', 'KB', 'MB', 'GB', 'TB']
        unit_index = 0
        size = float(size_in_bytes)
        
        while size >= 1024 and unit_index < len(units) - 1:
            size /= 1024
            unit_index += 1
        
        if unit_index == 0:
            return f"{int(size)} {units[unit_index]}"
        else:
            return f"{size:.2f} {units[unit_index]}"
    
    def _get_file_type_text(self, file_ext: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–æ–‡ä»¶ç±»å‹æ–‡æœ¬"""
        type_map = {
            'txt': 'æ–‡æœ¬æ–‡ä»¶',
            'pdf': 'PDFæ–‡æ¡£',
            'doc': 'Wordæ–‡æ¡£',
            'docx': 'Wordæ–‡æ¡£',
            'xls': 'Excelè¡¨æ ¼',
            'xlsx': 'Excelè¡¨æ ¼',
            'ppt': 'PowerPoint',
            'pptx': 'PowerPoint',
            'csv': 'CSVæ–‡ä»¶',
            'json': 'JSONæ–‡ä»¶',
            'xml': 'XMLæ–‡ä»¶',
            'html': 'HTMLæ–‡ä»¶',
            'htm': 'HTMLæ–‡ä»¶',
            'jpg': 'å›¾ç‰‡',
            'jpeg': 'å›¾ç‰‡',
            'png': 'å›¾ç‰‡',
            'gif': 'å›¾ç‰‡'
        }
        return type_map.get(file_ext.lower(), f'{file_ext}æ–‡ä»¶')
    
    async def upload_and_vectorize(self, file_content: bytes, filename: str, kb_name: str) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡æ¡£å¹¶æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—"""
        try:
            # è·å–è·¯å¾„ä¿¡æ¯
            paths = self._get_kb_paths(kb_name)
            self._ensure_directories(paths)
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            file_path = paths["content_dir"] / filename
            with file_path.open("wb") as f:
                f.write(file_content)
            
            logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
            
            # ç”Ÿæˆæ–‡æ¡£IDï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¶é—´æˆ³é¿å…å†²çªï¼‰
            timestamp = int(datetime.now().timestamp() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³
            doc_id = f"doc_{timestamp}"
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = os.path.getsize(file_path)
            file_ext = filename.split(".")[-1].lower() if "." in filename else "unknown"
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„åˆ†å—å¤§å°
            if file_ext == "excel":
                chunk_size = 3000
                chunk_overlap = 0
            else:
                chunk_size = 1500  # å¢åŠ å—å¤§å°ï¼Œå‡å°‘å—æ•°é‡
                chunk_overlap = 150  # ä¿æŒ10%çš„é‡å ç‡
            
            # åˆå§‹åŒ–æ–‡æ¡£å…ƒæ•°æ®
            doc_metadata = {
                "filename": filename,
                "file_size": file_size,
                "size_str": self._format_file_size(file_size),
                "file_type": file_ext,
                "file_type_text": self._get_file_type_text(file_ext),
                "upload_time": datetime.now().isoformat(),
                "has_vector": False,
                "vector_status": "waiting",
                "problem_status": "æ­£å¸¸",
                "file_hash": self._get_file_hash(file_path)
            }
            
            # ä½¿ç”¨å®‰å…¨çš„å…ƒæ•°æ®æ›´æ–°æ–¹æ³•
            self._safe_update_metadata(paths["metadata_file"], doc_id, doc_metadata)
            
            # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
            task = {
                "doc_id": doc_id,
                "filename": filename,
                "file_path": str(file_path),
                "vector_store_dir": str(paths["vector_store_dir"]),
                "metadata_file": str(paths["metadata_file"]),
                "kb_name": kb_name
            }
            
            processing_queue.add_task(task)
            
            return {
                "success": True,
                "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œå·²æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—",
                "data": {
                    "doc_id": doc_id,
                    "filename": filename,
                    "has_vector": False,
                    "vector_status": "waiting",
                    "queue_position": processing_queue.queue.qsize()
                }
            }
            
        except Exception as e:
            logger.error(f"ä¸Šä¼ æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise Exception(f"ä¸Šä¼ æ–‡æ¡£å¤±è´¥: {str(e)}")
    
    def get_processing_status(self) -> Dict[str, Any]:
        """è·å–å¤„ç†é˜Ÿåˆ—çŠ¶æ€"""
        return processing_queue.get_status()
    
    def get_documents(self, kb_name: str, page: int = 1, limit: int = 10) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£åˆ—è¡¨"""
        try:
            paths = self._get_kb_paths(kb_name)
            
            if not paths["metadata_file"].exists():
                return {
                    "success": True,
                    "data": [],
                    "pagination": {
                        "page": page,
                        "limit": limit,
                        "total": 0,
                        "total_pages": 0
                    }
                }
            
            metadata = self._load_metadata(paths["metadata_file"])
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
            documents = [{
                "id": doc_id,
                **doc_data
            } for doc_id, doc_data in metadata["documents"].items()]
            documents.sort(key=lambda x: x["upload_time"], reverse=True)
            
            # è®¡ç®—åˆ†é¡µ
            total = len(documents)
            total_pages = (total + limit - 1) // limit
            start_idx = (page - 1) * limit
            end_idx = start_idx + limit
            
            return {
                "success": True,
                "data": documents[start_idx:end_idx],
                "pagination": {
                    "page": page,
                    "limit": limit,
                    "total": total,
                    "total_pages": total_pages
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
            raise Exception(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
    
    def delete_document(self, doc_id: str, kb_name: str) -> Dict[str, Any]:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            paths = self._get_kb_paths(kb_name)
            
            if not paths["metadata_file"].exists():
                raise Exception("æ‰¾ä¸åˆ°æ–‡æ¡£å…ƒæ•°æ®")
            
            metadata = self._load_metadata(paths["metadata_file"])
            
            if doc_id not in metadata["documents"]:
                raise Exception("æ‰¾ä¸åˆ°æŒ‡å®šçš„æ–‡æ¡£")
            
            # åˆ é™¤åŸå§‹æ–‡ä»¶
            file_path = paths["content_dir"] / metadata["documents"][doc_id]["filename"]
            if file_path.exists():
                file_path.unlink()
            
            # ä»å…ƒæ•°æ®ä¸­åˆ é™¤
            del metadata["documents"][doc_id]
            self._save_metadata(paths["metadata_file"], metadata)
            
            return {
                "success": True,
                "message": "æ–‡æ¡£åˆ é™¤æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise Exception(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")