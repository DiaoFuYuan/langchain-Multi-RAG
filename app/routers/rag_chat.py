"""
RAGèŠå¤©è·¯ç”±
æä¾›åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½å¯¹è¯åŠŸèƒ½
"""

import os
import json
import sys
import asyncio
from typing import List, Dict, Any, Optional, Union
from fastapi import APIRouter, HTTPException, Request, Query, Depends
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from sqlalchemy.orm import Session

# æ·»åŠ dfy_langchainç›®å½•åˆ°è·¯å¾„ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
dfy_langchain_path = os.path.join(project_root, "dfy_langchain")

if dfy_langchain_path not in sys.path:
    sys.path.insert(0, dfy_langchain_path)

# å°è¯•å¯¼å…¥çœŸæ­£çš„RAGç”Ÿæˆå™¨
RAGGenerator = None
try:
    from rag_generation import RAGGenerator
    print("æˆåŠŸå¯¼å…¥dfy_langchain.RAGGenerator")
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥dfy_langchain.RAGGenerator: {e}")
    try:
        # å°è¯•ä½¿ç”¨ç»å¯¹å¯¼å…¥
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'dfy_langchain'))
        from rag_generation import RAGGenerator
        print("ä½¿ç”¨ç»å¯¹è·¯å¾„æˆåŠŸå¯¼å…¥RAGGenerator")
    except ImportError as e2:
        print(f"ç»å¯¹è·¯å¾„å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}")
        RAGGenerator = None

router = APIRouter(prefix="/rag", tags=["RAGèŠå¤©"])
templates = Jinja2Templates(directory="templates")

# å…¨å±€RAGç”Ÿæˆå™¨å®ä¾‹
rag_generator = None

# æ·»åŠ æ•°æ®åº“ä¾èµ–å¯¼å…¥
from app.database import get_db
from app.services.model_config_service import ModelConfigService

class ModelConfig(BaseModel):
    """æ¨¡å‹é…ç½®æ¨¡å‹"""
    model_id: Optional[int] = None
    provider: Optional[str] = None
    model_name: Optional[str] = None
    endpoint: Optional[str] = None
    display_name: Optional[str] = None

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    message: str
    knowledge_bases: List[str] = []
    knowledge_base_ids: List[Union[str, int]] = []
    history: List[dict] = []
    session_id: Optional[str] = None
    # RAGé…ç½®å‚æ•° - åŸºç¡€æ£€ç´¢é…ç½®
    top_k: Optional[int] = 10
    threshold: Optional[float] = 0.3
    context_window: Optional[int] = 150
    keyword_threshold: Optional[int] = 1
    # RAGé…ç½®å‚æ•° - å¢å¼ºåŠŸèƒ½
    enable_context_enrichment: Optional[bool] = True
    enable_ranking: Optional[bool] = True
    rerank: Optional[bool] = False
    # RAGé…ç½®å‚æ•° - ç”Ÿæˆå™¨é…ç½®
    temperature: Optional[float] = 0.3
    memory_window: Optional[int] = 5
    # ä¿®å¤å­—æ®µåå†²çªï¼šmodel_configæ”¹ä¸ºselected_model
    selected_model: Optional[ModelConfig] = None

class KnowledgeBaseInfo(BaseModel):
    """çŸ¥è¯†åº“ä¿¡æ¯æ¨¡å‹"""
    id: int
    name: str
    description: str
    status: str

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str
    knowledge_bases: List[str] = []
    knowledge_base_ids: List[Union[str, int]] = []
    history: List[dict] = []
    session_id: Optional[str] = None
    # RAGé…ç½®å‚æ•° - åŸºç¡€æ£€ç´¢é…ç½®
    top_k: Optional[int] = 10
    threshold: Optional[float] = 0.3
    context_window: Optional[int] = 150
    keyword_threshold: Optional[int] = 1
    # RAGé…ç½®å‚æ•° - å¢å¼ºåŠŸèƒ½
    enable_context_enrichment: Optional[bool] = True
    enable_ranking: Optional[bool] = True
    rerank: Optional[bool] = False
    # RAGé…ç½®å‚æ•° - ç”Ÿæˆå™¨é…ç½®
    temperature: Optional[float] = 0.3
    memory_window: Optional[int] = 5
    # æ£€ç´¢å™¨ç±»å‹é…ç½®
    retriever_type: Optional[str] = "auto"
    # ä¿®å¤å­—æ®µåå†²çªï¼šmodel_configæ”¹ä¸ºselected_model
    selected_model: Optional[ModelConfig] = None

def get_model_config_from_db(db: Session, model_id: int) -> Optional[dict]:
    """ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®"""
    try:
        model_config = ModelConfigService.get_model_config_by_id(db, model_id)
        if model_config:
            return {
                "provider": model_config.provider,
                "model_name": model_config.model_name,
                "api_key": model_config.api_key,
                "endpoint": model_config.endpoint,
                "organization": model_config.organization
            }
        return None
    except Exception as e:
        print(f"è·å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        return None

def create_dynamic_rag_generator(model_config: dict = None, rag_config: dict = None):
    """åˆ›å»ºåŠ¨æ€RAGç”Ÿæˆå™¨"""
    if RAGGenerator is None:
        print("RAGGeneratorç±»ä¸å¯ç”¨")
        return None
    
    try:
        # è®¾ç½®çŸ¥è¯†åº“è·¯å¾„
        kb_path = os.path.join("data", "knowledge_base")
        
        # è®¾ç½®RAGé…ç½®å‚æ•°ï¼Œä½¿ç”¨å‰ç«¯ä¼ é€’çš„å€¼æˆ–é»˜è®¤å€¼
        if rag_config is None:
            rag_config = {}
        
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        print("=" * 60)
        print("ğŸ”§ RAGé…ç½®å‚æ•°è¯¦æƒ…:")
        print(f"  ğŸ“Š æ¥æ”¶åˆ°çš„rag_config: {rag_config}")
        
        # åŸºç¡€æ£€ç´¢é…ç½®
        top_k = rag_config.get("top_k", int(os.getenv("RAG_TOP_K", "5")))
        threshold = rag_config.get("threshold", float(os.getenv("RAG_THRESHOLD", "0.7")))
        context_window = rag_config.get("context_window", int(os.getenv("RAG_CONTEXT_WINDOW", "150")))
        keyword_threshold = rag_config.get("keyword_threshold", int(os.getenv("RAG_KEYWORD_THRESHOLD", "1")))
        
        # å¢å¼ºåŠŸèƒ½é…ç½®
        enable_context_enrichment = rag_config.get("enable_context_enrichment", 
                                                   os.getenv("ENABLE_CONTEXT_ENRICHMENT", "true").lower() == "true")
        enable_ranking = rag_config.get("enable_ranking", 
                                       os.getenv("ENABLE_RANKING", "true").lower() == "true")
        rerank = rag_config.get("rerank", 
                               os.getenv("ENABLE_RERANK", "false").lower() == "true")
        
        # ç”Ÿæˆå™¨é…ç½®
        temperature = rag_config.get("temperature", float(os.getenv("RAG_TEMPERATURE", "0.3")))
        memory_window = rag_config.get("memory_window", int(os.getenv("RAG_MEMORY_WINDOW", "5")))
        
        print(f"  ğŸ¯ åŸºç¡€æ£€ç´¢é…ç½®:")
        print(f"    - top_k: {top_k}")
        print(f"    - threshold: {threshold}")
        print(f"    - context_window: {context_window}")
        print(f"    - keyword_threshold: {keyword_threshold}")
        print(f"  ğŸš€ å¢å¼ºåŠŸèƒ½é…ç½®:")
        print(f"    - enable_context_enrichment: {enable_context_enrichment}")
        print(f"    - enable_ranking: {enable_ranking}")
        print(f"    - rerank: {rerank}")
        print(f"  ğŸ›ï¸ ç”Ÿæˆå™¨é…ç½®:")
        print(f"    - temperature: {temperature}")
        print(f"    - memory_window: {memory_window}")
        print("=" * 60)
        
        # å¦‚æœæœ‰æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        if model_config:
            # å¤„ç†æ¨¡å‹æä¾›å•†å…¼å®¹æ€§é—®é¢˜
            provider = model_config["provider"]
            
            # å°†æ‰€æœ‰OpenAIå…¼å®¹çš„æä¾›å•†ç»Ÿä¸€å¤„ç†
            openai_compatible_providers = ["openai-compatible", "deepseek"]
            if provider in openai_compatible_providers:
                provider = "openai"  # ç»Ÿä¸€æ˜ å°„ä¸ºopenai
                print(f"âœ… æä¾›å•†æ˜ å°„: {model_config['provider']} -> {provider}")
            
            # è§„èŒƒåŒ–API URL
            api_url = model_config["endpoint"]
            if api_url:
                # ç¡®ä¿URLä»¥æ­£ç¡®çš„è·¯å¾„ç»“å°¾
                api_url = api_url.rstrip('/')
                if not api_url.endswith('/v1'):
                    api_url += '/v1'
                
                # éªŒè¯URLæ ¼å¼
                if not (api_url.startswith('http://') or api_url.startswith('https://')):
                    print(f"è­¦å‘Šï¼šAPI URLæ ¼å¼å¯èƒ½ä¸æ­£ç¡®: {api_url}")
            
            print(f"ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹é…ç½®: {provider} - {model_config['model_name']}")
            print(f"åŸå§‹æä¾›å•†: {model_config['provider']}")
            print(f"è§„èŒƒåŒ–åçš„API URL: {api_url}")
            
            # åœ¨åˆ›å»ºç”Ÿæˆå™¨å‰éªŒè¯APIé…ç½®
            if not model_config.get("api_key"):
                print("è­¦å‘Šï¼šAPIå¯†é’¥æœªé…ç½®")
                return None
            
            rag_generator = RAGGenerator(
                model_name=model_config["model_name"],
                model_provider=provider,  # ä½¿ç”¨è½¬æ¢åçš„provider
                temperature=temperature,
                top_k=top_k,
                enable_context_enrichment=enable_context_enrichment,
                enable_ranking=enable_ranking,
                memory_window_size=memory_window,
                knowledge_base_path=kb_path,
                api_key=model_config["api_key"],
                api_url=api_url,
                # ä¼ é€’æ–°å¢çš„RAGé…ç½®å‚æ•°
                keyword_threshold=keyword_threshold,
                context_window=context_window,
                score_threshold=threshold,
                weight_keyword_freq=0.4,  # ä½¿ç”¨é»˜è®¤æƒé‡
                weight_keyword_pos=0.3,
                weight_keyword_coverage=0.3
            )
        else:
            print("ä½¿ç”¨é»˜è®¤æ¨¡å‹é…ç½®")
            rag_generator = RAGGenerator(
                model_name=os.getenv("RAG_MODEL_NAME", "deepseek-chat"),
                model_provider=os.getenv("RAG_MODEL_PROVIDER", "openai"),
                temperature=temperature,
                top_k=top_k,
                enable_context_enrichment=enable_context_enrichment,
                enable_ranking=enable_ranking,
                memory_window_size=memory_window,
                knowledge_base_path=kb_path,
                # ä¼ é€’æ–°å¢çš„RAGé…ç½®å‚æ•°
                keyword_threshold=keyword_threshold,
                context_window=context_window,
                score_threshold=threshold,
                weight_keyword_freq=0.4,  # ä½¿ç”¨é»˜è®¤æƒé‡
                weight_keyword_pos=0.3,
                weight_keyword_coverage=0.3
            )
        
        print("âœ… åŠ¨æ€RAGç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return rag_generator
    except Exception as e:
        print(f"âŒ åŠ¨æ€RAGç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def init_rag_generator():
    """åˆå§‹åŒ–RAGç”Ÿæˆå™¨"""
    global rag_generator
    if rag_generator is None and RAGGenerator is not None:
        try:
            # è®¾ç½®çŸ¥è¯†åº“è·¯å¾„
            kb_path = os.path.join("data", "knowledge_base")
            
            print("å¼€å§‹åˆå§‹åŒ–çœŸæ­£çš„RAGç”Ÿæˆå™¨...")
            rag_generator = RAGGenerator(
                model_name=os.getenv("RAG_MODEL_NAME", "deepseek-chat"),
                model_provider=os.getenv("RAG_MODEL_PROVIDER", "openai"),
                temperature=float(os.getenv("RAG_TEMPERATURE", "0.3")),
                top_k=int(os.getenv("RAG_TOP_K", "5")),
                enable_context_enrichment=True,
                enable_ranking=True,
                memory_window_size=10,
                knowledge_base_path=kb_path,
                # ä¼ é€’æ–°å¢çš„RAGé…ç½®å‚æ•°
                keyword_threshold=int(os.getenv("RAG_KEYWORD_THRESHOLD", "1")),
                context_window=int(os.getenv("RAG_CONTEXT_WINDOW", "150")),
                score_threshold=float(os.getenv("RAG_THRESHOLD", "0.7")),
                weight_keyword_freq=0.4,  # ä½¿ç”¨é»˜è®¤æƒé‡
                weight_keyword_pos=0.3,
                weight_keyword_coverage=0.3
            )
            print("RAGç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ - ä½¿ç”¨çœŸæ­£çš„dfy_langchainå®ç°")
            return rag_generator
        except Exception as e:
            print(f"RAGç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            rag_generator = None
    
    # å¦‚æœRAGGeneratorä¸ºNoneæˆ–åˆå§‹åŒ–å¤±è´¥ï¼Œè¿”å›Noneè€Œä¸æ˜¯åˆ›å»ºç®€åŒ–ç‰ˆæœ¬
    if rag_generator is None:
        print("æ— æ³•åˆå§‹åŒ–RAGç”Ÿæˆå™¨ï¼Œæ£€æŸ¥dfy_langchainæ¨¡å—å’Œç¯å¢ƒé…ç½®")
        return None
    
    return rag_generator

def load_knowledge_bases() -> List[Dict[str, Any]]:
    """åŠ è½½çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        kb_file = os.path.join("data", "knowledge_base", "knowledge_bases.json")
        if os.path.exists(kb_file):
            with open(kb_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("knowledge_bases", [])
        return []
    except Exception as e:
        print(f"åŠ è½½çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")
        return []

@router.get("/api/knowledge-bases")
async def get_knowledge_bases():
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨API"""
    try:
        knowledge_bases = load_knowledge_bases()
        
        # ä¸ºæ¯ä¸ªçŸ¥è¯†åº“æ·»åŠ æ–‡æ¡£æ•°é‡
        from app.services.knowledge_base_service import KnowledgeBaseService
        kb_service = KnowledgeBaseService()
        
        for kb in knowledge_bases:
            kb_name = kb.get("name", "")
            if kb_name:
                kb["document_count"] = kb_service.count_knowledge_base_documents(kb_name)
            else:
                kb["document_count"] = 0
        
        return {
            "success": True,
            "knowledge_bases": knowledge_bases
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/api/chat")
async def rag_chat(chat_request: ChatMessage, db: Session = Depends(get_db)):
    """RAGèŠå¤©API"""
    try:
        # è·å–çŸ¥è¯†åº“IDåˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨knowledge_base_idsï¼Œå¹¶ç¡®ä¿ç±»å‹ä¸€è‡´æ€§
        kb_ids_raw = chat_request.knowledge_base_ids or chat_request.knowledge_bases
        kb_ids = [str(id) for id in kb_ids_raw]  # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = None
        if chat_request.selected_model and chat_request.selected_model.model_id:
            model_config = get_model_config_from_db(db, chat_request.selected_model.model_id)
            if not model_config:
                print(f"æ— æ³•è·å–æ¨¡å‹é…ç½®ï¼Œmodel_id: {chat_request.selected_model.model_id}")
        
        # å‡†å¤‡RAGé…ç½®å‚æ•°
        rag_config = {
            # åŸºç¡€æ£€ç´¢é…ç½®
            "top_k": chat_request.top_k,
            "threshold": chat_request.threshold,
            "context_window": chat_request.context_window,
            "keyword_threshold": chat_request.keyword_threshold,
            # å¢å¼ºåŠŸèƒ½é…ç½®
            "enable_context_enrichment": chat_request.enable_context_enrichment,
            "enable_ranking": chat_request.enable_ranking,
            "rerank": chat_request.rerank,
            # ç”Ÿæˆå™¨é…ç½®
            "temperature": chat_request.temperature,
            "memory_window": chat_request.memory_window
        }
        
        # åˆ›å»ºåŠ¨æ€RAGç”Ÿæˆå™¨
        generator = create_dynamic_rag_generator(model_config, rag_config)
        if generator is None:
            # å¦‚æœRAGç”Ÿæˆå™¨ä¸å¯ç”¨ï¼Œè¿”å›ç®€å•çš„å›å¤
            return {
                "success": True,
                "answer": "æŠ±æ­‰ï¼ŒRAGç³»ç»Ÿå½“å‰ä¸å¯ç”¨ã€‚è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›å¤ï¼šæ‚¨è¯¢é—®äº† \"" + chat_request.message + "\"ï¼Œä½†æˆ‘æš‚æ—¶æ— æ³•åŸºäºçŸ¥è¯†åº“å›ç­”ã€‚",
                "query": chat_request.message,
                "source_documents": [],
                "knowledge_bases_used": kb_ids
            }
        
        # å¤„ç†æ—¥å¿—ä¸­çš„æ¨¡å‹æä¾›å•†æ˜¾ç¤º
        display_provider = model_config['provider'] if model_config else ""
        if display_provider == "openai-compatible":
            display_provider = "openai"
        model_info = f" (ä½¿ç”¨æ¨¡å‹: {display_provider} - {model_config['model_name']})" if model_config else ""
        print(f"ä½¿ç”¨RAGé…ç½® - top_k: {rag_config['top_k']}, threshold: {rag_config['threshold']}, "
              f"context_window: {rag_config['context_window']}, temperature: {rag_config['temperature']}{model_info}")
        
        # ä½¿ç”¨RAGç”Ÿæˆå™¨è¿›è¡ŒæŸ¥è¯¢å’Œç”Ÿæˆ
        result = generator.retrieve_and_generate(
            query=chat_request.message,
            knowledge_base_ids=kb_ids,
            return_source_documents=True
        )
        
        # æ ¼å¼åŒ–æºæ–‡æ¡£ä¿¡æ¯
        source_docs = []
        if "source_documents" in result:
            seen_sources = set()  # ç”¨äºè®°å½•å·²ç»è§è¿‡çš„æºæ–‡ä»¶
            
            for doc in result["source_documents"]:
                source_name = os.path.basename(doc.metadata.get("source", "æœªçŸ¥æ¥æº"))
                
                # å¦‚æœè¿™ä¸ªæºæ–‡ä»¶è¿˜æ²¡æœ‰è¢«æ·»åŠ è¿‡ï¼Œåˆ™æ·»åŠ 
                if source_name not in seen_sources:
                    seen_sources.add(source_name)
                    source_docs.append({
                        "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                        "source": source_name,
                        "page": doc.metadata.get("page", None)
                    })
        
        return {
            "success": True,
            "answer": result.get("answer", "æ— æ³•ç”Ÿæˆå›ç­”"),
            "query": result.get("query", chat_request.message),
            "source_documents": source_docs,
            "knowledge_bases_used": kb_ids,
            "config_used": {
                "top_k": rag_config['top_k'],
                "threshold": rag_config['threshold'],
                "context_window": rag_config['context_window'],
                "keyword_threshold": rag_config['keyword_threshold'],
                "enable_context_enrichment": rag_config['enable_context_enrichment'],
                "enable_ranking": rag_config['enable_ranking'],
                "rerank": rag_config['rerank'],
                "temperature": rag_config['temperature'],
                "memory_window": rag_config['memory_window'],
                "model_config": model_config
            }
        }
        
    except Exception as e:
        print(f"RAGèŠå¤©å¤„ç†å¤±è´¥: {e}")
        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            "success": False,
            "error": f"èŠå¤©å¤„ç†å¤±è´¥: {str(e)}",
            "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "query": chat_request.message,
            "source_documents": [],
            "knowledge_bases_used": kb_ids
        }

def get_rag_generator():
    """è·å–RAGç”Ÿæˆå™¨å®ä¾‹"""
    return init_rag_generator()

@router.post("/api/chat-stream")
async def rag_chat_stream(chat_request: ChatRequest, db: Session = Depends(get_db)):
    """
    æµå¼RAGèŠå¤©API (æ”¯æŒæœåŠ¡å™¨å‘é€äº‹ä»¶)
    """
    async def generate_stream():
        try:
            # è·å–çŸ¥è¯†åº“IDåˆ—è¡¨
            kb_ids_raw = chat_request.knowledge_base_ids or chat_request.knowledge_bases
            kb_ids = [str(id) for id in kb_ids_raw]
            
            # è·å–æ¨¡å‹é…ç½®
            model_config = None
            if chat_request.selected_model and chat_request.selected_model.model_id:
                model_config = get_model_config_from_db(db, chat_request.selected_model.model_id)
                if not model_config:
                    error_msg = f"æ— æ³•è·å–æ¨¡å‹é…ç½®ï¼Œmodel_id: {chat_request.selected_model.model_id}"
                    print(error_msg)
                    yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                    return
            
            # éªŒè¯æ¨¡å‹é…ç½®
            if model_config:
                if not model_config.get("api_key"):
                    error_msg = "æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®"
                    yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                    return
                
                if not model_config.get("endpoint"):
                    error_msg = "æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘APIç«¯ç‚¹ï¼Œè¯·æ£€æŸ¥é…ç½®"
                    yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                    return
            
            # å‡†å¤‡RAGé…ç½®å‚æ•°
            rag_config = {
                # åŸºç¡€æ£€ç´¢é…ç½®
                "top_k": chat_request.top_k,
                "threshold": chat_request.threshold,
                "context_window": chat_request.context_window,
                "keyword_threshold": chat_request.keyword_threshold,
                # å¢å¼ºåŠŸèƒ½é…ç½®
                "enable_context_enrichment": chat_request.enable_context_enrichment,
                "enable_ranking": chat_request.enable_ranking,
                "rerank": chat_request.rerank,
                # ç”Ÿæˆå™¨é…ç½®
                "temperature": chat_request.temperature,
                "memory_window": chat_request.memory_window
            }
            
            # åˆ›å»ºåŠ¨æ€RAGç”Ÿæˆå™¨
            generator = create_dynamic_rag_generator(model_config, rag_config)
            if generator is None:
                error_msg = "RAGç³»ç»Ÿå½“å‰ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–ã€‚"
                yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                return
            
            # å¤„ç†æ—¥å¿—ä¸­çš„æ¨¡å‹æä¾›å•†æ˜¾ç¤º
            display_provider = model_config['provider'] if model_config else ""
            if display_provider == "openai-compatible":
                display_provider = "openai"
            model_info = f" (ä½¿ç”¨æ¨¡å‹: {display_provider} - {model_config['model_name']})" if model_config else ""
            print(f"æµå¼RAGé…ç½® - top_k: {rag_config['top_k']}, threshold: {rag_config['threshold']}, "
                  f"context_window: {rag_config['context_window']}, temperature: {rag_config['temperature']}{model_info}")
            
            # å‘é€å¼€å§‹ä¿¡å·
            yield f"data: {json.dumps({'type': 'start'})}\n\n"
            
            # æ‰§è¡ŒçœŸæ­£çš„RAGæ£€ç´¢å’Œç”Ÿæˆ
            print(f"æ‰§è¡ŒRAGæŸ¥è¯¢: {chat_request.message}, çŸ¥è¯†åº“: {kb_ids}")
            
            try:
                result = generator.retrieve_and_generate(
                    query=chat_request.message,
                    knowledge_base_ids=kb_ids,
                    return_source_documents=True,
                    retriever_type=chat_request.retriever_type or "auto"  # ä¼ é€’æ£€ç´¢å™¨ç±»å‹
                )
            except Exception as api_error:
                # æ£€æŸ¥æ˜¯å¦æ˜¯APIè¿æ¥é”™è¯¯
                error_str = str(api_error)
                if "404" in error_str and "Not Found" in error_str:
                    error_msg = f"APIç«¯ç‚¹ä¸å¯ç”¨ (404é”™è¯¯)ã€‚å¯èƒ½çš„åŸå› ï¼š\n1. APIæœåŠ¡å™¨å®•æœº\n2. ç«¯ç‚¹URLä¸æ­£ç¡®\n3. éœ€è¦æ·»åŠ /v1è·¯å¾„\n\nå½“å‰é…ç½®ï¼š{model_config.get('endpoint') if model_config else 'é»˜è®¤é…ç½®'}"
                elif "Connection" in error_str or "timeout" in error_str.lower():
                    error_msg = f"æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ï¼š\n1. ç½‘ç»œè¿æ¥\n2. APIæœåŠ¡å™¨çŠ¶æ€\n3. ç«¯ç‚¹URLæ˜¯å¦æ­£ç¡®\n\né”™è¯¯è¯¦æƒ…ï¼š{error_str}"
                elif "401" in error_str or "Unauthorized" in error_str:
                    error_msg = f"APIå¯†é’¥éªŒè¯å¤±è´¥ã€‚è¯·æ£€æŸ¥ï¼š\n1. APIå¯†é’¥æ˜¯å¦æ­£ç¡®\n2. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ\n3. æ˜¯å¦æœ‰æƒé™è®¿é—®è¯¥æ¨¡å‹"
                else:
                    error_msg = f"APIè°ƒç”¨å¤±è´¥ï¼š{error_str}"
                
                print(f"RAG APIè°ƒç”¨å¤±è´¥: {error_msg}")
                yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                return
            
            # è·å–ç”Ÿæˆçš„å›ç­”
            answer = result.get("answer", "æœªèƒ½ç”Ÿæˆå›ç­”")
            
            # é€å­—ç¬¦æµå¼å‘é€å›ç­”
            for char in answer:
                yield f"data: {json.dumps({'type': 'message', 'content': char})}\n\n"
                await asyncio.sleep(0.03)  # æ§åˆ¶å‘é€é€Ÿåº¦ï¼Œæ¨¡æ‹ŸçœŸå®çš„ç”Ÿæˆè¿‡ç¨‹
            
            # å‘é€æºæ–‡æ¡£ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            source_documents = result.get("source_documents", [])
            if source_documents:
                # ä½¿ç”¨é›†åˆæ¥å»é‡æºæ–‡æ¡£
                seen_sources = set()
                unique_sources = []
                
                for doc in source_documents:
                    source_name = os.path.basename(doc.metadata.get("source", "æœªçŸ¥æ¥æº"))
                    # åªæœ‰å½“æºæ–‡æ¡£åç§°æœªå‡ºç°è¿‡æ—¶æ‰æ·»åŠ 
                    if source_name not in seen_sources:
                        seen_sources.add(source_name)
                        unique_sources.append(source_name)
                
                if unique_sources:
                    source_info = "\n\nğŸ“š å‚è€ƒæ¥æº:\n"
                    for i, source_name in enumerate(unique_sources[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªä¸é‡å¤çš„æºæ–‡æ¡£
                        source_info += f"{i}. {source_name}\n"
                    
                    # å‘é€æºæ–‡æ¡£ä¿¡æ¯
                    for char in source_info:
                        yield f"data: {json.dumps({'type': 'message', 'content': char})}\n\n"
                        await asyncio.sleep(0.01)
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            print(f"æµå¼RAGèŠå¤©å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield f"data: {json.dumps({'type': 'error', 'message': f'å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream"
        }
    )

@router.post("/api/clear-history")
async def clear_chat_history():
    """æ¸…é™¤èŠå¤©å†å²"""
    try:
        generator = get_rag_generator()
        if generator:
            generator.clear_history()
        return {"success": True, "message": "èŠå¤©å†å²å·²æ¸…é™¤"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤å†å²å¤±è´¥: {str(e)}")

@router.get("/api/test")
async def test_rag_system():
    """æµ‹è¯•RAGç³»ç»ŸçŠ¶æ€"""
    try:
        generator = init_rag_generator()
        if generator is None:
            return {
                "success": False,
                "message": "RAGç”Ÿæˆå™¨æœªåˆå§‹åŒ–",
                "status": "offline"
            }
        
        # ç®€å•æµ‹è¯•æŸ¥è¯¢
        test_result = generator.retrieve_and_generate(
            query="æµ‹è¯•æŸ¥è¯¢",
            return_source_documents=False
        )
        
        return {
            "success": True,
            "message": "RAGç³»ç»Ÿè¿è¡Œæ­£å¸¸",
            "status": "online",
            "test_result": test_result
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"RAGç³»ç»Ÿæµ‹è¯•å¤±è´¥: {str(e)}",
            "status": "error"
        } 